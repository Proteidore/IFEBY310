{
  "hash": "ca2ff51fbea43ef30df8aa4e58283ac8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Building parquet dataset from extracted csv files\njupyter: python3\n---\n\n::: {#1203f8fe .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport sys\nimport re \nimport pandas as pd\nimport datetime\nfrom tqdm import tqdm\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n```\n:::\n\n\n::: {#a6c345a7 .cell execution_count=2}\n``` {.python .cell-code}\ndata_dir = \"../data\"\n# os.path.exists(data_dir)\n\nextract_dir = os.path.join(data_dir, \"xcitibike\")\nif not os.path.exists(extract_dir):\n    os.mkdir(extract_dir)\n\nparquet_dir = os.path.join(data_dir, \"pq_citibike\")\nif not os.path.exists(parquet_dir):\n    os.mkdir(parquet_dir)\n\ncheckpoint_dir = os.path.join(data_dir, \"citibike_charlie\")\nif not os.path.exists(checkpoint_dir):\n    os.mkdir(checkpoint_dir)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">FileNotFoundError</span>                         Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[2], line 6</span>\n<span class=\"ansi-green-fg ansi-bold\">      4</span> extract_dir <span style=\"color:rgb(98,98,98)\">=</span> os<span style=\"color:rgb(98,98,98)\">.</span>path<span style=\"color:rgb(98,98,98)\">.</span>join(data_dir, <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">xcitibike</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">      5</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">not</span> os<span style=\"color:rgb(98,98,98)\">.</span>path<span style=\"color:rgb(98,98,98)\">.</span>exists(extract_dir):\n<span class=\"ansi-green-fg\">----&gt; 6</span>     <span class=\"ansi-yellow-bg\">os</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">mkdir</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">extract_dir</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      8</span> parquet_dir <span style=\"color:rgb(98,98,98)\">=</span> os<span style=\"color:rgb(98,98,98)\">.</span>path<span style=\"color:rgb(98,98,98)\">.</span>join(data_dir, <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">pq_citibike</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">      9</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">not</span> os<span style=\"color:rgb(98,98,98)\">.</span>path<span style=\"color:rgb(98,98,98)\">.</span>exists(parquet_dir):\n\n<span class=\"ansi-red-fg\">FileNotFoundError</span>: [Errno 2] No such file or directory: '../data/xcitibike'</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#78d49d48 .cell execution_count=3}\n``` {.python .cell-code}\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.functions import pandas_udf\nfrom pyspark.sql.types import BooleanType\nfrom pyspark.sql.functions import PandasUDFType\n```\n:::\n\n\n::: {#19ed61e1 .cell execution_count=4}\n``` {.python .cell-code}\nspark = (SparkSession\n    .builder\n    .appName(\"Spark building citibike parquet file\")\n    .getOrCreate()\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n25/01/14 22:40:10 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 192.168.10.120 instead (on interface wlp0s20f3)\n25/01/14 22:40:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/01/14 22:40:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n```\n:::\n:::\n\n\n::: {#ffdc49cd .cell execution_count=5}\n``` {.python .cell-code}\nspark.sparkContext.setCheckpointDir(checkpoint_dir)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[5], line 1</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span> spark<span style=\"color:rgb(98,98,98)\">.</span>sparkContext<span style=\"color:rgb(98,98,98)\">.</span>setCheckpointDir(<span class=\"ansi-yellow-bg\">checkpoint_dir</span>)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'checkpoint_dir' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#4dcb9275 .cell execution_count=6}\n``` {.python .cell-code}\n@pandas_udf(BooleanType())\ndef detect_non_ISO(s: pd.Series) -> bool:\n    r = s.str.match(r\"\\d+/\\d+/\\d+\").any()\n    return r\n\n@pandas_udf(\"string\")\ndef make_iso(s: pd.Series) -> pd.Series:\n    t = s.str.split(' ', expand=True)\n    u = t[0].str.split('/')\n    v = u.map(lambda x  : [x[2], x[0], x[1]]).str.join('-')\n    w = v.combine(t[1], lambda x, y : ' '.join([x, y]))\n    return w\n```\n:::\n\n\n::: {#29dfd2a7 .cell execution_count=7}\n``` {.python .cell-code}\ndicts_rename = {\n    1: {\n 'tripduration': 'trip_duration',\n 'starttime': 'started_at',\n 'stoptime': 'ended_at',\n 'bikeid': 'bike_id',\n 'usertype': 'user_type',\n 'start station latitude': 'start_lat',\n 'start station longitude': 'start_lng',\n 'end station latitude': 'end_lat',\n 'end station longitude': 'end_lng'\n} ,\n  2:  {\n 'Trip Duration': 'trip_duration',\n  'Start Time': 'started_at',\n  'Stop Time': 'ended_at',\n  'Start Station Latitude': 'start_lat',\n  'Start Station Longitude': 'start_lng',\n  'End Station Latitude': 'end_lat',\n  'End Station Longitude': 'end_lng'    \n}\n}\n```\n:::\n\n\n::: {#24623fbe .cell execution_count=8}\n``` {.python .cell-code}\nfor (root, dirs ,files) in tqdm(os.walk(extract_dir, topdown=True)):\n    if dirs:\n        continue\n\n    for flnm in files:\n        if not flnm.endswith('.csv'):  \n            continue\n\n        fpath = os.path.join(root, flnm)\n        df = spark.read.option(\"header\",\"true\").csv(fpath)\n\n        df = (\n            df.withColumnsRenamed(dicts_rename[1])\n            .withColumnsRenamed(dicts_rename[2])\n        )\n\n        df = df.toDF(*[c.replace(' ','_').lower() for c in df.columns])\n\n        if re.match(r\"\\d+/\\d+/\\d+\", df.select(\"started_at\").first()[0]):\n            df = (\n                   df\n                    .withColumn('started_at', make_iso(fn.col(\"started_at\")))\n                    .withColumn('ended_at', make_iso(fn.col(\"ended_at\")))\n            ) \n\n        df = df.withColumns(\n                {\n                'started_at': fn.to_timestamp(fn.col(\"started_at\")),\n                'ended_at': fn.to_timestamp(fn.col(\"ended_at\"))\n                }\n            )   \n\n        df = df.withColumns(\n                {\n                    'start_year': fn.year(fn.col('started_at')),\n                    'start_month': fn.month(fn.col('ended_at'))\n                }\n            )\n\n        df.checkpoint(eager=True)\n\n        # df.printSchema()\n\n        df.write.parquet(\n            parquet_dir, \n            partitionBy=['start_year', 'start_month'], \n            mode=\"append\"\n        )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n```\n:::\n:::\n\n\n``` \nsch_1 = StructType(\n    [\n        StructField('trip_duration', StringType(), True), \n        StructField('started_at', TimestampType(), True), \n        StructField('ended_at', TimestampType(), True), \n        StructField('start_station_id', StringType(), True), \n        StructField('start_station_name', StringType(), True), \n        StructField('start_lat', StringType(), True), \n        StructField('start_lng', StringType(), True), \n        StructField('end_station_id', StringType(), True), \n        StructField('end_station_name', StringType(), True), \n        StructField('end_lat', StringType(), True), \n        StructField('end_lng', StringType(), True), \n        StructField('bike_id', StringType(), True), \n        StructField('user_type', StringType(), True), \n        StructField('birth_year', StringType(), True), \n        StructField('gender', StringType(), True), \n        StructField('start_year', IntegerType(), True), \n        StructField('start_month', IntegerType(), True)\n    ])\n```\n\n::: {#e2e3a50d .cell execution_count=9}\n``` {.python .cell-code}\n# spark.stop()\n```\n:::\n\n\n## References\n\n[Python vectorized string computations](https://jakevdp.github.io/PythonDataScienceHandbook/03.10-working-with-strings.html)\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: /usr/share/jupyter/kernels/python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.12.3\n---\n",
    "supporting": [
      "xcitibike_spark_files"
    ],
    "filters": []
  }
}