{
  "hash": "7ecb9a7d70d8b8d572e5216c9c286b5b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Using `JSON` data with `Python`\njupyter: python3\nexecute: \n  eval: false\n---\n\n\n\nThis notebook is concerned with `JSON` a format that serves many purposes. Just as `csv` files, `json` files are important sources and sinks for Spark. As a exchange format, `JSON` is also a serialization tool for Python  and many other languages. `JSON` provides a way to accomodate *semi-structured* data in otherwise tabular environments (dataframes and databases tables). \n\nThe notebook is organized in the following way:\n\n- Serialization/Deserialization of Python builtin types using `JSON`\n- Serialization/Deserialization of (some) custom types using `JSON`\n- `JSON` readers and writers for Spark dataframes\n- Composite types in Spark dataframes\n- Advanced `JSON` readers and writers for Spark dataframes\n\n\n## Serialization and deserialization of built-in types\n\n::: {#9601fc8e .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:25.759874Z\",\"start_time\":\"2020-03-17T14:29:25.726787Z\"}' execution_count=1}\n``` {.python .cell-code}\nimport json\n\nobj = {\n    \"name\": \"Foo Bar\",\n    \"age\": 78,\n    \"friends\": [\"Jane\",\"John\"],\n    \"balance\": 345.80,\n    \"other_names\":(\"Doe\",\"Joe\"),\n    \"active\": True,\n    \"spouse\": None\n}\n\nprint(json.dumps(obj, sort_keys=True, indent=4))\n```\n:::\n\n\n::: {.callout-note}\n\n`json.dumps()` outputs a `JSON` formatted string. \n\nNot every type of object can be fed to `json.dumps()`. Licit types are:\n\n- \n\n:::\n\n::: {#1cacd936 .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:26.039839Z\",\"start_time\":\"2020-03-17T14:29:26.027858Z\"}' execution_count=2}\n``` {.python .cell-code}\nwith open('user.json','w') as file:\n    json.dump(obj, file, sort_keys=True, indent=4)\n```\n:::\n\n\n::: {#ea01d0c7 .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:27.110218Z\",\"start_time\":\"2020-03-17T14:29:26.479550Z\"}' execution_count=3}\n``` {.python .cell-code}\n!cat user.json\n```\n:::\n\n\n::: {#348e68b1 .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:27.137307Z\",\"start_time\":\"2020-03-17T14:29:27.114179Z\"}' execution_count=4}\n``` {.python .cell-code}\njson.loads('{\"active\": true, \"age\": 78, \"balance\": 345.8, \"friends\": [\"Jane\",\"John\"], \"name\": \"Foo Bar\", \"other_names\": [\"Doe\",\"Joe\"],\"spouse\":null}')\n```\n:::\n\n\n::: {#f73450db .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:27.149816Z\",\"start_time\":\"2020-03-17T14:29:27.140548Z\"}' execution_count=5}\n``` {.python .cell-code}\nwith open('user.json', 'r') as file:\n    user_data = json.load(file)\n\nprint(user_data)\n```\n:::\n\n\n::: {.callout}\n\nWhat happens if we feed `json.dumps()` with a `numpy` array? \n\n:::\n\n## Serialization and deserialization of custom objects\n\n::: {#4452f2bf .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:27.733203Z\",\"start_time\":\"2020-03-17T14:29:27.718399Z\"}' execution_count=6}\n``` {.python .cell-code}\nclass User(object):\n    \"\"\"Custom User Class\n    \"\"\"\n    def __init__(self, name, age, active, balance, \n                 other_names, friends, spouse):\n        self.name = name\n        self.age = age\n        self.active = active\n        self.balance = balance\n        self.other_names = other_names\n        self.friends = friends\n        self.spouse = spouse\n            \n    def __repr__(self):\n        s = \"User(\"\n        s += \"name=\" + repr(self.name)\n        s += \", age=\" + repr(self.age)\n        s += \", active=\" + repr(self.active)\n        s += \", other_names=\" + repr(self.other_names)\n        s += \", friends=\" + repr(self.friends)\n        s += \", spouse=\" + repr(self.spouse) + \")\"\n        return s\n```\n:::\n\n\n::: {#9a3882a4 .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:27.733203Z\",\"start_time\":\"2020-03-17T14:29:27.718399Z\"}' execution_count=7}\n``` {.python .cell-code}\nnew_user = User(\n    name = \"Foo Bar\",\n    age = 78,\n    friends = [\"Jane\", \"John\"],\n    balance = 345.80,\n    other_names = (\"Doe\", \"Joe\"),\n    active = True,\n    spouse = None\n)\n\nnew_user\n```\n:::\n\n\n::: {#a72b5f8b .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:49.613442Z\",\"start_time\":\"2020-03-17T14:29:49.601102Z\"}' execution_count=8}\n``` {.python .cell-code}\n# This will raise a TypeError\n# json.dumps(new_user)\n```\n:::\n\n\nAs expected, the custom object `new_user` is not JSON serializable. So let's build a method that does that for us.\n\n- This comes as no surprise to us, since earlier on we observed that\nthe `json` module only handles the built-in types, and `User` is not one.\n\n- We need to send our user data to a client over a network, so how do we get \nourselves out of this error state?\n\n- A simple solution would be to convert our custom type into a serializable\ntype that is a built-in type. We can conveniently define a method `convert_to_dict()`\nthat returns a dictionary representation of our object. `json.dumps()` \ntakes in a optional argument, `default`, which specifies a function to be called if the object is not serializable. This function returns a JSON encodable version of the object.\n\nRecall that class `obj` has a dunder method `__dict__` that provides a basis for obtaining a dictionary with the attributes of any object:\n\n::: {#76692109 .cell execution_count=9}\n``` {.python .cell-code}\nnew_user.__dict__\n```\n:::\n\n\n::: {#3f74bba4 .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:52.854067Z\",\"start_time\":\"2020-03-17T14:29:52.847015Z\"}' execution_count=10}\n``` {.python .cell-code}\ndef obj_to_dict(obj):\n    \"\"\"Converts an object to a dictionary representation of the object including \n    meta-data information about the object's module and class name.\n\n    Parameters\n    ----------\n    obj : `object`\n        A python object to be converted into a dictionary representation\n\n    Returns\n    -------\n    output : `dict`\n        A dictionary representation of the object\n    \"\"\"\n    # Add object meta data \n    obj_dict = {\n        \"__class__\": obj.__class__.__name__,\n        \"__module__\": obj.__module__\n    }\n    # Add the object properties\n    return obj_dict | obj.__dict__\n```\n:::\n\n\n::: {#a3944c1b .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:52.854067Z\",\"start_time\":\"2020-03-17T14:29:52.847015Z\"}' execution_count=11}\n``` {.python .cell-code}\nobj_to_dict(new_user)\n```\n:::\n\n\nThe function `convert_to_dict` does the following:\n\n- create a dictionary named `obj_dict` to act as the dict representation of our object.\n\n- magic methods `__class__.__name__` and `__module__` provide crucial metadata on the object: the class name and the module name\n\n  add the instance attributes of the object using `obj.__dict__` (`Python` stores instance attributes in a dictionary under the hood)\n\n- The resulting `obj_dict` is now serializable (provided all attributes of our object are).\n\nNow we can comfortably call `json.dumps()` on the object and pass `default=convert_to_dict`\n\n::: {.callout-note}\n\nObviously this fails if one of the attributes is not `JSON` serializable\n\n:::\n\n::: {#42d39833 .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:53.566068Z\",\"start_time\":\"2020-03-17T14:29:53.560201Z\"}' execution_count=12}\n``` {.python .cell-code}\nprint(json.dumps(new_user, default=obj_to_dict, indent=4, sort_keys=True))\n```\n:::\n\n\nNow, if we want to decode (deserialiaze) a custom object, and create the correct object type, we need a function that does the opposite of convert_to_dict, since `json.loads` simply returns a `dict`:\n\n::: {#64813f4c .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:54.413286Z\",\"start_time\":\"2020-03-17T14:29:54.408660Z\"}' execution_count=13}\n``` {.python .cell-code}\nuser_data = json.loads(json.dumps(new_user, default=obj_to_dict))\nprint(user_data)\n```\n:::\n\n\nWe need `json.loads()` to reconstruct a `User` object from this dictionary: `json.loads()`\ntakes an optional argument `object_hook` which specifies a function that returns the desired custom object, given the decoded output (which in this case is a `dict`).\n\n::: {#b3715c86 .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:55.257839Z\",\"start_time\":\"2020-03-17T14:29:55.253496Z\"}' execution_count=14}\n``` {.python .cell-code}\ndef dict_to_obj(input_dict):\n    \"\"\"Converts a dictionary representation of an object to an instance of the object.\n\n    Parameters\n    ----------\n    input_dict : `dict`\n        A dictionary representation of the object, containing \"__module__\" \n        and \"__class__\" metadata\n\n    Returns\n    -------    \n    obj : `object`\n        A python object constructed from the dictionary representation    \n    \"\"\"\n    assert \"__class__\" in input_dict and \"__module__\" in input_dict\n    class_name = input_dict.pop(\"__class__\")\n    module_name = input_dict.pop(\"__module__\")\n    module = __import__(module_name)\n    class_ = getattr(module, class_name)\n    obj = class_(**input_dict)\n    return obj\n```\n:::\n\n\nThis function does the following: \n\n- Extract the class name from the dictionary under the key `__class__`\n\n- Extract the module name from the dictionary under the key `__module__`\n\n- Imports the module and get the class\n\n- Instantiate the class by giving to the class constructor all the instance arguments through dictionary unpacking\n\n::: {#ca728fab .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:57.102787Z\",\"start_time\":\"2020-03-17T14:29:57.097553Z\"}' execution_count=15}\n``` {.python .cell-code}\nobj_data = json.dumps(new_user, default=obj_to_dict)\nnew_object = json.loads(obj_data, object_hook=dict_to_obj)\nnew_object\n```\n:::\n\n\n::: {#fadc2e8c .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:57.944835Z\",\"start_time\":\"2020-03-17T14:29:57.939581Z\"}' execution_count=16}\n``` {.python .cell-code}\ntype(new_object)\n```\n:::\n\n\n::: {#6526be6e .cell ExecuteTime='{\"end_time\":\"2020-03-17T14:29:58.679143Z\",\"start_time\":\"2020-03-17T14:29:58.673617Z\"}' execution_count=17}\n``` {.python .cell-code}\nnew_object.age\n```\n:::\n\n\n::: {.callout-note}\n\nFunctions `obj_to_dict()` and `dict_to_obj()`  are showcases for special/magic/dunder methods.\n\nIn the definition of class `User`, two special methods were explicitly defined: `__init__` and `__repr__`. But many more are available, including `__dir__()`. \n\n:::\n\n::: {#81df90e8 .cell execution_count=18}\n``` {.python .cell-code}\n[dude for dude in dir(new_object) if dude.startswith('__')]\n```\n:::\n\n\n::: {#c037fb8c .cell execution_count=19}\n``` {.python .cell-code}\nnew_object.__getattribute__('age')\n```\n:::\n\n\n::: {.callout-note}\n\nClass `User` could have been implemented as a `dataclass`\n\n:::\n\n::: {#d801e763 .cell execution_count=20}\n``` {.python .cell-code}\nfrom dataclasses import dataclass\n\n@dataclass\nclass UserBis(object):\n    \"\"\"Custom User Class\n    \"\"\"\n    name: str \n    age: int\n    active: bool\n    balance: float\n    other_names: list[str]\n    friends: list[str]\n    spouse: str\n```\n:::\n\n\n::: {#17cafff7 .cell execution_count=21}\n``` {.python .cell-code}\nother_user = UserBis(**(new_user.__dict__))\n```\n:::\n\n\n::: {#babcf984 .cell execution_count=22}\n``` {.python .cell-code}\nrepr(other_user)\n```\n:::\n\n\n::: {#55e0b579 .cell execution_count=23}\n``` {.python .cell-code}\n{dude for dude in dir(other_user) if dude.startswith('__')} -  {dude for dude in dir(new_user) if dude.startswith('__')}\n```\n:::\n\n\n# Using `JSON` with Spark\n\nFirst, we download the data if it's not there yet\n\n::: {#fbcd313f .cell ExecuteTime='{\"end_time\":\"2020-03-17T16:19:34.238130Z\",\"start_time\":\"2020-03-17T16:19:34.163729Z\"}' execution_count=24}\n``` {.python .cell-code}\nimport requests, zipfile, io\nfrom pathlib import Path\n\npath = Path('drug-enforcement.json')\nif not path.exists():\n    url = \"https://stephanegaiffas.github.io/big_data_course/data/drug-enforcement.json.zip\"\n    r = requests.get(url)\n    z = zipfile.ZipFile(io.BytesIO(r.content))\n    z.extractall(path='./')\n```\n:::\n\n\n::: {#da1dec32 .cell execution_count=25}\n``` {.python .cell-code}\n!ls drug*\n```\n:::\n\n\n## Reading a `JSON` dataset with `Spark`\n\n::: {#45a8afeb .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:13:47.583384Z\",\"start_time\":\"2020-03-17T17:13:43.123090Z\"}' execution_count=26}\n``` {.python .cell-code}\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.functions import col\n\nspark = (SparkSession\n    .builder\n    .appName(\"Spark JSON\")\n    .getOrCreate()\n)\n\nsc = spark._sc\n```\n:::\n\n\n::: {#fa34114a .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:13:47.590453Z\",\"start_time\":\"2020-03-17T17:13:47.585590Z\"}' execution_count=27}\n``` {.python .cell-code}\nfilename = \"drug-enforcement.json\"\n```\n:::\n\n\nFirst, lets look at the data. It's a large set of JSON records about drugs enforcement.\n\n::: {#76f0c876 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:13:48.148481Z\",\"start_time\":\"2020-03-17T17:13:47.594201Z\"}' execution_count=28}\n``` {.python .cell-code}\n!head -n 1000 drug-enforcement.json\n```\n:::\n\n\nWe need to tell spark that rows span on several lines with the `multLine` option\n\n::: {#32a58f81 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:13:52.259661Z\",\"start_time\":\"2020-03-17T17:13:48.152192Z\"}' execution_count=29}\n``` {.python .cell-code}\ndf = spark.read.json(filename, multiLine=True)\n```\n:::\n\n\n::: {#d3b72b0f .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:13:52.294898Z\",\"start_time\":\"2020-03-17T17:13:52.261550Z\"}' execution_count=30}\n``` {.python .cell-code}\ndf.printSchema()\n```\n:::\n\n\n::: {#d004cee7 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:13:52.355556Z\",\"start_time\":\"2020-03-17T17:13:52.297210Z\"}' execution_count=31}\n``` {.python .cell-code}\ndf.schema\n```\n:::\n\n\n::: {.callout-note}\n\nThis dataset is a little bit of a mess! \n\nThis should not be surprising. The data used to populate the Spark dataframe are not classically tabular but what people call *semi-structured*. Json is well-suited to store, represent, and exchange such data. \n\nIn the classical age of tabular data (according to Codd's principles), a table cell could only hold a scalar value (numeric, logical, text, date, timestamp, ...), nowadays Relational Database Management Systems handle Arrays, Composite Types, Range Types, ..., and Json (see [PostgreSQL](https://www.postgresql.org/docs/current/datatype-json.html)). \n\nSpark, `R`, and `Pandas` also allow us to work with complex types. \n\n:::\n\n- First, there is a nested `opendfa` dictionary. Each element of the dictionary is an array\n- A first good idea is to **\"flatten\" the schema of the DataFrame**, so that there is no nested types\n\n## Flattening of the schema\n\nAll the columns in the nested structure `openfda` are put up in the schema. These columns nested in the openfda are as follows:\n\n::: {#d90b7114 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:14:00.472548Z\",\"start_time\":\"2020-03-17T17:14:00.352058Z\"}' execution_count=32}\n``` {.python .cell-code}\ndf.select('openfda.*').columns\n```\n:::\n\n\n::: {#d6e48921 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:14:01.904242Z\",\"start_time\":\"2020-03-17T17:14:00.646360Z\"}' execution_count=33}\n``` {.python .cell-code}\ndf.select(\"openfda.*\").head(2)\n```\n:::\n\n\n::: {#41d9609a .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:14:02.230811Z\",\"start_time\":\"2020-03-17T17:14:01.906641Z\"}' execution_count=34}\n``` {.python .cell-code}\nfor c in df.select(\"openfda.*\").columns:\n    df = df.withColumn(\"openfda_\" + c, col(\"openfda.\" + c))\n```\n:::\n\n\n::: {#e0921654 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:14:07.644985Z\",\"start_time\":\"2020-03-17T17:14:07.501380Z\"}' execution_count=35}\n``` {.python .cell-code}\ndf = df.select([c for c in df.columns if c != \"openfda\"])\n```\n:::\n\n\n::: {#ab71121b .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:14:09.871658Z\",\"start_time\":\"2020-03-17T17:14:09.865537Z\"}' execution_count=36}\n``` {.python .cell-code}\ndf.printSchema()\n```\n:::\n\n\n::: {#e8709d5d .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:14:19.204515Z\",\"start_time\":\"2020-03-17T17:14:18.651165Z\"}' execution_count=37}\n``` {.python .cell-code}\ndf.head(2)\n```\n:::\n\n\nNote that the display of the `DataFrame` is not as usual... it displays the dataframe like a list of `Row`, since the columns \"openfda*\" contain arrays of varying length\n\n## Missing data\n\nA strategy can be to remove rows with missing data. \n`dropna` has several options, explained below.\n\n::: {#01fc5ecd .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:41:17.413952Z\",\"start_time\":\"2020-03-17T17:41:16.898211Z\"}' execution_count=38}\n``` {.python .cell-code}\ndf.dropna().count()\n```\n:::\n\n\nIf we remove all lines with at least one missing value, we end up with an empty dataframe !\n\n::: {#e53659a4 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:41:17.895108Z\",\"start_time\":\"2020-03-17T17:41:17.416108Z\"}' execution_count=39}\n``` {.python .cell-code}\ndf.dropna(how='all').count()\n```\n:::\n\n\n`dropna` accepts the following arguments\n\n- `how`: can be `'any'` or `'all'`. If `'any'`, rows containing any null values will be dropped entirely (this is the default). If `'all'`, only rows which are entirely empty will be dropped.\n\n- `thresh`: accepts an integer representing the \"threshold\" for how many empty cells a row must have before being dropped. `tresh` is a middle ground between `how='any'` and `how='all'`. As a result, the presence of `thresh` will override `how`\n\n- `subset`: accepts a list of column names. When a subset is present, N/A values will only be checked against the columns whose names are provided.\n\n::: {#e9f29639 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:41:17.901172Z\",\"start_time\":\"2020-03-17T17:41:17.897860Z\"}' execution_count=40}\n``` {.python .cell-code}\nn_columns = len(df.columns)\n```\n:::\n\n\n::: {#a9dfee54 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:41:18.415066Z\",\"start_time\":\"2020-03-17T17:41:17.904093Z\"}' execution_count=41}\n``` {.python .cell-code}\ndf.dropna(thresh=n_columns).count()\n```\n:::\n\n\n::: {#8cd739e2 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:41:18.931320Z\",\"start_time\":\"2020-03-17T17:41:18.417317Z\"}' execution_count=42}\n``` {.python .cell-code}\ndf.dropna(thresh=n_columns-1).count()\n```\n:::\n\n\n::: {#900ee455 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:41:19.439726Z\",\"start_time\":\"2020-03-17T17:41:18.934217Z\"}' execution_count=43}\n``` {.python .cell-code}\ndf.dropna(thresh=n_columns-10).count()\n```\n:::\n\n\n::: {#d22ba8e1 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:41:19.752318Z\",\"start_time\":\"2020-03-17T17:41:19.441809Z\"}' execution_count=44}\n``` {.python .cell-code}\ndf = df.dropna(subset=['postal_code', 'city', 'country', 'address_1'])\ndf.count()\n```\n:::\n\n\nBut before this, let's count the number of missing value for each column\n\n::: {#691de5f9 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:41:20.424178Z\",\"start_time\":\"2020-03-17T17:41:19.754255Z\"}' execution_count=45}\n``` {.python .cell-code}\n# For each column we create a new column containing 1 if the value is null and 0 otherwise.\n# We need to bast Boolean to Int so that we can use fn.sum after\nfor c in df.columns:\n    # Do not do this for _isnull columns (ince case you run this cell twice...)\n    if not c.endswith(\"_isnull\"):\n        df = df.withColumn(c + \"_isnull\", fn.isnull(col(c)).cast('int'))\n```\n:::\n\n\n::: {#39297fba .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:41:20.925299Z\",\"start_time\":\"2020-03-17T17:41:20.426204Z\"}' execution_count=46}\n``` {.python .cell-code}\ndf.head()\n```\n:::\n\n\n::: {#e2833a2b .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:41:22.053794Z\",\"start_time\":\"2020-03-17T17:41:20.927796Z\"}' execution_count=47}\n``` {.python .cell-code}\n# Get the list of _isnull columns\nisnull_columns = [c for c in df.columns if c.endswith(\"_isnull\")]\n\n# On the _isnull columns :\n#  - we compute the sum to have the number of null values and rename the column\n#  - convert to pandas for better readability\n#  - transpose the pandas dataframe for better readability\nmissing_values = df.select(isnull_columns)\\\n    .agg(*[fn.sum(c).alias(c.replace(\"_isnull\", \"\")) for c in isnull_columns])\\\n    .toPandas()\n\nmissing_values.T\\\n    .rename({0: \"missing values\"}, axis=\"columns\")\n```\n:::\n\n\nWe see that `more_code_info` is always null and that `termination_date` if often null. \nMost of the `openfda*` columns are also almost always empty.\n\nWe can keep only the columns with no missing values\n\n::: {#36fb2118 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:41:24.802336Z\",\"start_time\":\"2020-03-17T17:41:24.797411Z\"}' execution_count=48}\n``` {.python .cell-code}\n# This line can seem complicated, run pieces of each to understand\nkept_columns = list(\n    missing_values.columns[(missing_values.iloc[0] == 0).values]\n)\n```\n:::\n\n\n::: {#75f339c9 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:41:27.488388Z\",\"start_time\":\"2020-03-17T17:41:27.398503Z\"}' execution_count=49}\n``` {.python .cell-code}\ndf_kept = df.select(kept_columns)\n```\n:::\n\n\n::: {#15aee724 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:41:27.948018Z\",\"start_time\":\"2020-03-17T17:41:27.539797Z\"}' execution_count=50}\n``` {.python .cell-code}\ndf_kept.head(2)\n```\n:::\n\n\n::: {#d897ecd2 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:41:41.419929Z\",\"start_time\":\"2020-03-17T17:41:41.413784Z\"}' execution_count=51}\n``` {.python .cell-code}\ndf_kept.printSchema()\n```\n:::\n\n\n::: {#f4dcc6cc .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:43:23.207185Z\",\"start_time\":\"2020-03-17T17:43:22.829140Z\"}' execution_count=52}\n``` {.python .cell-code}\ndf_kept.count()\n```\n:::\n\n\n## Filtering by string values \n\nCases from South San Francisco\n\n::: {#e2f79e83 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:43:54.396689Z\",\"start_time\":\"2020-03-17T17:43:53.465012Z\"}' execution_count=53}\n``` {.python .cell-code}\ndf.filter(df.city == \"South San Francisco\")\\\n    .toPandas()\n```\n:::\n\n\n**Remark.** Once again, we use `.toPandas()` to pretty format the results in the notebook. \nBut it's a BAD idea to do this if the spark DataFrame is large, since it requires a `collect()`\n\nAside from filtering strings by a perfect match, there are plenty of other powerful ways to filter by strings in `pyspark` :\n\n- `df.filter(df.city.contains('San Francisco'))`: returns rows where strings of a column contain a provided substring. In our example, filtering by rows which contain the substring \"San Francisco\" would be a good way to get all rows in San Francisco, instead of just \"South San Francisco\".\n\n- `df.filter(df.city.startswith('San'))`: Returns rows where a string starts with a provided substring.\n\n- `df.filter(df.city.endswith('ice'))`: Returns rows where a string starts with a provided substring.\n\n- `df.filter(df.city.isNull())`: Returns rows where values in a provided column are null.\n\n- `df.filter(df.city.isNotNull())`: Opposite of the above.\n\n- `df.filter(df.city.like('San%'))`: Performs a SQL-like query containing the LIKE clause.\n\n- `df.filter(df.city.rlike('[A-Z]*ice$'))`: Performs a regexp filter.\n\n- `df.filter(df.city.isin('San Francisco', 'Los Angeles'))`: Looks for rows where the string value of a column matches any of the provided strings exactly.\n\nYou can try some of these to understand\n\n::: {#d4ead91b .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:48:19.328007Z\",\"start_time\":\"2020-03-17T17:48:18.572570Z\"}' execution_count=54}\n``` {.python .cell-code}\ndf.filter(df.city.contains('San Francisco'))\\\n    .toPandas()\n```\n:::\n\n\n::: {#b9ec97a2 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:48:10.490019Z\",\"start_time\":\"2020-03-17T17:48:09.784496Z\"}' execution_count=55}\n``` {.python .cell-code}\ndf.filter(df.city.isin('San Francisco', 'Los Angeles')).toPandas()\n```\n:::\n\n\n## Filtering by Date Values\n\nIn addition to filtering by strings, we can also filter by columns where the values are stored as dates or datetimes (or strings that can be inferred as dates). Perhaps the most useful way to filter dates is by using the `between()` method, which allows us to find results within a certain date range. Here we find all the results which were reported in the years 2013 and 2014:\n\n::: {#8c8e9f57 .cell ExecuteTime='{\"end_time\":\"2020-03-17T17:50:34.344304Z\",\"start_time\":\"2020-03-17T17:50:33.638867Z\"}' execution_count=56}\n``` {.python .cell-code}\ndf.filter(df.city == \"South San Francisco\")\\\n    .filter(df.report_date.between('2013-01-01 00:00:00','2015-03-11 00:00:00'))\\\n    .toPandas()\n```\n:::\n\n\n::: {.callout-caution}\n\nIs Spark smart enough to understand that the string in column `report_date` contains a date?\n\n:::\n\n::: {#8a20cc54 .cell execution_count=57}\n``` {.python .cell-code}\ndf.filter(df.city == \"South San Francisco\")\\\n    .filter(df.center_classification_date.between('2013-01-01 00:00:00','2013-12-31 00:00:00'))\\\n    .toPandas()\n```\n:::\n\n\n::: {#2091fdb1 .cell execution_count=58}\n``` {.python .cell-code}\ndf_dates = df.select([c for c in df.columns if c.endswith(\"date\")])\n\ndf_dates.printSchema()\n```\n:::\n\n\n::: {#bdc8042c .cell execution_count=59}\n``` {.python .cell-code}\ndf_dates.show(5)\n```\n:::\n\n\nColumns are not dates  (`DateType`) but strings (`StringType`). When comparing `report_date` \nwith `'2013-01-01 00:00:00'` and `'2015-03-11 00:00:00'`, we are comparing strings and are lucky enough that in unicode `'-' < '0' < '...' < '9'` so that `2013-....` is less that any string starting with `20130...`, while any string starting with `2013...` is less than any string starting with `2015..`. \n\n::: {.callout-caution}\n\nIf some field in a Jason string is meant to represent a date or a datetime object, spark should be given a hint. \n\nJson loaders (from `Python`) as well as the Spark Json reader have optional arguments that can be used to indicate the date parser to be used. \n\n:::\n\n\n## Handling complex types \n\nBridging the gap between tabular and semi-structured data. \n\n::: {.callout-note}\n\nSQL, `R`, `Pandas` ... \n\n:::\n\n`struct`, `array`, `map`\n\n::: {#939a08d4 .cell execution_count=60}\n``` {.python .cell-code}\n# struct\n```\n:::\n\n\nThe problems we faced after loading data from the json file pertained to the fact that column `fda` was of complex `StrucType()` type. We shall revisit this dataframe. \n\n::: {#6027a99b .cell execution_count=61}\n``` {.python .cell-code}\ndf = spark.read.json(filename, multiLine=True)\n```\n:::\n\n\nThe dataframe schema `df.schema` which is of type `StructType` (defined in `pyspark.sql.types`) can be converted to a json string which in turn can be converted into a Python dictionary. \n\n::: {#90780afe .cell execution_count=62}\n``` {.python .cell-code}\ndf = spark.read.json(filename, multiLine=True)\n\nsj = json.loads(df.schema.json())\n```\n:::\n\n\nWe equip the dataframe with a primary key \n\n::: {#f9e35fd5 .cell execution_count=63}\n``` {.python .cell-code}\nfrom pyspark.sql import Window\n\nw = Window.orderBy(col(\"center_classification_date\"))\n\ndf = (\n  df\n    .withColumn(\"row_id\", fn.row_number()\n    .over(w))\n)\n```\n:::\n\n\n::: {#8358c823 .cell execution_count=64}\n``` {.python .cell-code}\n[(f['name'], f['type']) \n for f in sj['fields'] if not isinstance(f['type'], str)]\n```\n:::\n\n\nColumn `openfda` has type `StrucType()` with fields with composite type.\n\n::: {#57510667 .cell execution_count=65}\n``` {.python .cell-code}\n{f['type']['type']\n for f in sj['fields'] if not isinstance(f['type'], str)}\n```\n:::\n\n\nProjecting on `row_id`  and `openfda.*` leads to a (partially) flattened datafame, that, thanks to the `row_id` column can be joined with the original dataframe. \n\n::: {#7925b305 .cell execution_count=66}\n``` {.python .cell-code}\ndf_proj = df.select('row_id', 'openfda.*')\n\ndf_proj.printSchema()\n```\n:::\n\n\nWe can inspect the length of the arrays. \n\n::: {#fbf30751 .cell execution_count=67}\n``` {.python .cell-code}\n# array\ndf_proj.select(\n    fn.max(fn.size(col(\"application_number\"))).alias(\"Max\"), \n    fn.min(fn.size(col(\"application_number\"))).alias(\"min\"), \n    fn.avg(fn.size(col(\"application_number\"))).alias(\"Mean\")).show(1)\n```\n:::\n\n\nIn some rows, the *size* of the array is `-1` because the field is `NULL`.\n\n::: {#cd2888f5 .cell execution_count=68}\n``` {.python .cell-code}\n(\n  df_proj\n    .where(fn.size(col(\"application_number\"))>1)\n    .select(\"row_id\")\n    .show(5)\n)\n```\n:::\n\n\nAn `array` column can be *exploded*. This is like pivoting into long form. The result contains one row per item in the array. \n\n::: {#63b252e7 .cell execution_count=69}\n``` {.python .cell-code}\n(\n  df_proj\n    .select('row_id', 'application_number')\n    .withColumn(\"exploded\", fn.explode(col(\"application_number\")))\n    .select('row_id', 'exploded')\n    .groupBy('row_id')\n    .agg(fn.count('exploded').alias(\"n_lignes\"))\n    .where(\"n_lignes > 1\")\n    .show(5)\n)\n```\n:::\n\n\n",
    "supporting": [
      "notebook07_json-format_files"
    ],
    "filters": [],
    "includes": {}
  }
}