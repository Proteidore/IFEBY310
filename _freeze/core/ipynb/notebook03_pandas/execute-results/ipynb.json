{
  "hash": "002e3bcb25adc6e201b83cc769b4504e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Introduction to `pandas`\njupyter: python3\n---\n\n\n\nThe `pandas` library (https://pandas.pydata.org) is one of the most used tool at the disposal of people working with data in `python` today.\n\n- It allows to **crunch data** easily\n- It mainly provides a `DataFrame` object (a **table of data**) with a huge set of functionalities\n\n\n## Why ?\n\nThrough `pandas`, you get acquainted with your data by **analyzing** it \n\n- What's the average, median, max, or min of each column?\n- Does column A correlate with column B?\n- What does the distribution of data in column C look like?\n\n## Why  (con't) ?\n\nyou get acquainted with your data by **cleaning** and  **transforming** it \n\n- Removing missing values, filter rows or columns using some criteria\n- Store the cleaned, transformed data back into virtually any format or database\n- Data visualization (when combined `matplotlib` or `seaborn` or others)\n\n## Where ?\n\n`pandas` is a central component of the `python` \"stack\" for data science\n\n- `pandas` is built on top of `numpy`\n- often used in conjunction with other libraries\n- a `DataFrame` is often fed to plotting functions or machine learning algorithms (such as `scikit-learn`)\n- Well-interfaced with `jupyter`, leading to a nice interactive environment for data exploration and modeling\n\n## Core components of pandas\n\nThe two primary components of pandas are the `Series` and `DataFrame`.\n\n- A `Series` is essentially a column\n\n- A `DataFrame` is a multi-dimensional table made up of a collection of `Series` with equal length\n\n## Creating a `DataFrame` from scratch\n\n::: {#c73c9492 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\nfruits = {\n    \"apples\": [3, 2, 0, 1],\n    \"oranges\": [0, 3, 7, 2]\n}\n\ndf_fruits = pd.DataFrame(fruits)\ndf_fruits\n```\n:::\n\n\n::: {#5e03366c .cell execution_count=2}\n``` {.python .cell-code}\ntype(df_fruits)\n```\n:::\n\n\n::: {#9533ef7b .cell execution_count=3}\n``` {.python .cell-code}\ndf_fruits[\"apples\"]\n```\n:::\n\n\n::: {#9c3319fd .cell execution_count=4}\n``` {.python .cell-code}\ntype(df_fruits[\"apples\"])\n```\n:::\n\n\n## Indexing\n\n- By default, a `DataFrame` uses a contiguous index\n- But what if we want to say **who** buys the fruits ?\n\n::: {#d06a4825 .cell execution_count=5}\n``` {.python .cell-code}\ndf_fruits = pd.DataFrame(fruits, index=[\"Daniel\", \"Sean\", \"Pierce\", \"Roger\"])\ndf_fruits\n```\n:::\n\n\n## `.loc` versus `.iloc`\n\n- `.loc` **loc**ates by name\n- `.iloc` **loc**ates by numerical **i**ndex\n\n::: {#511710ef .cell execution_count=6}\n``` {.python .cell-code}\ndf_fruits\n```\n:::\n\n\n::: {#3b4dfc76 .cell execution_count=7}\n``` {.python .cell-code}\n# What's in Sean's basket ?\ndf_fruits.loc['Sean']\n```\n:::\n\n\n::: {#0ee3867b .cell execution_count=8}\n``` {.python .cell-code}\n# Who has oranges ?\ndf_fruits.loc[:, 'oranges']\n```\n:::\n\n\n::: {#7f0dfca8 .cell execution_count=9}\n``` {.python .cell-code}\n# How many apples in Pierce's basket ?\ndf_fruits.loc['Pierce', 'apples']\n```\n:::\n\n\n::: {#aad20074 .cell execution_count=10}\n``` {.python .cell-code}\ndf_fruits\n```\n:::\n\n\n::: {#1aa00c36 .cell execution_count=11}\n``` {.python .cell-code}\ndf_fruits.iloc[2, 1]\n```\n:::\n\n\n## Main attributes and methods of a `DataFrame`\n\nA `DataFrame` has many **attributes**\n\n::: {#59d42837 .cell execution_count=12}\n``` {.python .cell-code}\ndf_fruits.columns\n```\n:::\n\n\n::: {#6fb5c644 .cell execution_count=13}\n``` {.python .cell-code}\ndf_fruits.index\n```\n:::\n\n\n::: {#3ff319f1 .cell execution_count=14}\n``` {.python .cell-code}\ndf_fruits.dtypes\n```\n:::\n\n\nA `DataFrame` has many **methods**\n\n::: {#880c03ca .cell execution_count=15}\n``` {.python .cell-code}\ndf_fruits.info()\n```\n:::\n\n\n::: {#0b6ad7c5 .cell execution_count=16}\n``` {.python .cell-code}\ndf_fruits.describe()\n```\n:::\n\n\n## Missing values\n\nWhat if we don't know how many apples are in Sean's basket ?\n\n::: {#a3179a74 .cell execution_count=17}\n``` {.python .cell-code}\ndf_fruits.loc['Sean', 'apples'] = None\ndf_fruits\n```\n:::\n\n\n::: {#ab744d6c .cell execution_count=18}\n``` {.python .cell-code}\ndf_fruits.describe()\n```\n:::\n\n\nNote that `count` is **3** for apples now, since we have 1 missing value among the 4\n\n\n::: {.callout-note}\n\nTo review the members of objects of class `pandas.DataFrame`, `dir()` and module `inspect` are convenient. \n:::\n\n::: {#484e4647 .cell execution_count=19}\n``` {.python .cell-code}\n[x for x in dir(df_fruits) if not x.startswith('_') and not callable(x)]\n```\n:::\n\n\n::: {#d62e0fab .cell execution_count=20}\n``` {.python .cell-code}\nimport inspect\n\n# Get a list of methods\nmembres = inspect.getmembers(df_fruits)\n\nmethod_names = [m[0] for m in membres \n    if callable(m[1]) and not m[0].startswith('_')]\n\nprint(method_names)\n```\n:::\n\n\n::: {#6689fc3f .cell execution_count=21}\n``` {.python .cell-code}\nothers = [x for x in membres\n    if not callable(x[1])]\n\n[x[0] for x in others if not x[0].startswith('_')]\n```\n:::\n\n\n## Adding a column\n\nOoooops, we forgot about the bananas !\n\n::: {#2de5acb3 .cell execution_count=22}\n``` {.python .cell-code}\ndf_fruits[\"bananas\"] = [0, 2, 1, 6]\ndf_fruits\n```\n:::\n\n\n## Adding a column with the date\n\nAnd we forgot the dates !\n\n::: {#e50e840f .cell execution_count=23}\n``` {.python .cell-code}\ndf_fruits['time'] = [\n    \"2020/10/08 12:13\", \"2020/10/07 11:37\", \n    \"2020/10/10 14:07\", \"2020/10/09 10:51\"\n]\ndf_fruits\n```\n:::\n\n\n::: {#103005d8 .cell execution_count=24}\n``` {.python .cell-code}\ndf_fruits.dtypes\n```\n:::\n\n\n::: {#6801df7f .cell execution_count=25}\n``` {.python .cell-code}\ntype(df_fruits.loc[\"Roger\", \"time\"])\n```\n:::\n\n\nIt is not a date but a string (`str`) ! So we convert this column to something called `datetime`  \n\n::: {#de6a6012 .cell execution_count=26}\n``` {.python .cell-code}\ndf_fruits[\"time\"] = pd.to_datetime(df_fruits[\"time\"])\ndf_fruits\n```\n:::\n\n\n::: {#6a1db2d6 .cell execution_count=27}\n``` {.python .cell-code}\ndf_fruits.dtypes\n```\n:::\n\n\n::: {.callout-note}\n\nEvery data science framework implements some `datetime` handling scheme. For Python see [Python official documentation on `datetime` module](https://docs.python.org/3/library/datetime.html#module-datetime)\n\n:::\n\nWhat if we want to keep only the baskets after (including) October, 9th ?\n\n::: {#3dc8f0af .cell execution_count=28}\n``` {.python .cell-code}\ndf_fruits.loc[df_fruits[\"time\"] >= pd.Timestamp(\"2020/10/09\")]\n```\n:::\n\n\n## Slices and subsets of rows or columns\n\n::: {#9ac53f00 .cell execution_count=29}\n``` {.python .cell-code}\ndf_fruits\n```\n:::\n\n\n::: {#80162d4a .cell execution_count=30}\n``` {.python .cell-code}\ndf_fruits.loc[:, \"oranges\":\"time\"]\n```\n:::\n\n\n::: {#ae024371 .cell execution_count=31}\n``` {.python .cell-code}\ndf_fruits.loc[\"Daniel\":\"Sean\", \"apples\":\"bananas\"]\n```\n:::\n\n\n::: {#06ec9583 .cell execution_count=32}\n``` {.python .cell-code}\ndf_fruits[[\"apples\", \"time\"]]\n```\n:::\n\n\n## Write our data to a CSV file\n\nWhat if we want to write the file ?\n\n::: {#4b790f37 .cell execution_count=33}\n``` {.python .cell-code}\ndf_fruits\n```\n:::\n\n\n::: {#efecf3ea .cell execution_count=34}\n``` {.python .cell-code}\ndf_fruits.to_csv(\"fruits.csv\")\n```\n:::\n\n\n::: {#c4eefc6c .cell execution_count=35}\n``` {.python .cell-code}\n# Use !dir on windows\n!ls -alh | grep fru\n```\n:::\n\n\n::: {#2ead65a6 .cell execution_count=36}\n``` {.python .cell-code}\n!head -n 5 fruits.csv\n```\n:::\n\n\n## Reading data and working with it\n\n\n\n::: {.callout-note}\n\nThe `tips` dataset comes through [Kaggle](https://www.kaggle.com/code/sanjanabasu/tips-dataset/input)\n\n> This dataset is a treasure trove of information from a collection of case studies for business statistics. Special thanks to Bryant and Smith for their diligent work:\n\n> Bryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing.\n\n> You can also access this dataset now through the Python package Seaborn.\n\n:::\n\nIt contains data about a restaurant: the bill, tip and some informations about the customers.\n\n::: {.callout-note}\n\n### A toy extraction pattern\n\nA data pipeline usually starts with Extraction, that is gathering data from some source, possibly in a galaxy far, far awy. Here follows a toy extraction pattern\n\n- obtain the data from some `URL` using package `requests`\n- save the data on the hard drive\n- load the data using Pandas \n\n::: {#2f01014b .cell execution_count=37}\n``` {.python .cell-code}\nimport requests\nimport os\n\n# The path containing your notebook\npath_data = './'\n# The name of the file\nfilename = 'tips.csv'\n\nif os.path.exists(os.path.join(path_data, filename)):\n    print('The file %s already exists.' % os.path.join(path_data, filename))\nelse:\n    url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/tips.csv'\n    r = requests.get(url)\n    with open(os.path.join(path_data, filename), 'wb') as f:\n        f.write(r.content)\n    print('Downloaded file %s.' % os.path.join(path_data, filename))\n```\n:::\n\n\n::: {#9b2eabce .cell execution_count=38}\n``` {.python .cell-code}\ndf = pd.read_csv(\n    \"tips.csv\", \n    delimiter=\",\"\n)\n```\n:::\n\n\n:::\n\nThe data can be obtained from package `seaborn`.\n\n::: {#a8cc17b9 .cell execution_count=39}\n``` {.python .cell-code}\nimport seaborn as sns\n\nsns_ds = sns.get_dataset_names()\n\n'tips' in sns_ds\n\ndf = sns.load_dataset('tips')\n```\n:::\n\n\n::: {#e40f5ea2 .cell execution_count=40}\n``` {.python .cell-code}\n# `.head()` shows the first rows of the dataframe\ndf.head(n=10)\n```\n:::\n\n\n::: {#00871375 .cell execution_count=41}\n``` {.python .cell-code}\ndf.info()\n```\n:::\n\n\n::: {#ab72731e .cell execution_count=42}\n``` {.python .cell-code}\ndf.loc[42, \"day\"]\n```\n:::\n\n\n::: {#2605e551 .cell execution_count=43}\n``` {.python .cell-code}\ntype(df.loc[42, \"day\"])\n```\n:::\n\n\nBy default, columns that are non-numerical contain strings (`str` type)\n\n## The `category` type\n\nAn important type in `pandas` is `category` for variables that are **non-numerical**\n\n**Pro tip.** It's always a good idea to tell `pandas` which columns should be imported as **categorical**\n\nSo, let's read again the file specifying some `dtype`s to the `read_csv` function\n\n::: {#93f9dcf4 .cell execution_count=44}\n``` {.python .cell-code}\ndtypes = {\n    \"sex\": \"category\",\n    \"smoker\": \"category\",\n    \"day\": \"category\",\n    \"time\": \"category\"\n} \n\ndf = pd.read_csv(\"tips.csv\", dtype=dtypes)\n```\n:::\n\n\n::: {#405e81f7 .cell execution_count=45}\n``` {.python .cell-code}\ndf.dtypes\n```\n:::\n\n\n## Computing statistics\n\n::: {#a5e422a0 .cell execution_count=46}\n``` {.python .cell-code}\n# The describe method only shows statistics for the numerical columns by default\ndf.describe()\n```\n:::\n\n\n::: {#36f976a4 .cell execution_count=47}\n``` {.python .cell-code}\n# We use the include=\"all\" option to see everything\ndf.describe(include=\"all\")\n```\n:::\n\n\n::: {#55fdb1d0 .cell execution_count=48}\n``` {.python .cell-code}\n# Correlation between the numerical columns\ndf.corr(numeric_only = True)\n```\n:::\n\n\n::: {#86460ea2 .cell execution_count=49}\n``` {.python .cell-code}\n?df.corr\n```\n:::\n\n\n# Data visualization with `matplotlib` and `seaborn`\n\nLet's show how we can use `matplotlib` and `seaborn` to visualize data contained in a `pandas` dataframe\n\n::: {#698a335f .cell execution_count=50}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n## How do the tip depends on the total bill ?\n\n::: {#41757b64 .cell execution_count=51}\n``` {.python .cell-code}\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=df)\n```\n:::\n\n\n## When do customers go to this restaurant ?\n\n::: {#a4e380ec .cell execution_count=52}\n``` {.python .cell-code}\nsns.countplot(x='day', hue=\"time\", data=df)\n```\n:::\n\n\n## When do customers spend the most ?\n\n::: {#5de9a765 .cell execution_count=53}\n``` {.python .cell-code}\nplt.figure(figsize=(7, 5))\nsns.boxplot(x='day', y='total_bill', hue='time', data=df)\nplt.legend(loc=\"upper left\")\n```\n:::\n\n\n::: {#d16d4e50 .cell execution_count=54}\n``` {.python .cell-code}\nplt.figure(figsize=(7, 5))\nsns.violinplot(x='day', y='total_bill', hue='time', split=True, data=df)\nplt.legend(loc=\"upper left\")\n```\n:::\n\n\n## Who spends the most ?\n\n::: {#76e05aae .cell execution_count=55}\n``` {.python .cell-code}\nsns.boxplot(x='sex', y='total_bill', hue='smoker', data=df)\n```\n:::\n\n\n## When should waiters want to work ?\n\n::: {#03987b88 .cell execution_count=56}\n``` {.python .cell-code}\nsns.boxplot(x='day', y='tip', hue='time', data=df)\n```\n:::\n\n\n::: {#c7262cc4 .cell execution_count=57}\n``` {.python .cell-code}\nsns.violinplot(x='day', y='tip', hue='time', data=df)\n```\n:::\n\n\n# Data processing with `pandas`\n\nLet us read again the `tips.csv` file\n\n::: {#544b41cf .cell execution_count=58}\n``` {.python .cell-code}\nimport pandas as pd\n\ndtypes = {\n    \"sex\": \"category\",\n    \"smoker\": \"category\",\n    \"day\": \"category\",\n    \"time\": \"category\"\n} \n\ndf = pd.read_csv(\"tips.csv\", dtype=dtypes)\ndf.head()\n```\n:::\n\n\n## Computations using `pandas` : broadcasting\n\nLet's add a column that contains the tip percentage\n\n::: {#ae1259ba .cell execution_count=59}\n``` {.python .cell-code}\ndf[\"tip_percentage\"] = df[\"tip\"] / df[\"total_bill\"]\ndf.head()\n```\n:::\n\n\nThe computation\n\n```{{python}}\ndf[\"tip\"] / df[\"total_bill\"]\n```\nuses a **broadcast** rule.\n\n- We can multiply, add, subtract, etc. together `numpy` arrays, `Series` or `pandas` dataframes when the computation **makes sense** in view of their respective **shape**\n\nThis principle is called **broadcast** or **broadcasting**.\n\n::: {.callout-note}\n\nBroadcasting is a key feature of `numpy` `ndarray`, see \n\n- [Numpy User's guide](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n- [Pandas book](https://wesmckinney.com/book/advanced-numpy.html#numpy_broadcasting)\n\n:::\n\n::: {#53ac0f0a .cell execution_count=60}\n``` {.python .cell-code}\ndf[\"tip\"].shape, df[\"total_bill\"].shape\n```\n:::\n\n\nThe `tip` and `total_bill`columns have the same `shape`, so broadcasting performs **pairwise division**.\n\nThis corresponds to the following \"hand-crafted\" approach with a `for` loop:\n\n::: {#7e7fda09 .cell execution_count=61}\n``` {.python .cell-code}\nfor i in range(df.shape[0]):\n    df.loc[i, \"tip_percentage\"] = df.loc[i, \"tip\"] / df.loc[i, \"total_bill\"]\n```\n:::\n\n\nBut using such a loop is: \n\n- longer to write\n- less readable \n- prone to mistakes\n- and *slower* :(\n\n*NEVER* use `Python` for-loops unless you need to !\n\n::: {#585fe2e9 .cell execution_count=62}\n``` {.python .cell-code}\n%%timeit -n 10\nfor i in range(df.shape[0]):\n    df.loc[i, \"tip_percentage\"] = df.loc[i, \"tip\"] / df.loc[i, \"total_bill\"]\n```\n:::\n\n\n::: {#f05b0f4c .cell execution_count=63}\n``` {.python .cell-code}\n%%timeit -n 10\ndf[\"tip_percentage\"] = df[\"tip\"] / df[\"total_bill\"]\n```\n:::\n\n\nThe `for` loop is $\\approx$ **100 times slower** ! (even worse on larger data)\n\n### Pitfall. Changing values in a `DataFrame`\n\nWhen you want to change a value in a `DataFrame`, never use\n\n```python\ndf[\"tip_percentage\"].loc[i] = 42\n```\n\nbut use\n\n```python\ndf.loc[i, \"tip_percentage\"] = 42\n```\n\n::: {.callout-caution}\n\nUse a **single** `loc` or `iloc` statement. The first version **might not work**: it might modify a copy of the column and not the dataframe itself !\n\n:::\n\nAnother example of broadcasting is:\n\n::: {#d873e407 .cell execution_count=64}\n``` {.python .cell-code}\n(100 * df[[\"tip_percentage\"]]).head()\n```\n:::\n\n\nwhere we multiplied **each entry** of the `tip_percentage` column by 100.\n\n::: {.callout-note}\n\n### Remark \n\nNote the difference between\n\n\n```python\ndf[['tip_percentage']]\n```\n\nwhich returns a `DataFrame` containing only the `tip_percentage` column and\n\n```python\ndf['tip_percentage']\n```\n\nwhich returns a `Series` containing the data of the `tip_percentage` column\n\n:::\n\n\n## Some more plots\n\n### How do the tip percentages relates to the total bill ?\n\n::: {#29fc9e86 .cell execution_count=65}\n``` {.python .cell-code}\nsns.jointplot(\n    x=\"total_bill\", \n    y=\"tip_percentage\", \n    data=df\n)\n```\n:::\n\n\n### Who tips best ?\n\n::: {#1d73d640 .cell execution_count=66}\n``` {.python .cell-code}\nsns.boxplot(\n    x='sex', \n    y='tip_percentage', \n    hue='smoker', \n    data=df\n)\n```\n:::\n\n\n### Who tips best without the `tip_percentage` outliers ?\n\n::: {#ff81165d .cell execution_count=67}\n``` {.python .cell-code}\nsns.boxplot(\n    x='sex', \n    y='tip_percentage', \n    hue='smoker', \n    data=df.loc[df[\"tip_percentage\"] <= 0.3]\n)\n```\n:::\n\n\nObject identity\n\n::: {#cfa22e63 .cell execution_count=68}\n``` {.python .cell-code}\nid(df)\n```\n:::\n\n\n## The all-mighty `groupby` and `aggregate`\n\nMany computations can be formulated as a **groupby** followed by and **aggregation**.\n\n### What is the mean `tip` and `tip percentage` each day ?\n\n::: {#bce28a2d .cell execution_count=69}\n``` {.python .cell-code}\ndf.head()\n```\n:::\n\n\n::: {#13d6162e .cell execution_count=70}\n``` {.python .cell-code}\ntry:\n\n    df.groupby(\"day\", observed=True).mean()\nexcept TypeError:\n    print('TypeError: category dtype does not support aggregation \"mean\"')\n```\n:::\n\n\nBut we do not care about the `size` column here, so we can use instead\n\n::: {#ff8b95d8 .cell execution_count=71}\n``` {.python .cell-code}\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\"]]\n        .groupby(\"day\")\n        .mean()\n)\n```\n:::\n\n\nIf we want to be more precise, we can `groupby` using several columns\n\n::: {#85291011 .cell execution_count=72}\n``` {.python .cell-code}\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\", \"time\"]]   # selection\n        .groupby([\"day\",\"time\"])                                # partition\n        .mean()                                                  # aggregation\n)\n```\n:::\n\n\n::: {.callout-note}\n\n### Remarks \n\n- We obtain a `DataFrame` with a two-level indexing: on the `day` and the `time`\n- Groups must be homogeneous: we have `NaN` values for empty groups (e.g. `Sat`, `Lunch`)\n\n:::\n\n\n### Pro tip\n\nSometimes, it is more convenient to get the groups as columns instead of a multi-level index.\n\nFor this, use `reset_index`:\n\n::: {#168017f1 .cell execution_count=73}\n``` {.python .cell-code}\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\", \"time\"]]   # selection\n        .groupby([\"day\", \"time\"])                                # partition\n        .mean() # aggregation\n        .reset_index()   # ako ungroup\n)\n```\n:::\n\n\n### Another pro tip: care about code readers\n\nComputations with pandas can include many operations that are **pipelined** until the final computation.\n\nPipelining many operations is good practice and perfectly normal, but in order to make the code readable you can put it between parenthesis (`python` expression) as follows:\n\n::: {#6ba85275 .cell execution_count=74}\n``` {.python .cell-code}\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .mean()\n    .reset_index()\n    # and on top of all this we sort the dataframe with respect \n    # to the tip_percentage\n    .sort_values(\"tip_percentage\")\n)\n```\n:::\n\n\n## Displaying a `DataFrame` with `style`\n\nNow, we can answer, with style, to the question: what are the average tip percentages along the week ?\n\n::: {#a1ba7969 .cell execution_count=75}\n``` {.python .cell-code}\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .mean()\n    # At the end of the pipeline you can use .style\n    .style\n    # Print numerical values as percentages \n    .format(\"{:.2%}\")\n    .background_gradient()\n)\n```\n:::\n\n\n## Removing the `NaN` values\n\nBut the `NaN` values are somewhat annoying. Let's remove them\n\n::: {#953948f4 .cell execution_count=76}\n``` {.python .cell-code}\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .mean()\n    # We just add this from the previous pipeline\n    .dropna()\n    .style\n    .format(\"{:.2%}\")\n    .background_gradient()\n)\n```\n:::\n\n\nNow, we see when `tip_percentage` is maximal. But what about the standard deviation?\n\n- We used only `.mean()` for now, but we can use several aggregating function using `.agg()`\n\n::: {#37dc8d8a .cell execution_count=77}\n``` {.python .cell-code}\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .agg([\"mean\", \"std\"])   # we feed `agg`  with a list of names of callables \n    .dropna()\n    .style\n    .format(\"{:.2%}\")\n    .background_gradient()\n)\n```\n:::\n\n\nAnd we can use also `.describe()` as aggregation function. Moreover we\n- use the `subset` option to specify which column we want to style\n- we use `(\"tip_percentage\", \"count\")` to access multi-level index\n\n::: {#640db6c1 .cell execution_count=78}\n``` {.python .cell-code}\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .describe()    # all-purpose summarising function\n)\n```\n:::\n\n\n::: {#65a939d6 .cell execution_count=79}\n``` {.python .cell-code}\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .describe()\n    .dropna()\n    .style\n    .bar(subset=[(\"tip_percentage\", \"count\")])\n    .background_gradient(subset=[(\"tip_percentage\", \"50%\")])\n)\n```\n:::\n\n\n## Supervised learning of `tip` based on the `total_bill` \n\nAs an example of very simple **machine-learning** problem, let us try to understand how we can predict `tip` based on `total_bill`.\n\n::: {#7a016313 .cell execution_count=80}\n``` {.python .cell-code}\nimport numpy as np\n\nplt.scatter(df[\"total_bill\"], df[\"tip\"])\nplt.xlabel(\"total_bill\", fontsize=12)\nplt.ylabel(\"tip\", fontsize=12)\n```\n:::\n\n\nThere's a rough **linear** dependence between the two. Let us try to find it by hand!<br>\nNamely, we look for numbers $b$ and $w$ such that\n\n```\ntip ≈ b + w × total_bill\n```\n\nfor all the examples of pairs of `(tip, total_bill)` we observe in the data.\n\nIn **machine learning**, we say that this is a very simple example of a **supervised learning** problem (here it is a regression problem), where `tip` is the **label** and where `total_bill` is the (only) **feature**, for which we intend to use a **linear predictor**.\n\n::: {#d05dfa40 .cell execution_count=81}\n``` {.python .cell-code}\nplt.scatter(df[\"total_bill\"], df[\"tip\"])\nplt.xlabel(\"total_bill\", fontsize=12)\nplt.ylabel(\"tip\", fontsize=12)\n\nslope = 1.0\nintercept = 0.0\n\nx = np.linspace(0, 50, 1000)\nplt.plot(x, intercept + slope * x, color=\"red\")\n```\n:::\n\n\n### A more interactive way \n\nThis might require\n\n::: {#f86f3feb .cell execution_count=82}\n``` {.python .cell-code}\n# !pip install ipympl\n```\n:::\n\n\n::: {#17709384 .cell execution_count=83}\n``` {.python .cell-code}\nimport ipywidgets as widgets\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib widget\n%matplotlib inline\n\nx = np.linspace(0, 50, 1000)\n\n@widgets.interact(intercept=(-5, 5, 1.), slope=(0, 1, .05))\ndef update(intercept=0.0, slope=0.5):\n    plt.scatter(df[\"total_bill\"], df[\"tip\"])\n    plt.plot(x, intercept + slope * x, color=\"red\")\n    plt.xlim((0, 50))\n    plt.ylim((0, 10))\n    plt.xlabel(\"total_bill\", fontsize=12)\n    plt.ylabel(\"tip\", fontsize=12)\n```\n:::\n\n\nThis is kind of tedious to do this by hand... it would be nice to come up with an **automated** way of doing this. Moreover:\n\n- We are using a **linear** function, while something more complicated (such as a polynomial) might be better\n- More importantly, we use **only** the `total_bill` column to predict the `tip`, while we know about many other things\n\n::: {#063859f7 .cell execution_count=84}\n``` {.python .cell-code}\ndf.head()\n```\n:::\n\n\n## One-hot encoding of categorical variables\n\nWe can't perform computations (products and sums) with columns containing **categorical** variables. So, we can't use them like this to predict the `tip`.\nWe need to **convert** them to numbers somehow.\n\nThe most classical approach for this is **one-hot encoding** (or \"create dummies\" or \"binarize\") of the categorical variables, which can be easily achieved with `pandas.get_dummies`\n\nWhy *one-hot* ? See [wikipedia](https://en.wikipedia.org/wiki/One-hot) for a plausible explanation\n\n::: {#2d42e481 .cell execution_count=85}\n``` {.python .cell-code}\ndf_one_hot = pd.get_dummies(df, prefix_sep='#')\ndf_one_hot.head(5)\n```\n:::\n\n\nOnly the categorical columns have been one-hot encoded. For instance, the `\"day\"` column is replaced by 4 columns named `\"day#Thur\"`, `\"day#Fri\"`, `\"day#Sat\"`, `\"day#Sun\"`, since `\"day\"` has 4 modalities (see next line).\n\n::: {#4af000d0 .cell execution_count=86}\n``` {.python .cell-code}\ndf['day'].unique()\n```\n:::\n\n\n::: {#4d9d9f7f .cell execution_count=87}\n``` {.python .cell-code}\ndf_one_hot.dtypes\n```\n:::\n\n\n## Pitfall. Colinearities with one-hot encoding\n\nSums over dummies for `sex`, `smoker`, `day`, `time` and `size` are all equal to one (by constrution of the one-hot encoded vectors).\n\n- Leads to **colinearities** in the matrix of features\n- It is **much harder** to train a linear regressor when the columns of the features matrix has colinearities\n\n::: {#e0027bee .cell execution_count=88}\n``` {.python .cell-code}\nday_cols = [col for col in df_one_hot.columns if col.startswith(\"day\")]\ndf_one_hot[day_cols].head()\ndf_one_hot[day_cols].sum(axis=1)\n```\n:::\n\n\n::: {#7e16b08e .cell execution_count=89}\n``` {.python .cell-code}\nall(df_one_hot[day_cols].sum(axis=1) == 1)\n```\n:::\n\n\nThe most standard solution is to remove a modality (i.e. remove a one-hot encoding vector). Simply achieved by specifying `drop_first=True` in the `get_dummies` function.\n\n::: {#ffdf2a23 .cell execution_count=90}\n``` {.python .cell-code}\ndf[\"day\"].unique()\n```\n:::\n\n\n::: {#98d10bb2 .cell execution_count=91}\n``` {.python .cell-code}\npd.get_dummies(df, prefix_sep='#', drop_first=True).head()\n```\n:::\n\n\nNow, if a categorical feature has $K$ modalities, we use only $K-1$ dummies.\nFor instance, there is no more `sex#Female` binary column. \n\n**Question.** So, a linear regression won't fit a weight for `sex#Female`. But, where do the model weights of the dropped binary columns go ?\n\n**Answer.** They just \"go\" to the **intercept**: interpretation of the population bias depends on the \"dropped\" one-hot encodings.\n\nSo, we actually fit:\n$$\\begin{array}{rl} \\texttt{tip} \\approx b & + w_1 \\times \\texttt{total_bill} + w_2 \\times \\texttt{size} \\\\ & + w_3 \\times \\texttt{sex#Male} + w_4 \\times \\texttt{smoker#Yes} \\\\ & + w_5 \\times \\texttt{day#Sat} + w_6 \\times \\texttt{day#Sun} + w_7 \\times \\texttt{day#Thur} \\\\ & + w_8 \\times \\texttt{time#Lunch} \\end{array}$$\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: /usr/share/jupyter/kernels/python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.12.3\n---\n",
    "supporting": [
      "notebook03_pandas_files"
    ],
    "filters": []
  }
}