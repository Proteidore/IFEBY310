{
  "hash": "25610a099cb551befe0e4065592356b7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: '`DataFrame`'\njupyter: python3\n---\n\n::: {#d4115c44 .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport sys\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n```\n:::\n\n\n::: {#ee087bed .cell ExecuteTime='{\"end_time\":\"2022-02-08T20:23:05.867529Z\",\"start_time\":\"2022-02-08T20:23:01.418071Z\"}' execution_count=2}\n``` {.python .cell-code}\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf().setAppName(\"Spark SQL Course\")\nsc = SparkContext(conf=conf)  # no need for Spark 3...\n\nspark = (SparkSession\n    .builder\n    .appName(\"Spark SQL Course\")\n    .getOrCreate()\n)\n```\n:::\n\n\n::: {#a9b282f3 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:46.926825Z\",\"start_time\":\"2022-01-26T10:58:46.920913Z\"}' execution_count=3}\n``` {.python .cell-code}\nfrom pyspark.sql import Row\n\nrow1 = Row(name=\"John\", age=21)\nrow2 = Row(name=\"James\", age=32)\nrow3 = Row(name=\"Jane\", age=18)\nrow1['name']\n```\n:::\n\n\n::: {#f9a8b895 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:57.185741Z\",\"start_time\":\"2022-01-26T10:58:57.155181Z\"}' execution_count=4}\n``` {.python .cell-code}\ndf = spark.createDataFrame([row1, row2, row3])\n```\n:::\n\n\n::: {#50650819 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:59:13.491438Z\",\"start_time\":\"2022-01-26T10:59:13.486119Z\"}' execution_count=5}\n``` {.python .cell-code}\ndf.printSchema()\n```\n:::\n\n\n::: {#a083b359 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:59:17.760344Z\",\"start_time\":\"2022-01-26T10:59:17.597166Z\"}' execution_count=6}\n``` {.python .cell-code}\ndf.show()\n```\n:::\n\n\n::: {#a52778f4 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:59:25.889372Z\",\"start_time\":\"2022-01-26T10:59:25.866666Z\"}' execution_count=7}\n``` {.python .cell-code}\nprint(df.rdd.toDebugString().decode(\"utf-8\"))\n```\n:::\n\n\n::: {#1f2d3a12 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:59:45.432264Z\",\"start_time\":\"2022-01-26T10:59:45.426727Z\"}' execution_count=8}\n``` {.python .cell-code}\ndf.rdd.getNumPartitions()\n```\n:::\n\n\n## Creating dataframes\n\n::: {#cd0aaf3f .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:18.707591Z\",\"start_time\":\"2022-01-26T10:58:18.220608Z\"}' execution_count=9}\n``` {.python .cell-code}\nrows = [\n    Row(name=\"John\", age=21, gender=\"male\"),\n    Row(name=\"James\", age=25, gender=\"female\"),\n    Row(name=\"Albert\", age=46, gender=\"male\")\n]\n\ndf = spark.createDataFrame(rows)\n```\n:::\n\n\n::: {#eff6e7c1 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:18.707591Z\",\"start_time\":\"2022-01-26T10:58:18.220608Z\"}' execution_count=10}\n``` {.python .cell-code}\ndf.show()\n```\n:::\n\n\n::: {#a819ba4b .cell execution_count=11}\n``` {.python .cell-code}\nhelp(Row)\n```\n:::\n\n\n::: {#6c52d94b .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:19.065539Z\",\"start_time\":\"2022-01-26T10:58:18.710711Z\"}' execution_count=12}\n``` {.python .cell-code}\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrows = [\n    [\"John\", 21, \"male\"],\n    [\"James\", 25, \"female\"],\n    [\"Albert\", 46, \"male\"]\n]\n\ndf = spark.createDataFrame(\n    rows, \n    column_names\n)\n\ndf.show()\n```\n:::\n\n\n::: {#43e781d3 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:19.074335Z\",\"start_time\":\"2022-01-26T10:58:19.068088Z\"}' execution_count=13}\n``` {.python .cell-code}\ndf.printSchema()\n```\n:::\n\n\n::: {#ef788482 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:19.840178Z\",\"start_time\":\"2022-01-26T10:58:19.077057Z\"}' execution_count=14}\n``` {.python .cell-code}\n# sc = SparkContext(conf=conf)  # no need for Spark 3...\n\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrdd = sc.parallelize([\n    (\"John\", 21, \"male\"),\n    (\"James\", 25, \"female\"),\n    (\"Albert\", 46, \"male\")\n])\ndf = spark.createDataFrame(rdd, column_names)\ndf.show()\n```\n:::\n\n\n## Schema\n\nThere is special type schemata. A object of class `StructType` is made of a list of objects of type `StructField`. \n\n::: {#5b240c91 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:19.850578Z\",\"start_time\":\"2022-01-26T10:58:19.843835Z\"}' execution_count=15}\n``` {.python .cell-code}\ndf.schema\n```\n:::\n\n\n::: {#518c7d89 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:19.860631Z\",\"start_time\":\"2022-01-26T10:58:19.854012Z\"}' execution_count=16}\n``` {.python .cell-code}\ntype(df.schema)\n```\n:::\n\n\nA object of type `StructField` has a name, a PySpark type, an d a boolean parameter.\n\n::: {#5da86aac .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:20.199419Z\",\"start_time\":\"2022-01-26T10:58:19.863528Z\"}' execution_count=17}\n``` {.python .cell-code}\nfrom pyspark.sql.types import *\n\nschema = StructType(\n    [\n        StructField(\"name\", StringType(), True),\n        StructField(\"age\", IntegerType(), True),\n        StructField(\"gender\", StringType(), True)\n    ]\n)\n\nrows = [(\"John\", 21, \"male\")]\ndf = spark.createDataFrame(rows, schema)\ndf.printSchema()\ndf.show()\n```\n:::\n\n\n# Queries  (single table $σ$, $π$)\n\nPySpark offers two ways to query a datafrane:\n\n- An ad hod API with methods for the DataFrame class.\n- The possibility to post SQL queries (provided a temporary view has been created).\n\n::: {#51dfc0f6 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:20.882311Z\",\"start_time\":\"2022-01-26T10:58:20.201993Z\"}' execution_count=18}\n``` {.python .cell-code}\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrows = [\n    [\"John\", 21, \"male\"],\n    [\"Jane\", 25, \"female\"]\n]\n# \ndf = spark.createDataFrame(rows, column_names)\n\n# Create a temporary view from the DataFrame\ndf.createOrReplaceTempView(\"new_view\")\n\n# Apply the query\nquery = \"\"\"\n    SELECT \n        name, age \n    FROM \n        new_view \n    WHERE \n        gender='male'\n\"\"\"\n\nmen_df = spark.sql(query)\nmen_df.show()\n```\n:::\n\n\n## `SELECT`  (projection $π$)\n\n::: {#d278cbfa .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:21.162623Z\",\"start_time\":\"2022-01-26T10:58:20.884802Z\"}' execution_count=19}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")    \n\nquery = \"\"\"\n    SELECT \n        name, age \n    FROM \n        table\n\"\"\"\n\nspark.sql(query).show()\n```\n:::\n\n\nUsing the API:\n\n::: {#cce91e8b .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:21.388097Z\",\"start_time\":\"2022-01-26T10:58:21.164840Z\"}' execution_count=20}\n``` {.python .cell-code}\n(\n    df\n        .select(\"name\", \"age\")\n        .show()\n)\n```\n:::\n\n\n`π(df, \"name\", \"age\")`\n\n## `WHERE`  (filter, selection, $σ$)\n\n::: {#4439b4c5 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:21.704402Z\",\"start_time\":\"2022-01-26T10:58:21.402155Z\"}' execution_count=21}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        * \n    FROM \n        table\n    WHERE \n        age > 21\n\"\"\"\n\nspark.sql(query).show()\n```\n:::\n\n\nUsing the API \n\n::: {#c09b7122 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:21.924501Z\",\"start_time\":\"2022-01-26T10:58:21.706741Z\"}' execution_count=22}\n``` {.python .cell-code}\n( \n    df\n        .where(\"age > 21\")\n        .show()\n)\n```\n:::\n\n\nThis implements `σ(df, \"age > 21\")`\n\n::: {#09be6195 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:22.377417Z\",\"start_time\":\"2022-01-26T10:58:21.926708Z\"}' execution_count=23}\n``` {.python .cell-code}\n# Alternatively:\n( \n    df\n      .where(df['age'] > 21)\n      .show()\n)\n```\n:::\n\n\n::: {#eaabaa28 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:22.566385Z\",\"start_time\":\"2022-01-26T10:58:22.380036Z\"}' execution_count=24}\n``` {.python .cell-code}\n( \n    df\n      .where(df.age > 21)\n      .show()\n)\n```\n:::\n\n\nMethod chaining allows to construct complex queries \n\n::: {#6c49cad7 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:22.837136Z\",\"start_time\":\"2022-01-26T10:58:22.569324Z\"}' execution_count=25}\n``` {.python .cell-code}\n( \n    df\n      .where(\"age > 21\")\n      .select([\"name\", \"age\"])\n      .show()\n)\n```\n:::\n\n\nThis implements \n\n```\n    σ(df, \"age > 21\") |>\n    π([\"name\", \"age\"])\n```\n\n## `LIMIT`  \n\n::: {#98e3a5f8 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:23.315363Z\",\"start_time\":\"2022-01-26T10:58:22.842106Z\"}' execution_count=26}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        * \n    FROM \n        table \n    LIMIT 1\n\"\"\"\n\nspark.sql(query).show()\n```\n:::\n\n\n::: {#471e3b8f .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:23.522646Z\",\"start_time\":\"2022-01-26T10:58:23.318694Z\"}' execution_count=27}\n``` {.python .cell-code}\ndf.limit(1).show()\n```\n:::\n\n\n::: {#d1681313 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:23.778517Z\",\"start_time\":\"2022-01-26T10:58:23.525281Z\"}' execution_count=28}\n``` {.python .cell-code}\ndf.select(\"*\").limit(1).show()\n```\n:::\n\n\n## `ORDER BY`\n\n::: {#856910a3 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:24.190838Z\",\"start_time\":\"2022-01-26T10:58:23.781166Z\"}' execution_count=29}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        * \n    FROM \n        table\n    ORDER BY \n        name ASC\n\"\"\"\n\nspark.sql(query).show()\n```\n:::\n\n\n::: {#75c2cc60 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:24.368069Z\",\"start_time\":\"2022-01-26T10:58:24.193899Z\"}' execution_count=30}\n``` {.python .cell-code}\ndf.orderBy(df.name.asc()).show()\n```\n:::\n\n\n## `ALIAS`  (rename)\n\n::: {#862eb8b9 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:24.643668Z\",\"start_time\":\"2022-01-26T10:58:24.370758Z\"}' execution_count=31}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")\nquery = \"SELECT name, age, gender AS sex FROM table\"\nspark.sql(query).show()\n```\n:::\n\n\n::: {#574dd906 .cell execution_count=32}\n``` {.python .cell-code}\ntype(df.age)\n```\n:::\n\n\n::: {#cf02d93b .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:24.858104Z\",\"start_time\":\"2022-01-26T10:58:24.646119Z\"}' execution_count=33}\n``` {.python .cell-code}\ndf.select(df.name, df.age, df.gender.alias('sex')).show()\n```\n:::\n\n\n## `CAST`\n\n::: {#7eab8801 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:25.072286Z\",\"start_time\":\"2022-01-26T10:58:24.860474Z\"}' execution_count=34}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")\nquery = \"SELECT name, cast(age AS float) AS age_f FROM table\"\nspark.sql(query).show()\n```\n:::\n\n\n::: {#de0e7233 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:25.384433Z\",\"start_time\":\"2022-01-26T10:58:25.074523Z\"}' execution_count=35}\n``` {.python .cell-code}\ndf.select(df.name, df.age.cast(\"float\").alias(\"age_f\")).show()\n```\n:::\n\n\n::: {#cbac42f5 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:25.648155Z\",\"start_time\":\"2022-01-26T10:58:25.386952Z\"}' execution_count=36}\n``` {.python .cell-code}\nnew_age_col = df.age.cast(\"float\").alias(\"age_f\")\ntype(new_age_col), type(df.age)\n```\n:::\n\n\n::: {#4eec5547 .cell execution_count=37}\n``` {.python .cell-code}\ndf.select(df.name, new_age_col).show()\n```\n:::\n\n\n## Adding new columns\n\n::: {#e2c1f139 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:25.931495Z\",\"start_time\":\"2022-01-26T10:58:25.651283Z\"}' execution_count=38}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        *, \n        12*age AS age_months \n    FROM \n        table\n\"\"\"\n\nspark.sql(query).show()\n```\n:::\n\n\n::: {#f7b65119 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:26.195480Z\",\"start_time\":\"2022-01-26T10:58:25.933620Z\"}' execution_count=39}\n``` {.python .cell-code}\n( \n    df\n        .withColumn(\"age_months\", df.age * 12)\n        .show()\n)\n```\n:::\n\n\n::: {#1a6bc66b .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:26.422122Z\",\"start_time\":\"2022-01-26T10:58:26.197759Z\"}' execution_count=40}\n``` {.python .cell-code}\n(\n    df\n        .select(\"*\", \n                (df.age * 12).alias(\"age_months\"))\n        .show()\n)\n```\n:::\n\n\n::: {#4b921c79 .cell execution_count=41}\n``` {.python .cell-code}\nimport datetime\n\nhui = datetime.date.today()\n\nhui = hui.replace(year=hui.year-21)\n\nstr(hui)\n```\n:::\n\n\n::: {#50e23897 .cell execution_count=42}\n``` {.python .cell-code}\n# df.select(\"*\", hui.replace(year=hui.year - df.age ).alias(\"yob\")).show()\n```\n:::\n\n\n# Column functions\n\n## Numeric functions examples\n\n::: {#664d7aed .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:26.748718Z\",\"start_time\":\"2022-01-26T10:58:26.425451Z\"}' execution_count=43}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"brand\", \"cost\"]\ndf = spark.createDataFrame([\n    (\"garnier\", 3.49),\n    (\"elseve\", 2.71)\n], columns)\n\nround_cost = fn.round(df.cost, 1)\nfloor_cost = fn.floor(df.cost)\nceil_cost = fn.ceil(df.cost)\n\ndf.withColumn('round', round_cost)\\\n    .withColumn('floor', floor_cost)\\\n    .withColumn('ceil', ceil_cost)\\\n    .show()\n```\n:::\n\n\n## String functions examples\n\n::: {#a3a3892b .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:27.055563Z\",\"start_time\":\"2022-01-26T10:58:26.751235Z\"}' execution_count=44}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"first_name\", \"last_name\"]\n\ndf = spark.createDataFrame([\n    (\"John\", \"Doe\"),\n    (\"Mary\", \"Jane\")\n], columns)\n\nlast_name_initial = fn.substring(df.last_name, 0, 1)\n# last_name_initial_dotted = fn.concat(last_name_initial, \".\")\n\nname = fn.concat_ws(\" \", df.first_name, last_name_initial)\ndf.withColumn(\"name\", name).show()\n```\n:::\n\n\n::: {#4270ccd8 .cell execution_count=45}\n``` {.python .cell-code}\n( \n    df.selectExpr(\"*\", \"substring(last_name, 0, 1) as lni\")\n      .selectExpr(\"first_name\", \"last_name\", \"concat(first_name, ' ', lni, '.') as nname\")\n      .show()\n)\n```\n:::\n\n\n## Date functions examples\n\n::: {#a14a001c .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:27.373396Z\",\"start_time\":\"2022-01-26T10:58:27.057938Z\"}' execution_count=46}\n``` {.python .cell-code}\nfrom datetime import date\nfrom pyspark.sql import functions as fn\n\ndf = spark.createDataFrame([\n    (date(2015, 1, 1), date(2015, 1, 15)),\n    (date(2015, 2, 21), date(2015, 3, 8)),\n], [\"start_date\", \"end_date\"])\n\ndays_between = fn.datediff(df.end_date, df.start_date)\nstart_month = fn.month(df.start_date)\n\ndf.withColumn('days_between', days_between)\\\n    .withColumn('start_month', start_month)\\\n    .show()\n```\n:::\n\n\n::: {#638d3c0e .cell execution_count=47}\n``` {.python .cell-code}\nstr(date(2015, 1, 1) - date(2015, 1, 15))\n```\n:::\n\n\n::: {#07ce83f2 .cell execution_count=48}\n``` {.python .cell-code}\nfrom datetime import timedelta\n\ndate(2023, 2 , 14) + timedelta(days=3)\n```\n:::\n\n\n## Conditional transformations\n\n::: {#20f25e37 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:27.630822Z\",\"start_time\":\"2022-01-26T10:58:27.375855Z\"}' execution_count=49}\n``` {.python .cell-code}\ndf = spark.createDataFrame([\n    (\"John\", 21, \"male\"),\n    (\"Jane\", 25, \"female\"),\n    (\"Albert\", 46, \"male\"),\n    (\"Brad\", 49, \"super-hero\")\n], [\"name\", \"age\", \"gender\"])\n```\n:::\n\n\n::: {#7ef58cae .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:27.630822Z\",\"start_time\":\"2022-01-26T10:58:27.375855Z\"}' execution_count=50}\n``` {.python .cell-code}\nsupervisor = ( \n    fn.when(df.gender == 'male', 'Mr. Smith')\n      .when(df.gender == 'female', 'Miss Jones')\n      .otherwise('NA')\n)\n\ntype(supervisor), type(fn.when)\n```\n:::\n\n\n::: {#07fb054a .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:27.630822Z\",\"start_time\":\"2022-01-26T10:58:27.375855Z\"}' execution_count=51}\n``` {.python .cell-code}\ndf.withColumn(\"supervisor\", supervisor).show()\n```\n:::\n\n\n## User-defined functions\n\n::: {#9b2f7e09 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:28.037428Z\",\"start_time\":\"2022-01-26T10:58:27.633093Z\"}' execution_count=52}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.types import StringType\n\ndf = spark.createDataFrame([(1, 3), (4, 2)], [\"first\", \"second\"])\n\ndef my_func(col_1, col_2):\n    if (col_1 > col_2):\n        return \"{} is bigger than {}\".format(col_1, col_2)\n    else:\n        return \"{} is bigger than {}\".format(col_2, col_1)\n\nmy_udf = fn.udf(my_func, StringType())\n\ndf.withColumn(\"udf\", my_udf(df['first'], df['second'])).show()\n```\n:::\n\n\n# Joins  ($⋈$)\n\n## Using the `spark.sql` API\n\n::: {#94ac5b61 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:29.098957Z\",\"start_time\":\"2022-01-26T10:58:28.042691Z\"}' execution_count=53}\n``` {.python .cell-code}\nfrom datetime import date\n\nproducts = spark.createDataFrame([\n    ('1', 'mouse', 'microsoft', 39.99),\n    ('2', 'keyboard', 'logitech', 59.99),\n], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n\npurchases = spark.createDataFrame([\n    (date(2017, 11, 1), 2, '1'),\n    (date(2017, 11, 2), 1, '1'),\n    (date(2017, 11, 5), 1, '2'),\n], ['date', 'quantity', 'prod_id'])\n\n# The default join type is the \"INNER\" join\npurchases.join(products, 'prod_id').show()\n```\n:::\n\n\n::: {#18fbb575 .cell execution_count=54}\n``` {.python .cell-code}\npurchases.join(products, 'prod_id').explain()\n```\n:::\n\n\n## Using a `SQL` query\n\n::: {#ddf9b8d6 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:29.731271Z\",\"start_time\":\"2022-01-26T10:58:29.101559Z\"}' execution_count=55}\n``` {.python .cell-code}\nproducts.createOrReplaceTempView(\"products\")\npurchases.createOrReplaceTempView(\"purchases\")\n\nquery = \"\"\"\n    SELECT * \n    FROM purchases AS prc INNER JOIN \n        products AS prd \n    ON prc.prod_id = prd.prod_id\n\"\"\"\nspark.sql(query).show()\n```\n:::\n\n\n::: {#30869a5c .cell execution_count=56}\n``` {.python .cell-code}\nspark.sql(query).explain()\n```\n:::\n\n\n::: {#7e5008a3 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:30.660419Z\",\"start_time\":\"2022-01-26T10:58:29.734282Z\"}' execution_count=57}\n``` {.python .cell-code}\nnew_purchases = spark.createDataFrame([\n    (date(2017, 11, 1), 2, '1'),\n    (date(2017, 11, 2), 1, '3'),\n], ['date', 'quantity', 'prod_id_x'])\n\n# The default join type is the \"INNER\" join\njoin_rule = new_purchases.prod_id_x == products.prod_id\n\nprint(type(join_rule))\n\nnew_purchases.join(products, join_rule, 'left').show()\n```\n:::\n\n\n::: {#8cd627bf .cell execution_count=58}\n``` {.python .cell-code}\njoin_rule.info\n```\n:::\n\n\n::: {#4d9de3e1 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:31.319336Z\",\"start_time\":\"2022-01-26T10:58:30.663809Z\"}' execution_count=59}\n``` {.python .cell-code}\nnew_purchases = spark.createDataFrame([\n    (date(2017, 11, 1), 2, '1'),\n    (date(2017, 11, 2), 1, '3'),\n], ['date', 'quantity', 'prod_id_x'])\n\n# The default join type is the \"INNER\" join\njoin_rule = new_purchases.prod_id_x == products.prod_id\n\nnew_purchases.join(products, join_rule, 'left').show()\n```\n:::\n\n\n## Various types of joins\n\n::: {#fcfbba55 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:31.376310Z\",\"start_time\":\"2022-01-26T10:58:31.323600Z\"}' execution_count=60}\n``` {.python .cell-code}\nleft = spark.createDataFrame([\n    (1, \"A1\"), (2, \"A2\"), (3, \"A3\"), (4, \"A4\")], \n    [\"id\", \"value\"])\n\nright = spark.createDataFrame([\n    (3, \"A3\"), (4, \"A4\"), (4, \"A4_1\"), (5, \"A5\"), (6, \"A6\")], \n    [\"id\", \"value\"])\n\njoin_types = [\n    \"inner\", \"outer\", \"left\", \"right\",\n    \"leftsemi\", \"leftanti\"\n]\n```\n:::\n\n\n::: {#b2e1b136 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:34.708236Z\",\"start_time\":\"2022-01-26T10:58:31.380091Z\"}' execution_count=61}\n``` {.python .cell-code}\nfor join_type in join_types:\n    print(join_type)\n    left.join(right, on=\"id\", how=join_type)\\\n        .orderBy(\"id\")\\\n        .show()\n```\n:::\n\n\n# Agregations    (summarize)\n\n## Examples using the API\n\n::: {#6837d86b .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:35.398306Z\",\"start_time\":\"2022-01-26T10:58:34.710552Z\"}' execution_count=62}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as fn\n\nproducts = spark.createDataFrame([\n    ('1', 'mouse', 'microsoft', 39.99),\n    ('2', 'mouse', 'microsoft', 59.99),\n    ('3', 'keyboard', 'microsoft', 59.99),\n    ('4', 'keyboard', 'logitech', 59.99),\n    ('5', 'mouse', 'logitech', 29.99),\n], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n\n( \n    products\n        .groupBy('prod_cat')\n        .avg('prod_value')\n        .show()\n)\n```\n:::\n\n\n::: {#47143c86 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:35.782623Z\",\"start_time\":\"2022-01-26T10:58:35.400724Z\"}' execution_count=63}\n``` {.python .cell-code}\n(\n    products\n        .groupBy('prod_cat')\n        .agg(fn.avg('prod_value'))\n        .show()\n)\n```\n:::\n\n\n::: {#904586bb .cell execution_count=64}\n``` {.python .cell-code}\n(\n    products\n        .groupBy('prod_cat')\n        .agg(\n            fn.mean('prod_value'), \n            fn.stddev('prod_value')\n        )\n        .show()\n)\n```\n:::\n\n\n::: {#37b2c8b3 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:36.195471Z\",\"start_time\":\"2022-01-26T10:58:35.784780Z\"}' execution_count=65}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as fn\n\n(\n    products\n        .groupBy('prod_brand', 'prod_cat')\\\n        .agg(\n            fn.avg('prod_value')\n        )\n        .show()\n)\n```\n:::\n\n\n::: {#6f3e3724 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:36.650354Z\",\"start_time\":\"2022-01-26T10:58:36.207985Z\"}' execution_count=66}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as fn\n\n(\n    products\n        .groupBy('prod_brand')\n        .agg(\n            fn.round(\n                fn.avg('prod_value'), 1)\n                .alias('average'),\n            fn.ceil(\n                fn.sum('prod_value'))\n                .alias('sum'),\n            fn.min('prod_value')\n                .alias('min')\n        )\n        .show()\n)\n```\n:::\n\n\n## Example using a query\n\n::: {#e53cdb0d .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:37.089099Z\",\"start_time\":\"2022-01-26T10:58:36.652842Z\"}' execution_count=67}\n``` {.python .cell-code}\nproducts.createOrReplaceTempView(\"products\")\n```\n:::\n\n\n::: {#2c6b6178 .cell execution_count=68}\n``` {.python .cell-code}\nquery = \"\"\"\nSELECT\n    prod_brand,\n    round(avg(prod_value), 1) AS average,\n    min(prod_value) AS min\nFROM \n    products\nGROUP BY \n    prod_brand\n\"\"\"\n\nspark.sql(query).show()\n```\n:::\n\n\n# Window functions\n\n## Numerical window functions\n\n::: {#1cd9bacd .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:37.751296Z\",\"start_time\":\"2022-01-26T10:58:37.092075Z\"}' execution_count=69}\n``` {.python .cell-code}\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\n# First, we create the Window definition\nwindow = Window.partitionBy('prod_brand')\n\nprint(type(window))\n```\n:::\n\n\nThen, we can use `over` to aggregate on this window\n\n::: {#3e1e67cf .cell execution_count=70}\n``` {.python .cell-code}\navg = fn.avg('prod_value').over(window)\n\n# Finally, we can it as a classical column\n(\n    products\n        .withColumn('avg_brand_value', fn.round(avg, 2))\n        .show()\n)\n```\n:::\n\n\nWith SQL queries, using multiple windows is not a problem\n\n::: {#a233d7a3 .cell execution_count=71}\n``` {.python .cell-code}\nquery = \"\"\"\n    SELECT \n        *, \n        ROUND(AVG(prod_value) OVER w1, 2)  AS avg_brand_value,\n        ROUND(AVG(prod_value) OVER w2, 1)  AS avg_prod_value\n    FROM \n        products\n    WINDOW \n        w1 AS (PARTITION BY prod_brand),\n        w2 AS (PARTITION BY prod_cat)\n\"\"\"\n\nspark.sql(query).show()\n```\n:::\n\n\n::: {#3c0db3d0 .cell execution_count=72}\n``` {.python .cell-code}\nwindow2 = Window.partitionBy('prod_cat')\n\navg2 = fn.avg('prod_value').over(window2)\n\n# Finally, we can it as a classical column\n( \n    products\n        .withColumn('avg_brand_value', fn.round(avg, 2))\n        .withColumn('avg_prod_value', fn.round(avg2, 1))\n        .show()\n)\n```\n:::\n\n\nNow we can compare the physical plans associated with the  two jobs.\n\n::: {#a589ea17 .cell execution_count=73}\n``` {.python .cell-code}\n( \n    products\n        .withColumn('avg_brand_value', fn.round(avg, 2))\n        .withColumn('avg_prod_value', fn.round(avg2, 1))\n        .explain()\n)\n```\n:::\n\n\n::: {#edc1ae84 .cell execution_count=74}\n``` {.python .cell-code}\nspark.sql(query).explain()\n```\n:::\n\n\n# Windows can be defined on multiple columns\n\n::: {#e821a670 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:38.261379Z\",\"start_time\":\"2022-01-26T10:58:37.753256Z\"}' execution_count=75}\n``` {.python .cell-code}\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\nwindow = Window.partitionBy('prod_brand', 'prod_cat')\n\navg = fn.avg('prod_value').over(window)\n\n\n(\n    products    \n        .withColumn('avg_value', fn.round(avg, 2))\n        .show()\n)\n```\n:::\n\n\n## Lag and Lead\n\n::: {#44f413c3 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:39.785452Z\",\"start_time\":\"2022-01-26T10:58:39.084502Z\"}' execution_count=76}\n``` {.python .cell-code}\npurchases = spark.createDataFrame(\n    [\n        (date(2017, 11, 1), 'mouse'),\n        (date(2017, 11, 2), 'mouse'),\n        (date(2017, 11, 4), 'keyboard'),\n        (date(2017, 11, 6), 'keyboard'),\n        (date(2017, 11, 9), 'keyboard'),\n        (date(2017, 11, 12), 'mouse'),\n        (date(2017, 11, 18), 'keyboard')\n    ], \n    ['date', 'prod_cat']\n)\n\npurchases.show()\n\nwindow = Window.partitionBy('prod_cat').orderBy('date')\n\nprev_purch = fn.lag('date', 1).over(window)\nnext_purch = fn.lead('date', 1).over(window)\n\npurchases\\\n    .withColumn('prev', prev_purch)\\\n    .withColumn('next', next_purch)\\\n    .orderBy('prod_cat', 'date')\\\n    .show()\n```\n:::\n\n\n## Rank, DenseRank and RowNumber\n\n::: {#9198bf60 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:40.005845Z\",\"start_time\":\"2022-01-26T10:58:39.787433Z\"}' execution_count=77}\n``` {.python .cell-code}\ncontestants = spark.createDataFrame(\n    [   \n        ('veterans', 'John', 3000),\n        ('veterans', 'Bob', 3200),\n        ('veterans', 'Mary', 4000),\n        ('young', 'Jane', 4000),\n        ('young', 'April', 3100),\n        ('young', 'Alice', 3700),\n        ('young', 'Micheal', 4000),\n    ], \n    ['category', 'name', 'points']\n)\n\ncontestants.show()\n```\n:::\n\n\n::: {#0be70027 .cell ExecuteTime='{\"end_time\":\"2022-01-26T10:58:40.653650Z\",\"start_time\":\"2022-01-26T10:58:40.009618Z\"}' execution_count=78}\n``` {.python .cell-code}\nwindow = (\n    Window\n        .partitionBy('category')\n        .orderBy(contestants.points.desc())\n)\n\nrank = fn.rank().over(window)\ndense_rank = fn.dense_rank().over(window)\nrow_number = fn.row_number().over(window)\n\ncontestants\\\n    .withColumn('rank', rank)\\\n    .withColumn('dense_rank', dense_rank)\\\n    .withColumn('row_number', row_number)\\\n    .orderBy('category', fn.col('points').desc())\\\n    .show()\n```\n:::\n\n\n# Connection to a database {{< fa database >}}\n\nThe postgres server runs locally on my laptop, it is equiped with a\nnumber of training schemata, including `nycflights` (see [https://s-v-b.github.io/MA15Y030/schemas/schema-nycflights.html](https://s-v-b.github.io/MA15Y030/schemas/schema-nycflights.html))\n\n::: {#14d91de7 .cell execution_count=79}\n``` {.python .cell-code}\ndf_flights = spark.read \\\n    .format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:postgresql://localhost:5434/bd_2023-24\") \\\n    .option(\"dbschema\", \"nycflights\")\\\n    .option(\"dbtable\", \"flights\") \\\n    .option(\"user\", \"postgres\") \\\n    .option(\"password\", \"postgres\") \\\n    .option(\"driver\", \"org.postgresql.Driver\") \\\n    .load()\n\ndf_flights.printSchema()\n```\n:::\n\n\nTo load the five tables, we avoid cut and paste, and abide to the DRY principle.\n\nWe package the options in a dictionnary\n\n::: {#a9e99ce4 .cell execution_count=80}\n``` {.python .cell-code}\ndb_con_settings = {\n    'url': \"jdbc:postgresql://localhost:5434/bd_2023-24\",\n    'dbschema':  \"nycflights\",\n    'user':  \"postgres\",\n    'password':  \"postgres\",\n    'driver':  \"org.postgresql.Driver\"\n}\n```\n:::\n\n\nWe prepare a Python object using dictionnary unpacking. \n\n::: {#58d03e4f .cell execution_count=81}\n``` {.python .cell-code}\no  = spark.read \\\n    .format(\"jdbc\")\\\n    .options(**db_con_settings)\n```\n:::\n\n\nWe use the object to load the different tables in a `for` loop.\n\n::: {#2709bd0d .cell execution_count=82}\n``` {.python .cell-code}\ntbl_names = ['flights', 'airports', 'airlines', 'planes', 'weather']\n\ndic_df = {}\n\nfor tn in tbl_names:\n    dic_df[tn] = o.option('dbtable', tn).load()\n```\n:::\n\n\n::: {#1f587915 .cell execution_count=83}\n``` {.python .cell-code}\nfor k, v in dic_df.items():\n    v.printSchema()\n```\n:::\n\n\nWe can now query the tables. \n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: /usr/share/jupyter/kernels/python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.12.3\n---\n",
    "supporting": [
      "notebook06_sparksql_files"
    ],
    "filters": []
  }
}