{
  "hash": "d72597cfb43cfea5a15f704b00d8ee2c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Using with `pyspark` for data preprocessing\njupyter: python3\n---\n\n\n\n## Data description\n\nThe data is a `parquet` file which contains a dataframe with 8 columns:\n\n- `xid`: unique user id\n- `action`: type of action. 'C' is a click, 'O' or 'VSL' is a web-display\n- `date`: date of the action\n- `website_id`: unique id of the website\n- `url`: url of the webpage\n- `category_id`: id of the display\n- `zipcode`: postal zipcode of the user\n- `device`: type of device used by the user\n\n::: #q1\n\n## Q1. Some statistics / computations\n\nUsing `pyspark.sql` we want to do the following things:\n\n1. Compute the total number of unique users\n2. Construct a column containing the total number of actions per user\n3. Construct a column containing the number of days since the last action of the user\n4. Construct a column containing the number of actions of each user for each modality of device \n\n:::\n\n::: #q2\n\n## Q2. Feature engineering \n\nThen, we want to construct a classifier to predict the click on the category 1204. \nHere is an agenda for this:\n\n1. Construction of a features matrix for which each line corresponds to the information concerning a user.\n2. In this matrix, we need to keep only the users that have been exposed to the display in category 1204\n\n:::\n\n::: #q3\n\n## Q3. Classification \n\n3. Using this training dataset, train a binary classifier, and evaluate your classifier using a precision / recall curve computed on test data.\n\n:::\n\n\n# Download/read the data and a first look at the data\n\n::: {#cf5c507b .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport sys\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n```\n:::\n\n\n::: {.callout-note}\n\n### Spark in local mode\n\n::: {#7a2c9b62 .cell execution_count=2}\n``` {.python .cell-code}\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n\nspark = (SparkSession\n    .builder\n    .appName(\"Spark Webdata\")\n    .getOrCreate()\n)\n```\n:::\n\n\n::: {#ae2dc838 .cell execution_count=3}\n``` {.python .cell-code}\nimport requests, zipfile, io\nfrom pathlib import Path\n\npath = Path('webdata.parquet')\nif not path.exists():\n    url = \"https://s-v-b.github.io/IFEBY310/data/webdata.parquet.zip\"\n    r = requests.get(url)\n    z = zipfile.ZipFile(io.BytesIO(r.content))\n    z.extractall(path='./')\n```\n:::\n\n\n::: {#88a7e5ab .cell execution_count=4}\n``` {.python .cell-code}\ninput_path = './'\n\ninput_file = os.path.join(input_path, 'webdata.parquet')\n\ndf = spark.read.parquet(input_file)\n```\n:::\n\n\n:::\n\n::: {.callout-note}\n\nWe can also give a try to `pyarrow.parquet` module to load the Parquet file in an Arrow table.\n\n:::\n\n::: {#9affce5e .cell execution_count=5}\n``` {.python .cell-code}\nimport pyarrow as pa\nimport comet    as co\nimport pyarrow.parquet as pq\n\ndfa = pq.read_table(input_file)\n```\n:::\n\n\n::: {#a687edc0 .cell execution_count=6}\n``` {.python .cell-code}\ndfa.num_columns\n```\n:::\n\n\n:::\n\n\n::: {.callout-warning}\n\nLet us go back to the spark data frame\n\n:::\n\n::: {#61df13eb .cell execution_count=7}\n``` {.python .cell-code}\ndf.printSchema()\n```\n:::\n\n\n::: {#cd77ca5b .cell execution_count=8}\n``` {.python .cell-code}\ndf.rdd.getNumPartitions()\n```\n:::\n\n\n::: {.callout-note  title=\"Question\"}\n\nExplain the partition size. \n\n:::\n\n::: {#122dd99a .cell execution_count=9}\n``` {.python .cell-code}\ndf.rdd.toDebugString()\n```\n:::\n\n\n# Basic statistics\n\nFirst we need to import some things:\n\n- `Window` class\n- SQL functions module\n- Some very useful functions\n- Spark types\n\n::: {#e096f00c .cell execution_count=10}\n``` {.python .cell-code}\nfrom pyspark.sql import Window\nimport pyspark.sql.functions as func\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import col, lit\n```\n:::\n\n\n## Compute the total number of unique users\n\n::: {#4e2b5995 .cell execution_count=11}\n``` {.python .cell-code}\n( \n    df.select('xid')\n      .distinct()\n      .count()\n)\n```\n:::\n\n\n::: {#7f53937a .cell execution_count=12}\n``` {.python .cell-code}\ndef foo(x): yield len(set(x))\n```\n:::\n\n\n::: {#7690dafd .cell execution_count=13}\n``` {.python .cell-code}\n( df.rdd\n    .map(lambda x : x.xid)\n    .mapPartitions(foo)\n    .collect()\n)\n```\n:::\n\n\nThis might pump up some computational resources \n\n::: {#f36ffbb2 .cell execution_count=14}\n``` {.python .cell-code}\n( \n    df.select('xid')\n      .distinct() \n      .explain()\n)\n```\n:::\n\n\n::: {.callout-note}\n\nThe distinct values of `xid` seem to be evenly spread among the six files making the `parquet` directory. Note that the last six partitions look empty. \n\n:::\n\n## Construct a column containing the total number of actions per user\n\n::: {#ef2e9296 .cell execution_count=15}\n``` {.python .cell-code}\nxid_partition = Window.partitionBy('xid')\n\nn_events = func.count(col('action')).over(xid_partition)\n\ndf = df.withColumn('n_events', n_events)\n\ndf.head(n=2)\n```\n:::\n\n\n::: {#e14fa55d .cell execution_count=16}\n``` {.python .cell-code}\n( \n  df\n    .groupBy('xid')\n    .agg(func.count('action'))\n    .head(5)\n)\n```\n:::\n\n\n## Construct a column containing the number of days since the last action of the user\n\n::: {#8a06d76b .cell execution_count=17}\n``` {.python .cell-code}\nmax_date = (\n  func\n    .max(col('date'))\n    .over(xid_partition)\n)\n\nn_days_since_last_event = func.datediff(func.current_date(), max_date)\n\ndf = df.withColumn('n_days_since_last_event',\n                   n_days_since_last_event)\n\ndf.head(n=2)\n```\n:::\n\n\n::: {#811f5328 .cell execution_count=18}\n``` {.python .cell-code}\ndf.printSchema()\n```\n:::\n\n\n## Construct a column containing the number of actions of each user for each modality of device\n\nDoes this `partitionBy` triggers shuffling? \n\n::: {#c6322032 .cell execution_count=19}\n``` {.python .cell-code}\nxid_device_partition = xid_partition.partitionBy('device')\n\nn_events_per_device = func.count(col('action')).over(xid_device_partition)\n\ndf = df.withColumn('n_events_per_device', n_events_per_device)\n\ndf.head(n=2)\n```\n:::\n\n\n## Number of devices per user {{< fa mug-hot >}}\n\n::: {#5cd6d049 .cell execution_count=20}\n``` {.python .cell-code}\n# xid_partition = Window.partitionBy('xid')\n\nrank_device = (\n  func\n    .dense_rank()\n    .over(xid_partition.orderBy('device'))\n)\n\nn_unique_device = (\n    func\n      .last(rank_device)\n      .over(xid_partition)\n)\n\ndf = df.withColumn('n_device', n_unique_device)\n\ndf.head(n=2)\n```\n:::\n\n\n::: {#77a94d17 .cell execution_count=21}\n``` {.python .cell-code}\ndf\\\n    .where(col('n_device') > 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .head(n=8)\n```\n:::\n\n\n::: {#7127993b .cell execution_count=22}\n``` {.python .cell-code}\ndf\\\n    .where(col('n_device') > 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .count()\n```\n:::\n\n\n# Let's select the correct users and build a training dataset\n\nWe construct a ETL (Extract Transform Load) process on this data using the `pyspark.sql` API.\n\n## Extraction\n\nHere extraction is just about reading the data\n\n::: {#88cd9f07 .cell execution_count=23}\n``` {.python .cell-code}\ndf = spark.read.parquet(input_file)\ndf.head(n=3)\n```\n:::\n\n\n## Transformation of the data\n\nAt this step we compute a lot of extra things from the data. The aim is to build *features* that describe users.\n\n::: {#7b446be0 .cell execution_count=24}\n``` {.python .cell-code}\ndef n_events_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    n_events = func.count(col('action')).over(xid_partition)\n    \n    df = df.withColumn('n_events', n_events)\n\n    return df\n```\n:::\n\n\n::: {#b2d08e97 .cell execution_count=25}\n``` {.python .cell-code}\ndef n_events_per_action_transformer(df):\n    xid_action_partition = Window.partitionBy('xid', 'action')\n    n_events_per_action = func.count(col('action')).over(xid_action_partition)\n\n    df = df.withColumn('n_events_per_action', n_events_per_action)\n    \n    return df\n```\n:::\n\n\n::: {#672edd2b .cell execution_count=26}\n``` {.python .cell-code}\ndef hour_transformer(df):\n    hour = func.hour(col('date'))\n    df = df.withColumn('hour', hour)\n    return df\n\ndef weekday_transformer(df):\n    weekday = func.date_format(col('date'), 'EEEE')\n    df = df.withColumn('weekday', weekday)\n    return df\n\ndef n_events_per_hour_transformer(df):\n    xid_hour_partition = Window.partitionBy('xid', 'hour')\n    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n    return df\n\ndef n_events_per_weekday_transformer(df):\n    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n    return df\n\ndef n_days_since_last_event_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    max_date = func.max(col('date')).over(xid_partition)\n    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n    df = df.withColumn('n_days_since_last_event',\n                       n_days_since_last_event + lit(0.1))\n    return df\n\ndef n_days_since_last_action_transformer(df):\n    xid_partition_action = Window.partitionBy('xid', 'action')\n    max_date = func.max(col('date')).over(xid_partition_action)\n    n_days_since_last_action = func.datediff(func.current_date(),\n                                                        max_date)\n    df = df.withColumn('n_days_since_last_action',\n                       n_days_since_last_action + lit(0.1))\n    return df\n\ndef n_unique_day_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    dayofyear = func.dayofyear(col('date'))\n    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n    n_unique_day = func.last(rank_day).over(xid_partition)\n    df = df.withColumn('n_unique_day', n_unique_day)\n    return df\n\ndef n_unique_hour_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n    n_unique_hour = func.last(rank_hour).over(xid_partition)\n    df = df.withColumn('n_unique_hour', n_unique_hour)\n    return df\n\ndef n_events_per_device_transformer(df):\n    xid_device_partition = Window.partitionBy('xid', 'device')\n    n_events_per_device = func.count(func.col('device')) \\\n        .over(xid_device_partition)\n    df = df.withColumn('n_events_per_device', n_events_per_device)\n    return df\n\ndef n_unique_device_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n    n_unique_device = func.last(rank_device).over(xid_partition)\n    df = df.withColumn('n_device', n_unique_device)\n    return df\n\ndef n_actions_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n                                                   'action')\n    n_actions_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n    return df\n\ndef n_unique_category_id_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_category_id = func.dense_rank().over(xid_partition\\\n                                              .orderBy('category_id'))\n    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n    return df\n\ndef n_events_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n    n_events_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n    return df\n\ndef n_events_per_website_id_transformer(df):\n    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n    n_events_per_website_id = func.count(col('action'))\\\n        .over(xid_website_id_partition)\n    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n    return df\n```\n:::\n\n\n::: {#60c1753e .cell execution_count=27}\n``` {.python .cell-code}\ntransformers = [\n    hour_transformer,\n    weekday_transformer,\n    n_events_per_hour_transformer,\n    n_events_per_weekday_transformer,\n    n_days_since_last_event_transformer,\n    n_days_since_last_action_transformer,\n    n_unique_day_transformer,\n    n_unique_hour_transformer,\n    n_events_per_device_transformer,\n    n_unique_device_transformer,\n    n_actions_per_category_id_transformer,\n    n_events_per_category_id_transformer,\n    n_events_per_website_id_transformer,\n]\n```\n:::\n\n\n::: {#8c8927d3 .cell execution_count=28}\n``` {.python .cell-code}\nN = 10000\n```\n:::\n\n\n::: {#bcd33728 .cell execution_count=29}\n``` {.python .cell-code}\nsample_df = df.sample(withReplacement=False, fraction=.05)\n```\n:::\n\n\n::: {#5434dc9e .cell execution_count=30}\n``` {.python .cell-code}\nsample_df.count()\n```\n:::\n\n\n::: {#206929d3 .cell execution_count=31}\n``` {.python .cell-code}\nfor transformer in transformers:\n    df = transformer(df)\n\ndf.head(n=1)\n```\n:::\n\n\n::: {#df2b5b79 .cell execution_count=32}\n``` {.python .cell-code}\nfor transformer in transformers:\n    sample_df = transformer(sample_df)\n\nsample_df.head(n=1)\n```\n:::\n\n\n::: {#f1164e90 .cell execution_count=33}\n``` {.python .cell-code}\ndf = sample_df\n```\n:::\n\n\n::: {#b22a4e7a .cell execution_count=34}\n``` {.python .cell-code}\nsorted(df.columns)\n```\n:::\n\n\n::: {#5705379b .cell execution_count=35}\n``` {.python .cell-code}\ndf.explain()\n```\n:::\n\n\n::: {#81df4617 .cell execution_count=36}\n``` {.python .cell-code}\nspark._sc.setCheckpointDir(\".\")   \n\ndf.checkpoint()\n```\n:::\n\n\n::: {#a5ace83b .cell execution_count=37}\n``` {.python .cell-code}\ndf.explain()\n```\n:::\n\n\n## Load step\n\nHere, we use all the previous computations (saved in the columns of the dataframe) \nto compute aggregated informations about each user.\n\n\n::: {.callout-note}\n\nThis should be DRYED \n\n:::\n\n::: {#ba2d216b .cell execution_count=38}\n``` {.python .cell-code}\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct() \n            # action\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    \n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    \n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_weekday_loader(df):\n    csr = df\\\n        .select('xid', 'weekday', 'n_events_per_weekday')\\\n        .withColumnRenamed('n_events_per_weekday', 'value')\\\n        .distinct()\n\n    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n    \n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('weekday')\n\n    return csr\n\ndef n_days_since_last_event_loader(df):\n    csr = df.select('xid',  'n_days_since_last_event')\\\n        .withColumnRenamed('n_days_since_last_event', 'value')\\\n        .distinct()\n    feature_name = lit('n_days_since_last_event')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_days_since_last_action_loader(df):\n    csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n        .withColumnRenamed('n_days_since_last_action', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('action')\n    return csr\n\ndef n_unique_day_loader(df):\n    csr = df.select('xid', 'n_unique_day')\\\n        .withColumnRenamed('n_unique_day', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_day')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_unique_hour_loader(df):\n    csr = df.select('xid', 'n_unique_hour')\\\n        .withColumnRenamed('n_unique_hour', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_hour')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_device_loader(df):\n    csr = df\\\n        .select('xid', 'device', 'n_events_per_device')\\\n        .withColumnRenamed('n_events_per_device', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('device')\n    return csr\n\ndef n_unique_device_loader(df):\n    csr = df.select('xid', 'n_device')\\\n        .withColumnRenamed('n_device', 'value')\\\n        .distinct()\n    feature_name = lit('n_device')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n        .withColumnRenamed('n_events_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_category_id#'),\n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\n    return csr\n\ndef n_actions_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n        .withColumnRenamed('n_actions_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_actions_per_category_id#'),\n                               col('action'), lit('#'), \n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\\\n        .drop('action')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_website_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n```\n:::\n\n\n::: {#347e6208 .cell execution_count=39}\n``` {.python .cell-code}\nfrom functools import reduce\n```\n:::\n\n\n::: {#5d71bdc0 .cell execution_count=40}\n``` {.python .cell-code}\nloaders = [\n    n_events_per_hour_loader,\n    n_events_per_website_id_loader,\n    n_events_per_hour_loader,\n    n_events_per_weekday_loader,\n    n_days_since_last_event_loader,\n    n_days_since_last_action_loader,\n    n_unique_day_loader,\n    n_unique_hour_loader,\n    n_events_per_device_loader,\n    n_unique_device_loader,\n    n_events_per_category_id_loader,\n    n_actions_per_category_id_loader,\n    n_events_per_website_id_loader,\n]\n```\n:::\n\n\n::: {#c70542de .cell execution_count=41}\n``` {.python .cell-code}\ndef union(df, other):\n    return df.union(other)\n```\n:::\n\n\n::: {.callout-caution title=\"About DataFrame.union()\"}\n\nThis method performs a SQL-style set union of the rows from both DataFrame objects, with no automatic deduplication of elements.\n\nUse the distinct() method to perform deduplication of rows.\n\nThe method resolves columns by position (not by name), following the standard behavior in SQL.\n\n:::\n\n::: {#96a459d8 .cell execution_count=42}\n``` {.python .cell-code}\nspam = [loader(df) for loader in loaders]\n```\n:::\n\n\n::: {#916723ce .cell execution_count=43}\n``` {.python .cell-code}\nspam[0].printSchema()\n```\n:::\n\n\n::: {#bc76c15e .cell execution_count=44}\n``` {.python .cell-code}\nall(spam[0].columns == it.columns for it in spam[1:])\n```\n:::\n\n\n::: {#e9f513ae .cell execution_count=45}\n``` {.python .cell-code}\nlen(spam)\n```\n:::\n\n\n::: {#0647a437 .cell execution_count=46}\n``` {.python .cell-code}\ncsr = reduce(\n    lambda df1, df2: df1.union(df2),\n    spam\n)\n\ncsr.head(n=3)\n```\n:::\n\n\n::: {#e718055b .cell ExecuteTime='{\"end_time\":\"2020-05-03T15:25:54.862814Z\",\"start_time\":\"2020-05-03T15:25:54.857914Z\"}' execution_count=47}\n``` {.python .cell-code}\ncsr.columns\n```\n:::\n\n\n::: {#67db5464 .cell ExecuteTime='{\"end_time\":\"2020-05-03T15:26:13.629146Z\",\"start_time\":\"2020-05-03T15:25:55.683800Z\"}' execution_count=48}\n``` {.python .cell-code}\ncsr.show(5)\n```\n:::\n\n\n::: {#490e8690 .cell execution_count=49}\n``` {.python .cell-code}\ncsr.rdd.getNumPartitions()\n```\n:::\n\n\n::: {#eac1f68f .cell ExecuteTime='{\"end_time\":\"2020-05-03T15:30:20.643141Z\",\"start_time\":\"2020-05-03T15:29:45.221790Z\"}' execution_count=50}\n``` {.python .cell-code}\n# Replace features names and xid by a unique number\nfeature_name_partition = Window().orderBy('feature_name')\n\nxid_partition = Window().orderBy('xid')\n\ncol_idx = func.dense_rank().over(feature_name_partition)\nrow_idx = func.dense_rank().over(xid_partition)\n```\n:::\n\n\n::: {#28b781fa .cell ExecuteTime='{\"end_time\":\"2020-05-03T15:30:20.643141Z\",\"start_time\":\"2020-05-03T15:29:45.221790Z\"}' execution_count=51}\n``` {.python .cell-code}\ncsr = csr.withColumn('col', col_idx)\\\n    .withColumn('row', row_idx)\n\ncsr = csr.na.drop('any')\n\ncsr.head(n=5)\n```\n:::\n\n\n::: {#2e67f48b .cell ExecuteTime='{\"end_time\":\"2020-05-03T15:32:02.552364Z\",\"start_time\":\"2020-05-03T15:31:14.990298Z\"}' execution_count=52}\n``` {.python .cell-code}\n# Let's save the result of our hard work into a new parquet file\noutput_path = './'\noutput_file = os.path.join(output_path, 'csr.parquet')\ncsr.write.parquet(output_file, mode='overwrite')\n```\n:::\n\n\n# Preparation of the training dataset\n\n::: {#49da4ba2 .cell execution_count=53}\n``` {.python .cell-code}\ncsr_path = './'\ncsr_file = os.path.join(csr_path, 'csr.parquet')\n\ndf = spark.read.parquet(csr_file)\ndf.head(n=5)\n```\n:::\n\n\n::: {#771ab4ea .cell ExecuteTime='{\"end_time\":\"2020-05-03T15:33:17.229477Z\",\"start_time\":\"2020-05-03T15:33:16.995048Z\"}' execution_count=54}\n``` {.python .cell-code}\ndf.count()\n```\n:::\n\n\n::: {#9b9926ca .cell ExecuteTime='{\"end_time\":\"2020-05-03T15:33:20.881392Z\",\"start_time\":\"2020-05-03T15:33:19.624525Z\"}' execution_count=55}\n``` {.python .cell-code}\n# What are the features related to campaign_id 1204 ?\nfeatures_names = \\\n    df.select('feature_name')\\\n    .distinct()\\\n    .toPandas()['feature_name']\n```\n:::\n\n\n::: {#99660614 .cell ExecuteTime='{\"end_time\":\"2020-05-03T15:33:21.818568Z\",\"start_time\":\"2020-05-03T15:33:21.812810Z\"}' execution_count=56}\n``` {.python .cell-code}\nfeatures_names\n```\n:::\n\n\n::: {#f032fc63 .cell ExecuteTime='{\"end_time\":\"2020-05-03T15:33:27.083141Z\",\"start_time\":\"2020-05-03T15:33:27.078374Z\"}' execution_count=57}\n``` {.python .cell-code}\n[feature_name for feature_name in features_names if '1204' in feature_name]\n```\n:::\n\n\n::: {#f166bcad .cell ExecuteTime='{\"end_time\":\"2020-05-03T15:33:28.560631Z\",\"start_time\":\"2020-05-03T15:33:27.903921Z\"}' execution_count=58}\n``` {.python .cell-code}\n# Look for the xid that have at least one exposure to campaign 1204\nkeep = func.when(\n    (col('feature_name') == 'n_actions_per_category_id#C#1204.0') |\n    (col('feature_name') == 'n_actions_per_category_id#O#1204.0'),\n    1).otherwise(0)\ndf = df.withColumn('keep', keep)\n\ndf.where(col('keep') > 0).count()\n```\n:::\n\n\n::: {#4b3338b9 .cell execution_count=59}\n``` {.python .cell-code}\n# Sum of the keeps :)\nxid_partition = Window.partitionBy('xid')\nsum_keep = func.sum(col('keep')).over(xid_partition)\ndf = df.withColumn('sum_keep', sum_keep)\n```\n:::\n\n\n::: {#3f63b47d .cell execution_count=60}\n``` {.python .cell-code}\n# Let's keep the xid exposed to 1204\ndf = df.where(col('sum_keep') > 0)\n```\n:::\n\n\n::: {#0bfb2dac .cell execution_count=61}\n``` {.python .cell-code}\ndf.count()\n```\n:::\n\n\n::: {#1b2f882d .cell execution_count=62}\n``` {.python .cell-code}\ndf.select('xid').distinct().count()\n```\n:::\n\n\n::: {#c5ab0cf2 .cell execution_count=63}\n``` {.python .cell-code}\nrow_partition = Window().orderBy('row')\ncol_partition = Window().orderBy('col')\n\nrow_new = func.dense_rank().over(row_partition)\ncol_new = func.dense_rank().over(col_partition)\n\ndf = df.withColumn('row_new', row_new)\ndf = df.withColumn('col_new', col_new)\n\ncsr_data = df.select('row_new', 'col_new', 'value').toPandas()\n```\n:::\n\n\n::: {#cacd093c .cell ExecuteTime='{\"end_time\":\"2020-05-03T15:33:52.617724Z\",\"start_time\":\"2020-05-03T15:33:52.609488Z\"}' execution_count=64}\n``` {.python .cell-code}\ncsr_data.head()\n```\n:::\n\n\n::: {#c13eb352 .cell execution_count=65}\n``` {.python .cell-code}\nfeatures_names = df.select('feature_name', 'col_new').distinct()\nfeatures_names.where(col('feature_name') == 'n_actions_per_category_id#C#1204.0').head()\n```\n:::\n\n\n::: {#9a16d677 .cell execution_count=66}\n``` {.python .cell-code}\nfeatures_names.where(col('feature_name') == 'n_actions_per_category_id#O#1204.0').head()\n```\n:::\n\n\n::: {#8f09006f .cell ExecuteTime='{\"end_time\":\"2020-05-03T15:34:11.510538Z\",\"start_time\":\"2020-05-03T15:34:11.454802Z\"}' execution_count=67}\n``` {.python .cell-code}\nfrom scipy.sparse import csr_matrix\nimport numpy as np\n\nrows = csr_data['row_new'].values - 1\ncols = csr_data['col_new'].values - 1\nvals = csr_data['value'].values\n\nX_csr = csr_matrix((vals, (rows, cols)))\n```\n:::\n\n\n::: {#058e90fa .cell ExecuteTime='{\"end_time\":\"2020-05-03T15:34:11.977267Z\",\"start_time\":\"2020-05-03T15:34:11.972602Z\"}' execution_count=68}\n``` {.python .cell-code}\nX_csr.shape\n```\n:::\n\n\n::: {#d1c15d50 .cell execution_count=69}\n``` {.python .cell-code}\nX_csr.shape, X_csr.nnz\n```\n:::\n\n\n::: {#6025957a .cell execution_count=70}\n``` {.python .cell-code}\nX_csr.nnz / (X_csr.shape[0]* X_csr.shape[1])   # 0152347 * 92)\n```\n:::\n\n\n::: {#e5e24a1d .cell execution_count=71}\n``` {.python .cell-code}\n# The label vector. Let's make it dense, flat and binary\ny = np.array(X_csr[:, 1].todense()).ravel()\ny = np.array(y > 0, dtype=np.int64)\n```\n:::\n\n\n::: {#3d92d068 .cell execution_count=72}\n``` {.python .cell-code}\nX_csr.shape\n```\n:::\n\n\n::: {#6f03194c .cell execution_count=73}\n``` {.python .cell-code}\n# We remove the second and fourth column. \n# It actually contain the label we'll want to predict.\nkept_cols = list(range(X_csr.shape[1]))\nkept_cols.pop(1)\nkept_cols.pop(2)\nX = X_csr[:, kept_cols]\n```\n:::\n\n\n::: {#9587f3bd .cell execution_count=74}\n``` {.python .cell-code}\nlen(kept_cols)\n```\n:::\n\n\n::: {#364d972c .cell execution_count=75}\n``` {.python .cell-code}\nX_csr.shape, X.shape\n```\n:::\n\n\n## Finally !!\n\nWow ! That was a lot of work. Now we have a features matrix $X$ and a vector of labels $y$.\n\n::: {#9307e51c .cell execution_count=76}\n``` {.python .cell-code}\nX.indices\n```\n:::\n\n\n::: {#598e52c2 .cell execution_count=77}\n``` {.python .cell-code}\nX.indptr\n```\n:::\n\n\n::: {#81891a9b .cell execution_count=78}\n``` {.python .cell-code}\nX.shape, X.nnz\n```\n:::\n\n\n::: {#acbbe365 .cell execution_count=79}\n``` {.python .cell-code}\ny.shape, y.sum()\n```\n:::\n\n\n# Some learning for/from this data\n\n::: {#b9023330 .cell execution_count=80}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Normalize the features\nX = MaxAbsScaler().fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)\n\nclf = LogisticRegression(\n    penalty='l2',\n    C=1e3,\n    solver='lbfgs',\n    class_weight='balanced'\n)\n\nclf.fit(X_train, y_train)\n```\n:::\n\n\n::: {#010202df .cell execution_count=81}\n``` {.python .cell-code}\nfeatures_names = features_names.toPandas()['feature_name']\n```\n:::\n\n\n::: {#92def0c6 .cell execution_count=82}\n``` {.python .cell-code}\nfeatures_names[range(6)]\n```\n:::\n\n\n::: {#5fff01c6 .cell execution_count=83}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n%matplotlib inline\n```\n:::\n\n\n::: {#9eebc966 .cell execution_count=84}\n``` {.python .cell-code}\nplt.figure(figsize=(16, 5))\nplt.stem(clf.coef_[0]) # , use_line_collection=True)\nplt.title('Logistic regression coefficients', fontsize=18)\n```\n:::\n\n\n::: {#f3396510 .cell execution_count=85}\n``` {.python .cell-code}\nclf.coef_[0].shape[0]\n```\n:::\n\n\n::: {#ddec1aa7 .cell execution_count=86}\n``` {.python .cell-code}\nlen(features_names)\n```\n:::\n\n\n::: {#a7706cde .cell execution_count=87}\n``` {.python .cell-code}\n# We change the fontsize of minor ticks label\n_ = plt.xticks(np.arange(clf.coef_[0].shape[0]), features_names, \n           rotation='vertical', fontsize=8)\n```\n:::\n\n\n::: {#c8609d2e .cell execution_count=88}\n``` {.python .cell-code}\n_ = plt.yticks(fontsize=14)\n```\n:::\n\n\n::: {#f534e3d7 .cell ExecuteTime='{\"end_time\":\"2020-05-03T15:51:25.280157Z\",\"start_time\":\"2020-05-03T15:51:25.081464Z\"}' execution_count=89}\n``` {.python .cell-code}\nfrom sklearn.metrics import precision_recall_curve, f1_score\n\nprecision, recall, _ = precision_recall_curve(y_test, clf.predict_proba(X_test)[:, 1])\n    \nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label='LR (F1=%.2f)' % f1_score(y_test, clf.predict(X_test)), lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Recall', fontsize=16)\nplt.ylabel('Precision', fontsize=16)\nplt.title('Precision/recall curve', fontsize=18)\nplt.legend(loc=\"upper right\", fontsize=14)\n```\n:::\n\n\n# Analyse the tables \n\n::: {#03c15bc6 .cell execution_count=90}\n``` {.python .cell-code}\nquery = \"\"\"ANALYZE TABLE db_table COMPUTE STATISTICS\n            FOR COLUMNS xid\"\"\"\n```\n:::\n\n\n::: {#cb68c9de .cell execution_count=91}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"db_table\")\n```\n:::\n\n\n::: {#2302baa8 .cell execution_count=92}\n``` {.python .cell-code}\ndf.columns\n```\n:::\n\n\n::: {#cd3683f6 .cell execution_count=93}\n``` {.python .cell-code}\nspark.sql(\"cache table db_table\")\n```\n:::\n\n\n::: {#d670a103 .cell execution_count=94}\n``` {.python .cell-code}\nspark.sql(query)\n```\n:::\n\n\n::: {#ead4eae8 .cell execution_count=95}\n``` {.python .cell-code}\nspark.sql(\"show tables\")\n```\n:::\n\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: /usr/share/jupyter/kernels/python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.12.3\n---\n",
    "supporting": [
      "notebook08_webdata-II_files"
    ],
    "filters": []
  }
}