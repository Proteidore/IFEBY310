{
  "hash": "3b078cc4edc3d6c7aafdb3667e4155ca",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Introduction to `Spark` RDD\njupyter: python3\n---\n\n::: {#e125357c .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n```\n:::\n\n\n::: {#18900557 .cell execution_count=2}\n``` {.python .cell-code}\nimport os\nimport sys\nimport inspect\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n```\n:::\n\n\n::: {#c5926b08 .cell execution_count=3}\n``` {.python .cell-code}\nfrom pyspark import SparkConf, SparkContext\n\nconf = SparkConf().setAppName(\"Spark RDD Course\")\nsc = SparkContext(conf=conf)\n```\n:::\n\n\n::: {#b56c2ef1 .cell execution_count=4}\n``` {.python .cell-code}\nrdd = sc.parallelize(range(64))\n```\n:::\n\n\nNote that `parallelize` takes an optional argument to choose the number of partitions\n\n::: {#dc84310a .cell execution_count=5}\n``` {.python .cell-code}\nrdd.getNumPartitions()\n```\n:::\n\n\n::: {#e4b060e6 .cell execution_count=6}\n``` {.python .cell-code}\nrdd = sc.parallelize(range(1000), 10)\nrdd.getNumPartitions()\n```\n:::\n\n\n## Transformations\n\n### `map`\n\n::: {#05d8238b .cell execution_count=7}\n``` {.python .cell-code}\nrdd = sc.parallelize([2, 3, 4])\nrdd = rdd.map(lambda x: list(range(1, x)))\n```\n:::\n\n\n::: {#50a876ce .cell execution_count=8}\n``` {.python .cell-code}\nrdd\n```\n:::\n\n\n::: {#8ef7e966 .cell execution_count=9}\n``` {.python .cell-code}\n(\n    sc.parallelize([2, 3, 4])\n      .map(lambda x: list(range(1, x)))\n)\n```\n:::\n\n\n`map` is a *transformation*. It is *lazily* evaluated. Hence execution is delayed until an *action* is met in the DAG).\n\n::: {#6dd6d9b5 .cell execution_count=10}\n``` {.python .cell-code}\nrdd.collect()  # collect is an action \n```\n:::\n\n\n::: {#b99d3ecf .cell execution_count=11}\n``` {.python .cell-code}\n(\n    sc.parallelize([2, 3, 4])\n      .map(lambda x: list(range(1, x)))\n      .collect()\n)\n```\n:::\n\n\n### Exercice: `map` with a method\n\n**Warning.** This example is a bad practice !!! Don't do this at home\n\n::: {#0602ef2a .cell execution_count=12}\n``` {.python .cell-code}\ndbtel = {'arthur': 1234, 'riad': 4567, 'anatole': 3615}\n```\n:::\n\n\n::: {#ffd5b616 .cell execution_count=13}\n``` {.python .cell-code}\nclass TelephoneDB(object):\n    \n    def __init__(self):\n        self.tel = {'arthur': 1234, 'riad': 4567, 'anatole': 3615}\n   \n    def add_tel(self, name):\n        return name, self.tel.get(name)\n```\n:::\n\n\n::: {#5d421268 .cell execution_count=14}\n``` {.python .cell-code}\ntel_db = TelephoneDB()\nnames = ['arthur', 'riad']\n```\n:::\n\n\n::: {#611249a4 .cell execution_count=15}\n``` {.python .cell-code}\nrdd = (\n    sc\n        .parallelize(names)\n        .map(tel_db.add_tel)\n        .collect()\n)\n\nrdd\n```\n:::\n\n\n- Replace the `tel` dictionary by a `defaultdict` with default number `999` \n- Use it on a `rdd` containing names as above including an unknown one, and try it\n\n::: {#aea4ce33 .cell execution_count=16}\n``` {.python .cell-code}\nfrom collections import defaultdict\n\nclass TelephoneDefaultDB(object):\n    \n    def __init__(self):\n        self.tel = defaultdict(lambda: 999, {'arthur': 1234, 'riad': 4567, 'anatole': 3615})\n    \n    def add_tel(self, name):\n        return name, self.tel[name]\n    \n    def add_tel_rdd(self, rdd):  \n        return rdd.map(self.add_tel)\n```\n:::\n\n\n::: {#63b99c8a .cell execution_count=17}\n``` {.python .cell-code}\ntel_db = TelephoneDefaultDB()\nnames = ['riad', 'anatole', 'yiyang']\nrdd = (\n    sc\n        .parallelize(names)\n        .map(tel_db.add_tel)\n        .collect()\n)\nrdd\n```\n:::\n\n\n::: {.callout-caution}\n\nIt is a bad idea to pass *methods* to spark's `map`.\nSince `add_tel` needs `self`, the whole object is serialized so that `spark` can use it.\n\nThis breaks if the `tel` is large, or if it is not serializable.\n\n:::\n\n### `flatMap`\n\n::: {#dfee5b5d .cell execution_count=18}\n``` {.python .cell-code}\nrdd = sc.parallelize([2, 3, 4, 5])\n( \n    rdd\n        .flatMap(lambda x: range(1, x))\n        .collect()\n)\n```\n:::\n\n\n### `filter`\n\n::: {#460356d7 .cell execution_count=19}\n``` {.python .cell-code}\nrdd = sc.parallelize(range(10))\n\nrdd\\\n    .filter(lambda x: x % 2 == 0)\\\n    .collect()\n```\n:::\n\n\n### `distinct`\n\n::: {#a54072e6 .cell execution_count=20}\n``` {.python .cell-code}\nrdd = sc.parallelize([1, 1, 4, 2, 1, 3, 3])\nrdd.distinct().collect()\n```\n:::\n\n\n### \"Pseudo-set\" operations\n\n::: {#ee655ddd .cell execution_count=21}\n``` {.python .cell-code}\nrdd1 = sc.parallelize(range(5))\nrdd2 = sc.parallelize(range(3, 9))\nrdd3 = rdd1.union(rdd2)\nrdd3.collect()\n```\n:::\n\n\n::: {#50079034 .cell execution_count=22}\n``` {.python .cell-code}\nrdd3.distinct().collect()\n```\n:::\n\n\n::: {#7b6b4f04 .cell execution_count=23}\n``` {.python .cell-code}\nrdd1 = sc.parallelize([1, 2])\nrdd2 = sc.parallelize([\"a\", \"b\"])\nrdd1.cartesian(rdd2).collect()\n```\n:::\n\n\n## Actions\n\n`collect` is obviously an action...\n\n### `count`, `countByValue`\n\n::: {#2bd42477 .cell execution_count=24}\n``` {.python .cell-code}\nrdd = sc.parallelize([1, 3, 1, 2, 2, 2])\nrdd.count()\n```\n:::\n\n\n::: {#02f93b0c .cell execution_count=25}\n``` {.python .cell-code}\nrdd.countByValue()\n```\n:::\n\n\nWhy does `countByValue()` returns a dictionary?\n\nAre `count()` and `countByValue()` actions or transformations?\n\n::: {#0a4a9823 .cell execution_count=26}\n``` {.python .cell-code}\nu = np.int32((np.random.sample(100000) * 100000))  # 100000 random integers uniformly distributed on 0, ..., 100000\n\np = (\n    sc.parallelize(u)\n    .countByValue()\n)\n\nq = sorted(\n    p.items(), \n    key = lambda x : x[1], \n    reverse=True\n)\n\nq[0:10]\n\nq[0], 1 + np.log(len(u))/ np.log(np.log(len(u))), len(q)\n```\n:::\n\n\n- How many distinct values do you expect in `u` ?\n- How large is the largest value in $q$ ?\n\n::: {#1c1bafa7 .cell execution_count=27}\n``` {.python .cell-code}\nfrom scipy.stats import poisson \n\n( \n    len(q), \n    (1-np.exp(-1)) * len(u),\n    poisson.ppf(1.-1./len(u), 1)\n)\n```\n:::\n\n\n### `take`, `takeOrdered`\n\n::: {#c2f4973b .cell execution_count=28}\n``` {.python .cell-code}\nrdd = sc.parallelize([(3, 'a'), (1, 'b'), (2, 'd')])\n```\n:::\n\n\n::: {#0214c36f .cell execution_count=29}\n``` {.python .cell-code}\n(1, 'b') <=  (2, 'd') <= (3, 'a')\n```\n:::\n\n\n::: {#653f9b7f .cell execution_count=30}\n``` {.python .cell-code}\nrdd.takeOrdered(2)\n```\n:::\n\n\n::: {#de5a8ccb .cell execution_count=31}\n``` {.python .cell-code}\nrdd.takeOrdered(2, key=lambda x: x[1])\n```\n:::\n\n\n### `reduce`, `fold`\n\n::: {#0d4c10d1 .cell execution_count=32}\n``` {.python .cell-code}\nrdd = sc.range(1, 4)\nrdd.reduce(lambda a, b: a + b)\n```\n:::\n\n\n::: {#d2cd09f6 .cell execution_count=33}\n``` {.python .cell-code}\nrdd = sc.range(1, 4, numSlices=7)\nrdd.reduce(lambda a, b: a + b)\n```\n:::\n\n\n::: {#6821e106 .cell execution_count=34}\n``` {.python .cell-code}\nrdd = sc.parallelize(range(1,4), 3)\nrdd.reduce(lambda a, b: a + b)\n```\n:::\n\n\n::: {#8ff254b9 .cell execution_count=35}\n``` {.python .cell-code}\n( \n    sc.parallelize(range(1, 4), 2)\n      .fold(0, lambda a, b: a + b)\n)\n```\n:::\n\n\n::: {#139d52f9 .cell execution_count=36}\n``` {.python .cell-code}\n( \n    sc.parallelize(range(1, 4), 1)\n      .fold(3, lambda a, b: a + b)\n),( \n    sc.parallelize(range(1, 4), 2)\n      .fold(2, lambda a, b: a + b)\n)\n```\n:::\n\n\n::: {#1a3bfaed .cell execution_count=37}\n``` {.python .cell-code}\nrdd =  sc.parallelize(range(1, 4),3)\n( \n    rdd.fold(1, lambda a, b: a + b), \n    rdd.getNumPartitions()\n)\n```\n:::\n\n\n::: {#a5f51b42 .cell execution_count=38}\n``` {.python .cell-code}\nrdd =  sc.parallelize(range(1, 4),4)\n\n(\n    rdd.fold(1, lambda a, b: a + b), \n    rdd.getNumPartitions()\n)\n```\n:::\n\n\n::: {#a2a31f02 .cell execution_count=39}\n``` {.python .cell-code}\nrdd = sc.parallelize([1, 2, 4], 2)\nrdd.fold(2, lambda a, b: a + b)\n```\n:::\n\n\n::: {#42ee5d8d .cell execution_count=40}\n``` {.python .cell-code}\nrdd = sc.parallelize([1, 2, 4], 3)\nrdd.fold(2, lambda a, b: a + b)\n```\n:::\n\n\n::: {#a851b6ab .cell execution_count=41}\n``` {.python .cell-code}\nrdd.getNumPartitions()\n```\n:::\n\n\n### `aggregate`\n\n::: {#606d2bb7 .cell execution_count=42}\n``` {.python .cell-code}\nseqOp = lambda x, y: (x[0] + y, x[1] + 1)\ncombOp = lambda x, y: (x[0] + y[0], x[1] + y[1])\n\nrdd = sc.parallelize([1, 2, 3, 4], 8)\n(\n    rdd.aggregate((0, 0), seqOp, combOp), rdd.getNumPartitions()\n)\n```\n:::\n\n\n::: {#a016bdaf .cell execution_count=43}\n``` {.python .cell-code}\nop = lambda x, y: x+y\nrdd = sc.parallelize([1, 2, 3, 4], 4)\n(\n    rdd.aggregate(0, op, op),\n    rdd.getNumPartitions()\n)\n```\n:::\n\n\n### Exercice: sum of powers with `aggregate`\n\n- Using `aggregate`, compute the sum, the sum of squares $x^2$ and the sum of cubes $x^3$ for \n$x \\in \\{1, \\ldots, 10 \\}$.\n- Check your computations using `numpy`\n\n::: {#3d821919 .cell execution_count=44}\n``` {.python .cell-code}\nseqOp = lambda x, y: (x[0] + y, x[1] + y ** 2, x[2] + y ** 3)\n```\n:::\n\n\n::: {#4bb8c97e .cell execution_count=45}\n``` {.python .cell-code}\ncombOp = lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2])\n```\n:::\n\n\n::: {#17e14ad7 .cell execution_count=46}\n``` {.python .cell-code}\nsc.range(5)\n```\n:::\n\n\n::: {#751d893e .cell execution_count=47}\n``` {.python .cell-code}\n( \n    sc\n        .range(1, 11)\n        .aggregate((0, 0, 0), seqOp, combOp)\n)\n```\n:::\n\n\n::: {#312581bf .cell execution_count=48}\n``` {.python .cell-code}\nimport numpy as np\n\nx = np.arange(1, 11)\nx\n```\n:::\n\n\n::: {#59bf139c .cell execution_count=49}\n``` {.python .cell-code}\nx.sum(), (x**2).sum(), (x**3).sum(), x.cumsum()\n```\n:::\n\n\n### Computing an empirical variance with `aggregate`\n\nAssume a sample is stored as a RDD. Using `aggregate`, compute the sample variance $\\frac{1}{n}\\sum_{i=1}^n (x_i - \\overline{X}_n)^2$ where $\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n x_i$ \n\n# `PairRDD`\n\n::: {#0bc884c3 .cell execution_count=50}\n``` {.python .cell-code}\nrdd = sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\n\nrdd.collect()  # not yet \n```\n:::\n\n\n::: {#04f118a7 .cell execution_count=51}\n``` {.python .cell-code}\nrdd = rdd.map(lambda x: (x[0], x[1:]))\n\nrdd.collect()  # done \n```\n:::\n\n\n## Transformations\n\n### `keys`, `values`\n\n::: {#0f153047 .cell execution_count=52}\n``` {.python .cell-code}\nrdd.keys().collect()\n```\n:::\n\n\n::: {#2babf415 .cell execution_count=53}\n``` {.python .cell-code}\nrdd.values().collect()\n```\n:::\n\n\n::: {.callout-warning}\n\nAll elements must be tuples with two elements (key and  value)\n:::\n\n::: {#7cffda53 .cell execution_count=54}\n``` {.python .cell-code}\nrdd = sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\nrdd.keys().collect()\n```\n:::\n\n\n::: {#c6014308 .cell execution_count=55}\n``` {.python .cell-code}\nrdd.values().collect()\n```\n:::\n\n\nThe values are *not* what we expected wrong... so we *must* do\n\n::: {#65c1eeb3 .cell execution_count=56}\n``` {.python .cell-code}\nrdd = ( sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\n          .map(lambda x: (x[0], x[1:]))\n      )\nrdd.keys().collect()\n```\n:::\n\n\n::: {#e7015d02 .cell execution_count=57}\n``` {.python .cell-code}\nrdd.values().collect()\n```\n:::\n\n\nNow, the values are correct. \n\n### `mapValues`, `flatMapValues`\n\n::: {#2309aa8c .cell execution_count=58}\n``` {.python .cell-code}\nrdd = sc.parallelize([(\"a\", \"x y z\"), (\"b\", \"p r\")])\n\nrdd.mapValues(lambda v: v.split(' ')).collect(), rdd.collect()\n```\n:::\n\n\n::: {#cdf43f6a .cell execution_count=59}\n``` {.python .cell-code}\nrdd.flatMapValues(lambda v: v.split(' ')).collect()\n```\n:::\n\n\n### `groupByKey`\n\n::: {#1b022924 .cell execution_count=60}\n``` {.python .cell-code}\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 42)])\n( \n    rdd.groupByKey()\n       .mapValues(list)\n       .collect()\n)\n```\n:::\n\n\n::: {#77552f15 .cell execution_count=61}\n``` {.python .cell-code}\nrdd.groupByKey().collect()\n```\n:::\n\n\n### `reduceByKey`\n\n::: {#c30a654d .cell execution_count=62}\n``` {.python .cell-code}\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\nrdd.reduceByKey(lambda a, b: a + b).collect()\n```\n:::\n\n\n### `combineByKey`\n\n::: {#1d5be751 .cell execution_count=63}\n``` {.python .cell-code}\nrdd = sc.parallelize([('a', 1), ('b', 2), ('a', 13)])\n\ndef add(a, b): \n    return a + str(b)\n\nrdd.combineByKey(str, add, add).collect()\n```\n:::\n\n\n### `join`, `rightOuterJoin`, `leftOuterJoin`\n\n::: {#d4c057b5 .cell execution_count=64}\n``` {.python .cell-code}\nemployees = sc.parallelize([\n    (31, \"Rafferty\"),\n    (33, \"Jones\"),\n    (33, \"Heisenberg\"),\n    (34, \"Robinson\"),\n    (34, \"Smith\"),\n    (None, \"Williams\")\n])\n```\n:::\n\n\n::: {#4058d842 .cell execution_count=65}\n``` {.python .cell-code}\ndepartments = sc.parallelize([\n    (31, \"Sales\"),\n    (33, \"Engineering\"),\n    (34, \"Clerical\"),\n    (35, \"Marketing\")\n])\n```\n:::\n\n\n::: {#79e5f7ad .cell execution_count=66}\n``` {.python .cell-code}\n( \n    employees\n        .join(departments)\n        .sortByKey()\n        .collect()\n)\n```\n:::\n\n\n::: {#e3498380 .cell execution_count=67}\n``` {.python .cell-code}\n( \n    employees\n        .rightOuterJoin(departments)\n        .sortByKey()\n        .collect()\n)\n```\n:::\n\n\n::: {#89521a18 .cell execution_count=68}\n``` {.python .cell-code}\n(\n    employees\n        .leftOuterJoin(departments)\n        .collect()\n)\n```\n:::\n\n\n## Actions\n\n::: {#a081f30f .cell execution_count=69}\n``` {.python .cell-code}\nemployees.countByKey()\n```\n:::\n\n\n::: {#fff93787 .cell execution_count=70}\n``` {.python .cell-code}\nemployees.lookup(33)\n```\n:::\n\n\n::: {#10adbf4f .cell execution_count=71}\n``` {.python .cell-code}\nemployees.lookup(None)\n```\n:::\n\n\n::: {#7b8ff7ac .cell execution_count=72}\n``` {.python .cell-code}\nemployees.collectAsMap()\n```\n:::\n\n\n## References\n\n[Spark Core reference](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: /usr/share/jupyter/kernels/python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.12.3\n---\n",
    "supporting": [
      "notebook05_sparkrdd_files"
    ],
    "filters": []
  }
}