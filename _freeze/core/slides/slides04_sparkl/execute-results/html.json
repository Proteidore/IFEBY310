{
  "hash": "15d718928c104775bb0d58989d49ef5f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Spark SQL\"\nengine: knitr\ndate: \"2025-01-17\"\n---\n\n\n\n \n\n\n# Spark SQL  Bird Eye View  {background-color=\"#1c191c\"}\n\n\n\n## [PySpark overview](https://spark.apache.org/docs/latest/api/python/)\n\n![](./IMG/pyspark-apis.png)\n\n[Official documentation](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n\n\n\n\n## Overview\n\n- `Spark SQL` is a library included in `Spark` since version 1.3\n\n- `Spark Dataframes` was introduced with version \n\n- It provides an *easier interface to process tabular data*\n\n- Instead of `RDD`s, we deal with `DataFrame`s\n\n- Since `Spark` 1.6, there is also the concept of `Dataset`s, but only for `Scala` and `Java`\n\n\n\n\n## `SparkContext` and `SparkSession`\n\n- Before `Spark 2`, there was only `SparkContext` and `SQLContext`\n\n- All core functionality was accessed with `SparkContext`\n\n- All `SQL` functionality needed the `SQLContext`, which can be created from an `SparkContext`\n\n- With `Spark 2` came the `SparkSession` class\n\n- `SparkSession` is the .stress[*global entry-point*] for everything `Spark`-related\n\n::: {.notes}\n\n- `SparkContext` was enough for handling RDDs\n- Purpose of `SQLContext` ?\n- Could we use `SparkSession` to handle RDDs?\n\n:::\n\n\n---\n\n## `SparkContext` and `SparkSession`\n\nBefore `Spark 2`\n\n```{.python}\n>>> from pyspark import SparkConf, SparkContext\n>>> from pyspark.sql import SQLContext\n\n>>> conf = SparkConf().setAppName(appName).setMaster(master)\n>>> sc = SparkContext(conf = conf)\n>>> sql_context = new SQLContext(sc)\n```\n\n. . .\n\n\nSince `Spark 2`\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pyspark.sql import SparkSession\n\napp_name = \"Spark Dataframes\"\n\nspark = (\n  SparkSession \n    .builder \n    .appName(app_name) \n#        .master(master) \n#        .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n)\n```\n:::\n\n\n\n---\n\n\n\n\n#  DataFrame  {background-color=\"#1c191c\"}\n\n\n\n---\n\n## `DataFrame`\n\n- The main entity of `Spark SQL` is the `DataFrame`\n\n- A DataFrame is actually an `RDD` of `Row`s with a *schema*\n\n- A schema gives the **names of the columns** and their **types**\n\n- `Row` is a class representing a row of the `DataFrame`.\n\n- It can be used almost as a `python` `list`, with its size equal to the number of columns in the schema.\n\n\n::: {.notes}\n\nRow-oriented or column-oriented?\n\n:::\n\n\n---\n\n## `DataFrame`\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pyspark.sql import Row\n\nrow1 = Row(name=\"John\", age=21)\nrow2 = Row(name=\"James\", age=32)\nrow3 = Row(name=\"Jane\", age=18)\nrow1['name']\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf = spark.createDataFrame([row1, row2, row3])\ndf\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.show()\n```\n:::\n\n\n\n::: {.notes}\n\nRelate `Row` to named tuple or dictionary \n\nWhat does `.show()` ?\n\n:::\n\n\n\n## `DataFrame`\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.printSchema()\n```\n:::\n\n\n\nYou can access the underlying `RDD` object using `.rdd`\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(df.rdd.toDebugString().decode(\"utf-8\"))\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.rdd.getNumPartitions()\n```\n:::\n\n\n\n::: {.notes}\n\n:::\n\n\n## Creating DataFrames\n\n- We can use the method `createDataFrame` from the SparkSession instance\n\n- Can be used to create a `Spark` `DataFrame` from:\n\n  - a `pandas.DataFrame` object\n  - a local python list\n  - an RDD\n\n- Full documentation can be found in the [[API docs]](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.createDataFrame)\n\n\n\n## Creating DataFrames\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrows = [\n        Row(name=\"John\", age=21, gender=\"male\"),\n        Row(name=\"Jane\", age=25, gender=\"female\"),\n        Row(name=\"Albert\", age=46, gender=\"male\")\n    ]\ndf = spark.createDataFrame(rows)\ndf.show()\n```\n:::\n\n\n\n\n\n## Creating DataFrames\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrows = [\n        [\"John\", 21, \"male\"],\n        [\"James\", 25, \"female\"],\n        [\"Albert\", 46, \"male\"]\n    ]\ndf = spark.createDataFrame(rows, column_names)\ndf.show()\n```\n:::\n\n\n\n\n## Creating DataFrames\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncolumn_names = [\"name\", \"age\", \"gender\"]\n\nsc = spark._sc\n\nrdd = sc.parallelize([\n        (\"John\", 21, \"male\"),\n        (\"James\", 25, \"female\"),\n        (\"Albert\", 46, \"male\")\n    ])\n\ndf = spark.createDataFrame(rdd, column_names)\ndf.show()\n```\n:::\n\n\n\n\n\n#  Schemas and types {background-color=\"#1c191c\"}\n\n\n## Schema and Types\n\n- A `DataFrame` always contains a *schema*\n\n- The schema defines the *column names* and *types*\n\n- In all previous examples, the schema was *inferred*\n\n- The schema of a `DataFrame` is represented by the class `types.StructType` [[API doc]](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructType)\n\n- When creating a `DataFrame`, the schema can be either **inferred** or **defined by the user**\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pyspark.sql.types import *\n\ndf.schema\n# StructType(List(StructField(name,StringType,true),\n#                 StructField(age,IntegerType,true),\n#                 StructField(gender,StringType,true)))\n```\n:::\n\n\n\n::: {.notes}\n\ncheck absence of quotation\n\nSpark has its own collection (tree) of types. The Python counterparts are defined in `pyspark.sql.types` \n\n:::\n\n\n\n\n\n\n## Creating a custom Schema\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pyspark.sql.types import *\n\nschema = StructType([\n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", IntegerType(), True),\n    StructField(\"gender\", StringType(), True)\n])\n\nrows = [(\"John\", 21, \"male\")]\ndf = spark.createDataFrame(rows, schema)\ndf.printSchema()\ndf.show()\n```\n:::\n\n\n\n\n## Types supported by `Spark SQL`\n\n- `StringType`\n- `IntegerType`\n- `LongType`\n- `FloatType`\n- `DoubleType`\n- `BooleanType`\n- `DateType`\n- `TimestampType`\n- `...`\n  \n  \nThe full list of types can be found in [[API doc]](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types)\n\n\n::: {.notes}\n\n:::\n\n\n\n\n#  Reading data {background-color=\"#1c191c\"}\n\n\n## Reading data from sources\n\n- Data is usually read from *external sources* \n(move the **algorithms**, not the **data**)\n\n- `Spark SQL` provides *connectors* to read from many different sources:\n\n  - Text files (`CSV`, `JSON`)\n\n  - Distributed tabular files (`Parquet`, `ORC`)\n\n  - In-memory data sources (`Apache Arrow`)\n\n  - General relational Databases (via `JDBC`)\n\n  - Third-party connectors to connect to many other databases\n\n  - And you can create your own connector for `Spark` (in `Scala`)\n\n\n## Reading data from sources\n\n- In all cases, the syntax is similar: \n\n```{.python} \nspark.read.{source}(path)\n```\n\n- Spark supports different *file systems* to look at the data:\n\n  - Local files: `file://path/to/file` or just `path/to/file`\n\n  - `HDFS` (Hadoop Distributed FileSystem): `hdfs://path/to/file`\n\n  - `Amazon S3`: `s3://path/to/file`\n\n\n\n## Reading from a `CSV` file\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\npath_to_csv = \"../../../../Downloads/tips.csv\"\ndf = spark.read.csv(path_to_csv)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf = (\n  spark.read\n    .format('csv')\n    .option('header', 'true')\n    .option('sep', \",\")\n    .load(path_to_csv)\n)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmy_csv_options = {\n  'header': True,\n  'sep': ';',\n}\n\ndf = (\n  spark\n    .read\n    .csv(path_to_csv, **my_csv_options)\n)\n```\n:::\n\n\n\n::: {.notes}\n\n:::\n\n\n\n---\n\n## Reading from a `CSV` file  \n\n**Main options**\n\nSome important options of the `CSV` reader are listed here:\n\n\n\n| Option | Description |\n|:----|:------ |\n| `sep` | The separator character |\n| `header` | If \"true\", the first line contains the column names |\n| `inferSchema` | If \"true\", the column types will be guessed from the contents |\n| `dateFormat` | A string representing the format of the date columns |\n\n\nThe full list of options can be found in the [API Docs](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv)\n\n---\n\n## Reading from other file types\n\n```{.python}\n## JSON file\ndf = spark.read.json(\"/path/to/file.json\")\ndf = spark.read.format(\"json\").load(\"/path/to/file.json\")\n```\n\n```{.python}\n## Parquet file (distributed tabular data)\ndf = spark.read.parquet(\"hdfs://path/to/file.parquet\")\ndf = spark.read.format(\"parquet\").load(\"hdfs://path/to/file.parquet\")\n```\n\n```{.python}\n## ORC file (distributed tabular data)\ndf = spark.read.orc(\"hdfs://path/to/file.orc\")\ndf = spark.read.format(\"orc\").load(\"hdfs://path/to/file.orc\")\n```\n\n---\n\n## Reading from external databases\n\n- We can use `JDBC` drivers (Java) to read from relational databases\n\n- Examples of databases: `Oracle`, `PostgreSQL`, `MySQL`, etc.\n\n- The `java` driver file must be uploaded to the cluster before trying to access\n\n- This operation can be **very heavy**. When available, specific connectors should be used\n\n- Specific connectors are often provided by **third-party libraries**\n\n---\n\n## Reading from external databases\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nspark = (\n  SparkSession \n    .builder \n    .appName(\"Python Spark SQL basic example\") \n    .config(\"spark.jars\", \n            spark_home + \"/jars/\" + \"postgresql-42.7.2.jar\") \n    .getOrCreate()\n)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf = (\n  spark\n    .read.format(\"jdbc\") \n    .option(\"url\", \"jdbc:postgresql:dbserver\") \n    .option(\"dbtable\", \"schema.tablename\") \n    .option(\"user\", usrnm) \n    .option(\"password\", pwd) \n    .load()\n)\n```\n:::\n\n\nor\n```{.python}\ndf = spark.read.jdbc(\n      url=\"jdbc:postgresql:dbserver\",\n      table=\"schema.tablename\"\n      properties={\n          \"user\": \"username\",\n          \"password\": \"p4ssw0rd\"\n      }\n)\n```\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf_airlines = (\n  spark\n    .read\n    .format(\"jdbc\")\n    .options(**(dico_jdbc_pg | {'dbtable': 'nycflights.airlines'}))\n    .load()\n)\n```\n:::\n\n\n---\n\n# Queries in Spark SQL  {background-color=\"#1c191c\"}\n\n---\n\n\n## Spark SQL as a Substitute for `HiveQL`\n\n- {{< fa brands hive >}} `Hive` (`Hadoop` InteractiVE)\n  + Devlopped by {{< fa brands facebook >}} dring 2000's\n  + Released 2010 as `Apache` project\n\n`HiveQL`:\nSQL-like interface to query data stored in various databases and file systems that integrate with Hadoop.  \n\n\n[`Hive` on wikipedia](https://en.wikipedia.org/wiki/Apache_Hive)\n\n::: {.notes}\n\n> Apache Hive is a data warehouse software project, built on top of Apache Hadoop for providing data query and analysis.[3][4] Hive gives an SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop. Traditional SQL queries must be implemented in the MapReduce Java API to execute SQL applications and queries over distributed data. Hive provides the necessary SQL abstraction to integrate SQL-like queries (HiveQL) into the underlying Java without the need to implement queries in the low-level Java API. Since most data warehousing applications work with SQL-based querying languages, Hive aids the portability of SQL-based applications to Hadoop.[5] While initially developed by Facebook, Apache Hive is used and developed by other companies such as Netflix and the Financial Industry Regulatory Authority (FINRA).[6][7] Amazon maintains a software fork of Apache Hive included in Amazon Elastic MapReduce on Amazon Web Services.[8]\n\n\n:::\n\n---\n\n## Performing queries\n\n- `Spark SQL` is designed to be compatible with ANSI SQL queries\n\n- `Spark SQL` allows  `SQL`-like queries to be evaluated on Spark `DataFrame`s  (and on many other tables)\n\n- Spark `DataFrames` have to be *tagged as temporary views*\n\n- `Spark SQL` Queries can be submitted using `spark.sql()`\n\n{{< fa hand-point-right >}} Method `sql` for class `SparkSession` provides access to `SQLContext`\n\n---\n\n## Performing queries\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrows = [\n        [\"John\", 21, \"male\"],\n        [\"Jane\", 25, \"female\"]\n    ]\ndf = spark.createDataFrame(rows, column_names)\n\ndf.show()\n```\n:::\n\n\n\n. . .\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## Create a temporary view from the DataFrame\ndf.createOrReplaceTempView(\"new_view\")\n\n## Define the query\nquery = \"\"\"\n  SELECT name, age \n  FROM new_view \n  WHERE gender='male'\n\"\"\"\n\nmen_df = spark.sql(query)\nmen_df.show()\n```\n:::\n\n\n\n::: {.notes}\n\n:::\n\n---\n\n## Using the API\n\n`SQL` queries form an expresive feature, it's *not the best way* to code a complex logic\n\n- Errors are **harder to find** in strings\n- Queries makes the code **less modular**\n\n. . .\n\nThe Spark dataframe API offers a developper-friendly API for implementing \n\n- Relational algebra  $\\sigma, \\pi, \\bowtie, \\cup, \\cap, \\setminus$\n- Partitionning `GROUP BY`\n- Aggregation and Window functions \n\n. . .\n\nCompare the  Spark `Dataframe` API  with: \n\n{{< fa brands r-project >}} `dplyr`, `dtplyr`, `dbplyr` in `R` `Tidyverse`\n\n{{< fa brands python >}} `Pandas`  \n\nChaining and/or piping enable modular query construction\n\n---\n\n## Basic Single Tables Operations (methods/verbs)\n\n\n| Operation | Description |\n|:---|:----|\n| `select` | Chooses columns from the table   $\\pi$ |\n| `selectExpr` | Chooses columns and expressions from table $\\pi$ |\n| `where` | Filters rows based on a boolean rule  $\\sigma$ |\n| `limit` | Limits the number of rows `LIMIT ...`|\n| `orderBy` | Sorts the DataFrame based on one or more columns `ORDER BY ...` |\n| `alias` | Changes the name of a column `AS ...`|\n| `cast` | Changes the type of a column |\n| `withColumn` | Adds a new column |\n\n\n---\n\n## `SELECT`\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## SQL query:\nquery = \"\"\"\n  SELECT name, age \n  FROM table\n\"\"\"\n\n## Using Spark SQL API:\n( \n  df.select(\"name\", \"age\")\n    .show()\n)\n```\n:::\n\n\n\n\n\n\n## `SELECT` (continued)\n\nThe argument of `select()`  is `*cols` where `cols` \ncan be built from column names (strings), column expressions like `df.age + 10`,  lists \n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.select( df.name.alias(\"nom\"), df.age+10 ).show()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.select([c for c in df.columns if \"a\" in c]).show()\n```\n:::\n\n\n\n\n\n## `selectExpr`\n\n```python\n###  A variant of select() that accepts SQL expressions.\n>>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n```\n\n. . .\n\n\n\n## `WHERE`\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## In a SQL query:\nquery = \"\"\"\n  SELECT * \n  FROM table \n  WHERE age > 21\n\"\"\"\n\n## Using Spark SQL API:\ndf.where(\"age > 21\").show()\n\n## Alternatively:\n# df.where(df['age'] > 21).show()\n# df.where(df.age > 21).show()\n# df.select(\"*\").where(\"age > 21\").show()\n```\n:::\n\n\n\n---\n\n## `LIMIT`\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## In a SQL query:\nquery = \"\"\"\n  SELECT * \n  FROM table \n  LIMIT 1\n\"\"\"\n\n## Using Spark SQL API:\n( \n  df.limit(1)\n    .show()\n)\n\n## Or even\ndf.select(\"*\").limit(1).show()\n```\n:::\n\n\n\n---\n\n## `ORDER BY`\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## In a SQL query:\nquery = \"\"\"\n  SELECT * \n  FROM table \n  ORDER BY name ASC\n\"\"\"\n\n## Using Spark SQL API:\ndf.orderBy(df.name.asc()).show()\n```\n:::\n\n\n\n---\n\n## `ALIAS` (name change)\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## In a SQL query:\nquery = \"\"\"\n  SELECT name, age, gender AS sex \n  FROM table\n\"\"\"\n\n## Using Spark SQL API:\ndf.select(\n    df.name, \n    df.age, \n    df.gender.alias('sex')\n  ).show()\n```\n:::\n\n\n\n---\n\n## `CAST` (type change)\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## In a SQL query:\nquery = \"\"\"\n  SELECT name, cast(age AS float) AS age_f \n  FROM table\n\"\"\"\n\n## Using Spark SQL API:\ndf.select(\n  df.name, \n  df.age.cast(\"float\").alias(\"age_f\")\n).show()\n\n## Or\nnew_age_col = df.age.cast(\"float\").alias(\"age_f\")\n\ndf.select(df.name, new_age_col).show()\n```\n:::\n\n\n\n---\n\n## Adding new columns\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## In a SQL query:\nquery = \"SELECT *, 12*age AS age_months FROM table\"\n\n## Using Spark SQL API:\ndf.withColumn(\"age_months\", df.age * 12).show()\n\n## Or\ndf.select(\"*\", \n          (df.age * 12).alias(\"age_months\")\n  ).show()\n```\n:::\n\n\n\n---\n\n## Basic operations\n\n- The *full list of operations* that can be applied to a `DataFrame` can be found in the [[DataFrame doc]](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n\n- The *list of operations on columns* can be found in the [[Column docs]](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column)\n\n---\n\n#  Column functions  {background-color=\"#1c191c\"}\n\n---\n\n## Column functions\n\n- Often, we need to make many *transformations using one or more functions*\n\n- `Spark SQL` has a package called `functions` with many functions available for that\n\n- Some of those functions are only for **aggregations** <br> Examples: `avg`, `sum`, etc. We will cover them later\n\n- Some others are for **column transformation** or **operations** <br> Examples:  \n  - `substr`, `concat`, ... (string and regex manipulation)\n  - `datediff`, ... (timestamp and duration)\n  - `floor`, ... (numerics)\n \n- The full list is, once again, in the [[API docs]](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)\n\n---\n\n## Column functions\n\nTo use these functions, we first need to import them:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pyspark.sql import functions as fn\n```\n:::\n\n\n\n**Note**: the \"`as fn`\" part is important to **avoid confusion** with native `Python` functions such as \"sum\"\n\n---\n\n## Numeric functions examples\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"brand\", \"cost\"]\ndf = spark.createDataFrame([\n        (\"garnier\", 3.49),\n        (\"elseve\", 2.71)\n        ], columns)\n\nround_cost = fn.round(df.cost, 1)\nfloor_cost = fn.floor(df.cost)\nceil_cost = fn.ceil(df.cost)\n\ndf.withColumn('round', round_cost)\\\n        .withColumn('floor', floor_cost)\\\n        .withColumn('ceil', ceil_cost)\\\n        .show()\n```\n:::\n\n\n\n---\n\n## String functions examples\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"first_name\", \"last_name\"]\n\ndf = spark.createDataFrame([\n        (\"John\", \"Doe\"),\n        (\"Mary\", \"Jane\")\n  ], \n  columns      \n)\n\nlast_name_initial = fn.substring(df.last_name, 0, 1)\nname = fn.concat_ws(\" \", df.first_name, last_name_initial)\ndf.withColumn(\"name\", name).show()\n```\n:::\n\n\n\n---\n\n## Date functions examples\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom datetime import date\nfrom pyspark.sql import functions as fn\n\ndf = spark.createDataFrame([\n        (date(2015, 1, 1), date(2015, 1, 15)),\n        (date(2015, 2, 21), date(2015, 3, 8)),\n        ], [\"start_date\", \"end_date\"]\n    )\ndays_between = fn.datediff(df.end_date, df.start_date)\nstart_month = fn.month(df.start_date)\n\ndf.withColumn('days_between', days_between)\\\n        .withColumn('start_month', start_month)\\\n        .show()\n```\n:::\n\n\n\n---\n\n## Conditional transformations\n\n- In the `functions` package is a *special function* called `when`\n\n- This function is used to *create a new column* which value **depends on the value of other columns**\n\n- `otherwise` is used to match \"the rest\"\n\n- Combination between conditions can be done using `\"&\"` for \"and\" and `\"|\"` for \"or\"\n\n\n---\n\n## Examples\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf = spark.createDataFrame([\n        (\"John\", 21, \"male\"),\n        (\"Jane\", 25, \"female\"),\n        (\"Albert\", 46, \"male\"),\n        (\"Brad\", 49, \"super-hero\")\n    ], [\"name\", \"age\", \"gender\"])\n\nsupervisor = fn.when(df.gender == 'male', 'Mr. Smith')\\\n        .when(df.gender == 'female', 'Miss Jones')\\\n        .otherwise('NA')\n\ndf.withColumn(\"supervisor\", supervisor).show()\n```\n:::\n\n\n\n\n---\n\n## {{< fa database >}} Functions in Relational Database Management Systems\n\nCompare functions defined in `pyspark.sql.functions` with functions specified in ANSI SQL and defined in popular RDBMs\n\n[PostgreSQL Documentation](https://www.postgresql.org/docs/current/index.html)\n\n{{< fa hand-point-right >}} [Section on Functions and Operators](https://www.postgresql.org/docs/current/functions.html)\n\n::: {.callout}\n\nIn RDBMs functions serve many purposes\n\n- querying\n- system administration\n- triggers\n- ...\n\n:::\n\n---\n\n## User-defined functions\n\n- When you need a **transformation** that is **not available** in the `functions` package, you can create a *User Defined Function* (UDF)\n\n- **Warning**: the performance of this can be *very very low*\n\n- So, it should be used **only** when you are **sure** the operation *cannot be done* with available functions\n\n- To create an UDF, use `functions.udf`, passing a lambda or a named functions\n\n- It is similar to the `map` operation of RDDs\n\n---\n\n## Example\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.types import StringType\n\ndf = spark.createDataFrame([(1, 3), (4, 2)], [\"first\", \"second\"])\n\ndef my_func(col_1, col_2):\n        if (col_1 > col_2):\n            return \"{} is bigger than {}\".format(col_1, col_2)\n        else:\n            return \"{} is bigger than {}\".format(col_2, col_1)\n\nmy_udf = fn.udf(my_func, StringType())\n\ndf.withColumn(\"udf\", my_udf(df['first'], df['second'])).show()\n```\n:::\n\n\n\n---\n\n\n\n\n#  Joins  {background-color=\"#1c191c\"}\n\n---\n\n## Performing joins  {.smaller}\n\n- `Spark SQL` supports *joins* between two `DataFrame`\n\n- As in  `ANSI SQL`, a join **rule** must be defined\n\n- The **rule** can either be a set of **join keys** (equi-join), or a **conditional rule**\n($\\theta$-join)\n\n- Join with conditional rules ($\\theta$-joins) in `Spark` can be *very heavy*\n\n- **Several types of joins** are available, default is `inner`\n\nSyntax  for $\\texttt{left_df} \\bowtie_{\\texttt{cols}} \\texttt{right_df}$\n is simple:\n```{.python}\nleft_df.join(\n  other=right_df, \n  on=cols, \n  how=join_type\n)\n```\n- `cols` contains a column name or a list of column names\n- `join_type` is the type of join\n\n\n\n---\n\n## Examples\n\n\n\n::: {.cell}\n\n```{.python .cell-code  code-line-numbers=\"1-12|14\"}\nfrom datetime import date\n\nproducts = spark.createDataFrame([\n        ('1', 'mouse', 'microsoft', 39.99),\n        ('2', 'keyboard', 'logitech', 59.99),\n    ], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n\npurchases = spark.createDataFrame([\n        (date(2017, 11, 1), 2, '1'),\n        (date(2017, 11, 2), 1, '1'),\n        (date(2017, 11, 5), 1, '2'),\n    ], ['date', 'quantity', 'prod_id'])\n\n# The default join type is the \"INNER\" join\npurchases.join(products, 'prod_id').show()\n```\n:::\n\n\n\n---\n\n## Examples\n\n\n\n::: {.cell}\n\n```{.python .cell-code  code-line-numbers=\"4-9|11\"}\n# We can also use a query string (not recommended)\nproducts.createOrReplaceTempView(\"products\")\npurchases.createOrReplaceTempView(\"purchases\")\n\nquery = \"\"\"\n  SELECT * \n  FROM  purchases AS prc INNER JOIN \n        products AS prd \n    ON (prc.prod_id = prd.prod_id)\n\"\"\"\n\nspark.sql(query).show()\n```\n:::\n\n\n\n---\n\n## Examples\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnew_purchases = spark.createDataFrame([\n        (date(2017, 11, 1), 2, '1'),\n        (date(2017, 11, 2), 1, '3'),\n    ], ['date', 'quantity', 'prod_id_x']\n)\n\njoin_rule = new_purchases.prod_id_x == products.prod_id\n\nnew_purchases.join(products, join_rule, 'left').show()\n```\n:::\n\n\n\n\n---\n\n## Performing joins: some remarks\n\n- Spark *removes the duplicated column* in the `DataFrame` it outputs after a join operation\n\n- When joining using columns *with nulls*, `Spark` just skips those\n\n```{.scala}\n>>> df1.show()               >>> df2.show()\n+----+-----+                 +----+-----+\n|  id| name|                 |  id| dept|\n+----+-----+                 +----+-----+\n| 123|name1|                 |null|sales|\n| 456|name3|                 | 223|Legal|\n|null|name2|                 | 456|   IT|\n+----+-----+                 +----+-----+\n\n>>> df1.join(df2, \"id\").show\n+---+-----+-----+\n| id| name| dept|\n+---+-----+-----+\n|123|name1|sales|\n|456|name3|   IT|\n+---+-----+-----+\n```\n\n---\n\n## Join types   {.smaller}\n\n\n| SQL Join Type | In Spark (synonyms)                | Description          |\n|:--------------|:-----------------------------------|:---------------------|\n| `INNER`         | `\"inner\"`                          | Data from left and right matching both ways (intersection) |\n| `FULL OUTER`    | `\"outer\"`, `\"full\"`, `\"fullouter\"` | All rows from left and right with extra data if present (union) |\n| `LEFT OUTER`    | `\"leftouter\"`, `\"left\"`            | Rows from left with extra data from right if present |\n| `RIGHT OUTER`   | `\"rightouter\"`, `\"right\"`          | Rows from right with extra data from left if present |\n| `LEFT SEMI`     | `\"leftsemi\"`                       | Data from left with a match with right |\n| `LEFT ANTI`     | `\"leftanti\"`                       | Data from left with no match with right |\n| `CROSS`         | `\"cross\"`                          | Cartesian product of left and right (never used) |\n\n\n---\n\n\n\n## Join types\n\n::: {.center}\n\n<img width=\"700px\" src=\"./IMG/join-types.png\"/>\n\n:::\n\n---\n\n## Inner join (\"inner\")\n\n```{.python}\n>>> inner = df_left.join(df_right, \"id\", \"inner\")\n\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\ninner\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n+---+-----+-----+\n```\n\n---\n\n## Outer join (\"outer\", \"full\" or \"fullouter\")\n\n```{.python}\n>>> outer = df_left.join(df_right, \"id\", \"outer\")\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\nouter\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  1|   A1| null|\n|  2|   A2| null|\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n|  5| null|   A5|\n|  6| null|   A6|\n+---+-----+-----+\n```\n\n---\n\n## Left join (\"leftouter\" or \"left\" )\n\n```{.python}\n>>> left = df_left.join(df_right, \"id\", \"left\")\n\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\nleft\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  1|   A1| null|\n|  2|   A2| null|\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n+---+-----+-----+\n```\n\n---\n\n## Right (\"rightouter\" or \"right\")\n\n```{.python}\n>>> right = df_left.join(df_right, \"id\", \"right\")\n\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\nright\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n|  5| null|   A5|\n|  6| null|   A6|\n+---+-----+-----+\n```\n\n---\n\n## Left semi join (\"leftsemi\")\n\n```{.python}\n>>> left_semi = df_left.join(df_right, \"id\", \"leftsemi\")\n\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\nleft_semi\n+---+-----+\n| id|value|\n+---+-----+\n|  3|   A3|\n|  4|   A4|\n+---+-----+\n```\n\n---\n\n## Left anti joint (\"leftanti\")\n\n```{.python}\n>>> left_anti = df_left.join(df_right, \"id\", \"leftanti\")\n\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\nleft_anti\n+---+-----+\n| id|value|\n+---+-----+\n|  1|   A1|\n|  2|   A2|\n+---+-----+\n```\n\n---\n\n\n## Performing joins\n\n- Node-to-node communication strategy \n\n- Per node computation strategy\n\n\n\n::: {.notes}\n\nSection *“How Spark Performs Joins”\n\n:::\n\n\n\n---\n\n\n### From the Definitive Guide: \n\n> Spark approaches cluster communication in two different ways during joins. \n\n> It either incurs a *shuffle* join, which results in an all-to-all communication or a *broadcast* join. \n\n> The core foundation of our simplified view of joins is that in Spark you will have either a big table or a small table. \n\n> When you join a big table to another big table, you end up with a *shuffle* join\n\n---\n\n\n> When you join a big table to another big table, you end up with a *shuffle* join\n\n\n![](./IMG/spark_shuffle_join.jpg)\n\n\n---\n\n\n> When you join a big table to a small table, you end up with a *broadcast* join\n\n![](./IMG/spark_broadcast_join.jpg)\n\n\n---\n\n\n# Aggregations  {background-color=\"#1c191c\"}\n\n---\n\n## Performing aggregations\n\n- Maybe *the most used operations* in `SQL` and `Spark SQL`\n\n- Similar to `SQL`, we use `\"group by\"` to perform *aggregations*\n\n- We usually can call the aggregation function just after `groupBy` <br> \nNamely, we use `groupBy().agg()`\n\n- *Many aggregation functions* in `pyspark.sql.functions`\n\n- Some examples:\n\n  - Numerical: `fn.avg`, `fn.sum`, `fn.min`, `fn.max`, etc.\n\n  - General: `fn.first`, `fn.last`, `fn.count`, `fn.countDistinct`, etc.\n\n---\n\n## Examples\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pyspark.sql import functions as fn\n\nproducts = spark.createDataFrame([\n        ('1', 'mouse', 'microsoft', 39.99),\n        ('2', 'mouse', 'microsoft', 59.99),\n        ('3', 'keyboard', 'microsoft', 59.99),\n        ('4', 'keyboard', 'logitech', 59.99),\n        ('5', 'mouse', 'logitech', 29.99),\n    ], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n\nproducts.groupBy('prod_cat').avg('prod_value').show()\n\n# Or\nproducts.groupBy('prod_cat').agg(fn.avg('prod_value')).show()\n```\n:::\n\n\n\n---\n\n## Examples\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pyspark.sql import functions as fn\n\nproducts.groupBy('prod_brand', 'prod_cat')\\\n        .agg(fn.avg('prod_value')).show()\n```\n:::\n\n\n\n\n---\n\n## Examples\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pyspark.sql import functions as fn\n\nproducts.groupBy('prod_brand').agg(\n    fn.round(fn.avg('prod_value'), 1).alias('average'),\n    fn.ceil(fn.sum('prod_value')).alias('sum'),\n    fn.min('prod_value').alias('min')\n).show()\n```\n:::\n\n\n\n---\n\n## Examples\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Using an SQL query\nproducts.createOrReplaceTempView(\"products\")\n\nquery = \"\"\"\n  SELECT\n    prod_brand,\n    round(avg(prod_value), 1) AS average,\n    min(prod_value) AS min\n  FROM products\n  GROUP BY prod_brand\n\"\"\"\n\nspark.sql(query).show()\n```\n:::\n\n\n\n\n# Window functions  {background-color=\"#1c191c\"}\n\n\n\n## Window (analytic) functions\n\n- A very, very *powerful feature*\n\n- They allow to solve *complex problems*\n\n- ANSI SQL2003 allows for a `window_clause` in aggregate function calls, \nthe addition of which makes those functions into window functions\n\n- A good article about this feature is [[here]](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html)\n\nSee also :\n\n[https://www.postgresql.org/docs/current/tutorial-window.html](https://www.postgresql.org/docs/current/tutorial-window.html)\n\n::: {.notes}\n\n> A window function performs a calculation across a set of table rows that are somehow related to the current row. \n> This is comparable to the type of calculation that can be done with an aggregate function. \n> However, window functions do not cause rows to become grouped into a single output row like non-window aggregate calls would. \n> Instead, the rows retain their separate identities. \n> Behind the scenes, the window function is able to access more than just the current row of the query result.\n\n:::\n\n\n## Window functions\n\n- It's *similar to aggregations*, but the **number of rows doesn't change**\n\n- Instead, *new columns are created*, and the **aggregated values are duplicated** for values of the same \"group\"\n\n- There are \n  + \"traditional\" aggregations, such as `min`, `max`, `avg`, `sum` and \n  + \"special\" types, such as `lag`, `lead`, `rank`\n\n\n## Numerical window functions\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\n# First, we create the Window definition\nwindow = Window.partitionBy('prod_brand')\n\n# Then, we can use \"over\" to aggregate on this window\navg = fn.avg('prod_value').over(window)\n\n# Finally, we can it as a classical column\nproducts.withColumn('avg_brand_value', fn.round(avg, 2)).show()\n```\n:::\n\n\n\n\n## Numerical window functions\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\n# The window can be defined on multiple columns\nwindow = Window.partitionBy('prod_brand', 'prod_cat')\n\navg = fn.avg('prod_value').over(window)\n\nproducts.withColumn('avg_value', fn.round(avg, 2)).show()\n```\n:::\n\n\n\n\n## Numerical window functions\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\n# Multiple windows can be defined\nwindow1 = Window.partitionBy('prod_brand')\nwindow2 = Window.partitionBy('prod_cat')\n\navg_brand = fn.avg('prod_value').over(window1)\navg_cat = fn.avg('prod_value').over(window2)\n\nproducts \\\n  .withColumn('avg_by_brand', fn.round(avg_brand, 2)) \\\n  .withColumn('avg_by_cat', fn.round(avg_cat, 2)) \\\n  .show()\n```\n:::\n\n\n\n\n\n## Lag and Lead\n\n- `lag` and `lead` are special functions used over an *ordered window*\n\n- They are used to **take the \"previous\" and \"next\" value** within the window\n\n- Very useful in datasets with a date column for instance\n\n\n## Lag and Lead\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\npurchases = spark.createDataFrame([\n        (date(2017, 11, 1), 'mouse'),\n        (date(2017, 11, 2), 'mouse'),\n        (date(2017, 11, 4), 'keyboard'),\n        (date(2017, 11, 6), 'keyboard'),\n        (date(2017, 11, 9), 'keyboard'),\n        (date(2017, 11, 12), 'mouse'),\n        (date(2017, 11, 18), 'keyboard')\n    ], ['date', 'prod_cat'])\npurchases.show()\n```\n:::\n\n\n\n\n## Lag and Lead\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nwindow = Window.partitionBy('prod_cat').orderBy('date')\n\nprev_purch = fn.lag('date', 1).over(window)\nnext_purch = fn.lead('date', 1).over(window)\n\npurchases\\\n  .withColumn('prev', prev_purch)\\\n  .withColumn('next', next_purch)\\\n  .orderBy('prod_cat', 'date')\\\n  .show()\n```\n:::\n\n\n\n\n## Rank, DenseRank and RowNumber\n\n- Another set of **useful \"special\" functions**\n\n- Also used on *ordered windows*\n\n- They create a **rank**, or an **order** of the items within the window\n\n\n## Rank and RowNumber\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncontestants = spark.createDataFrame([\n    ('veterans', 'John', 3000),\n    ('veterans', 'Bob', 3200),\n    ('veterans', 'Mary', 4000),\n    ('young', 'Jane', 4000),\n    ('young', 'April', 3100),\n    ('young', 'Alice', 3700),\n    ('young', 'Micheal', 4000)], \n  ['category', 'name', 'points']\n)\n\ncontestants.show()\n```\n:::\n\n\n\n\n## Rank and RowNumber\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nwindow = Window.partitionBy('category')\\\n        .orderBy(contestants.points.desc())\n\nrank = fn.rank().over(window)\ndense_rank = fn.dense_rank().over(window)\nrow_number = fn.row_number().over(window)\n\ncontestants\\\n        .withColumn('rank', rank)\\\n        .withColumn('dense_rank', dense_rank)\\\n        .withColumn('row_number', row_number)\\\n        .orderBy('category', fn.col('points').desc())\\\n        .show()\n```\n:::\n\n\n\n#  Writing dataframes  {background-color=\"#1c191c\"}\n\n\n## Writing dataframes   {.smaller}\n\n- Very *similar to reading*. Output formats are the same: `csv`, `json`, `parquet`, `orc`, `jdbc`, etc. Note that `write` *is an action*\n\n- Instead of `df.read.{source}` use `df.write.{target}`\n\n- Main option is `mode` with possible values:\n\n  - `\"append\"`: append contents of this `DataFrame` to existing data.\n  - `\"overwrite\"`: overwrite existing data\n  - `\"error\"`: throw an exception if data already exists\n  - `\"ignore\"`: silently ignore this operation if data already exists.\n\nExample\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nproducts.write.csv('./products.csv')\nproducts.write.mode('overwrite').parquet('./produits.parquet')\nproducts.write.format('parquet').save('./produits_2.parquet')\n```\n:::\n\n\n\n\n\n# Under the hood...  {background-color=\"#1c191c\"}\n\n## Query planning and optimization\n\nA *lot happens under the hood* when executing an **action** on a `DataFrame`.\nThe query goes through the following **exectution stages**:\n\n1. Logical Analysis\n1. Caching Replacement\n1. Logical Query Optimization (using rule-based and cost-based optimizations)\n1. Physical Planning\n1. Physical Optimization (e.g. Whole-Stage Java Code Generation or Adaptive Query Execution)\n1. Constructing the RDD of Internal Binary Rows (that represents the structured query in terms of Spark Core’s RDD API)\n\n::: {.aside}\n\n[https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql.html](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql.html)]\n\n:::\n\n\n## Query planning and optimization\n\n::: {.center}\n  \n![](./IMG/QueryExecution-execution-pipeline.png)\n\n:::\n\n::: {.aside}\n\n[https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql.html](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql.html)]\n\n:::\n\n## References \n\n[PySpark Quickstart](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html)\n\n[Pandas on Spark User's Guide](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html)\n\n\n\n\n\n# Thank you !  {.unlisted background-color=\"#1c191c\"}\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}