{
  "hash": "e892738c9492462e2dff6f112153c971",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Spark Presentation\"\nengine: knitr\ndate: \"2025-01-17\"\n---\n\n\n\n\n# Spark in perspective {background-color=\"#1c191c\"}\n\n## From the archive\n\nSpark project\n\n:   launched in 2010 by M. Zaharia (UC Berkeley) et al.\n\n. . .\n\nSpark 1.0.0 released (May 30, 2014):\n\n:   ... This release expands Spark’s standard libraries, introducing a new SQL package (Spark SQL) that lets users integrate SQL queries into existing Spark workflows. MLlib, Spark’s machine learning library, is expanded with sparse vector support and several new algorithms. The GraphX and Streaming libraries also introduce new features and optimizations. Spark’s core engine adds support for secured YARN clusters, a unified tool for submitting Spark applications, and several performance and stability improvements.\n\n. . .\n\nSpark 2.0.0 released (July 26, 2016)\n\n:   The major updates are API usability, SQL 2003 support, performance improvements, structured streaming, R UDF support, as well as operational improvements.\n\n## From the archive (continued)\n\nSpark 3.0.0 released (June 18, 2020)\n\n:   ... This year is Spark’s 10-year anniversary as an open source project. Since its initial release in 2010, Spark has grown to be one of the most active open source projects. Nowadays, Spark is the de facto unified engine for big data processing, data science, machine learning and data analytics workloads.\n\n. . .\n\nSpark SQL is the top active component in this release. 46% of the resolved tickets are for Spark SQL. These enhancements benefit all the higher-level libraries, including structured streaming and MLlib, and higher level APIs, including SQL and DataFrames. Various related optimizations are added in this release. In TPC-DS 30TB benchmark, Spark 3.0 is roughly two times faster than Spark 2.4.\n\n::: notes\nPython is now the most widely used language on Spark. PySpark has more than 5 million monthly downloads on PyPI, the Python Package Index. This release improves its functionalities and usability, including the pandas UDF API redesign with Python type hints, new pandas UDF types, and more Pythonic error handling.\n\nHere are the feature highlights in Spark 3.0: adaptive query execution; dynamic partition pruning; ANSI SQL compliance; significant improvements in pandas APIs; new UI for structured streaming; up to 40x speedups for calling R user-defined functions; accelerator-aware scheduler; and SQL reference documentation.\n\nFrom <https://spark.apache.org/releases/spark-release-3-0-0.html>\n:::\n\n::: notes\n> \"Présentation de l'outil SPARK. Cette séance offrira tout d'abord un aperçu général et théorique du développement de cette technologie, un rappel historique et des solutions existantes, la scalabilité, entre autres. Un second volet de la présentation se concentrera sur l'application de cette technologie dans la recherche, avec une démonstration pratique illustrant son utilisation.\"\n:::\n\n## Why Spark?\n\n::: columns\n::: {.column width=\"60%\"}\n::: incremental\n-   Scalability\n\n-   Beyond OLTP: OLAP (and BI)\n\n-   From Data Mining to Big Data\n\n-   From Datawarehouses to Datalakes\n:::\n\n::: {.fragment .fade-in}\n-   MapReduce {{< fa brands google >}}\n:::\n\n::: {.fragment .fade-in}\n-   Apache Hadoop\n:::\n\n::: {.fragment .fade-in}\n-   Hive {{< fa brands hive >}} {{< fa brands facebook >}}\n    -   Before 2010, de facto big data SQL API\n    -   Helped propel `Hadoop` to industry\n:::\n:::\n\n::: {.column width=\"40%\"}\n::: {.fragment .fade-in}\n![1995-2005 Beyond ACID SQL](IMG/harrison_next_gen_database.jpg)\n:::\n:::\n:::\n\n# Spark organization {background-color=\"#1c191c\"}\n\n------------------------------------------------------------------------\n\n::: columns\n::: column\n![Cluster overview from Spark official documentation](IMG/cluster-overview.png)\n:::\n\n::: column\n::: {.fragment .fade-in-out}\n::: callout-note\n\n-   There is one master per cluster.\n-   The cluster manager/master is launched by `start-master.sh`.\n-   There are as many workers per machine on the cluster.\n-   A worker process is launched by `start-worker.sh` (standalone mode)\n-   Spark applications (interactive or not) exchange informations using a driver process.\n-   Master is per cluster, and driver is per application.\n:::\n:::\n:::\n:::\n\n## \n\n![](./IMG/spark_archi.png)\n\n## Sparksession\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pyspark.sql import SparkSession\n\nspark = (\n    SparkSession \n        .builder \n        .appName(\"Presentation\") \n        .getOrCreate()\n)\n```\n:::\n\n\n\n\n# Spark SQL and Dataframes {background-color=\"#1c191c\"}\n\n---\n\n::: {.middle}\n\n::: {.center}\n\n![](./IMG/pyspark-apis.png)\n\n:::\n\n:::\n\n## Spark core\n\n- Implements the `RDD` (Resilient Distributed Datasets)\n\n. . .\n\n- Spark project was launched to implement the RDD concept presented by Zaharia et al at the end of the 2000'\n\n. . .\n\n- In words, RDDs behave like distributed, fault-tolerant, Python collections (list or dict)\n\n. . .\n\n- RDDs areo *immutable*, they can be *transformed* using `map` like operations, transformed RDDs can be *reduced*, and the result can be `collected` to the driver process   \n\n\n## Spark SQL and `HIVE` (Hadoop InteractiVE)\n\nSpark SQL relies on Hive SQL's conventions and functions\n\nSince release 2.0, Spark offers a native SQL parser that supports ANSI-SQL and HiveQL\n\nWorks for analysts, data engineers, data scientists\n\n. . .\n\n::: {.callout-caution}\n\n### Spark-SQL is geared towards OLAP not OLTP\n\n:::\n\n## Rows\n\nSpark dataframes are RDDs (collections of `Row`s)\n\n\n\n::: {.cell}\n\n```{.python .cell-code  code-line-numbers=\"3-5|12|14\"}\nfrom pyspark.sql import Row\n\nrow1 = Row(name=\"John\", age=21)\nrow2 = Row(name=\"James\", age=32)\nrow3 = Row(name=\"Jane\", age=18)\n\nrow1['name']\n\nrows = [row1, row2, row3]\ncolumn_names = [\"Name\", \"Age\"]\n\ndf = spark.createDataFrame(rows, column_names)\n\ndf.show()\n```\n:::\n\n\n\n## Schema\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.printSchema()\n```\n:::\n\n\n\n## From dataframes to `RDDs`\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(df.rdd.toDebugString().decode(\"utf-8\"))\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.rdd.getNumPartitions()\n```\n:::\n\n\n\n## Spark dataframe API\n\nThe Spark dataframe API offers a developper-friendly API for implementing \n\n- Relational algebra  $\\sigma, \\pi, \\bowtie, \\cup, \\cap, \\setminus$\n- Partitionning `GROUP BY`\n- Aggregation and Window functions \n\n. . .\n\nCompare the  Spark `Dataframe` API  with: \n\n{{< fa brands r-project >}} `dplyr`, `dtplyr`, `dbplyr` in `R` `Tidyverse`\n\n{{< fa brands python >}} `Pandas`  \n\n{{< fa brands python >}} `Pandas on Spark`\n\nChaining and/or piping enable modular query construction\n\n## Basic Single Tables Operations (methods/verbs)\n\n\n| Operation | Description |\n|:---|:----|\n| `select` | Chooses columns from the table   $\\pi$ |\n| `selectExpr` | Chooses columns and expressions from table $\\pi$ |\n| `where` | Filters rows based on a boolean rule  $\\sigma$ |\n| `limit` | Limits the number of rows `LIMIT ...`|\n| `orderBy` | Sorts the DataFrame based on one or more columns `ORDER BY ...` |\n| `alias` | Changes the name of a column `AS ...`|\n| `cast` | Changes the type of a column |\n| `withColumn` | Adds a new column |\n\n## Toy example\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrows = [\n        [\"John\", 21, \"male\"],\n        [\"Jane\", 25, \"female\"]\n    ]\ndf = spark.createDataFrame(rows, column_names)\n\ndf.show()\n```\n:::\n\n\n\n## Querying SQL style\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## Create a temporary view from the DataFrame\ndf.createOrReplaceTempView(\"new_view\")\n\n## Define the query\nquery = \"\"\"\n  SELECT name, age \n  FROM new_view \n  WHERE gender='male'\n\"\"\"\n\nmen_df = spark.sql(query)\nmen_df.show()\n```\n:::\n\n\n\n## Select \n\nThe argument of `select()`  is `*cols` where `cols` \ncan be built from column names (strings), column expressions like `df.age + 10`,  lists \n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.select(df.name.alias(\"nom\"), df.age+10 ).show()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.select([c for c in df.columns if \"a\" in c]).show()\n```\n:::\n\n\n\n## Adding new columns\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## In a SQL query:\nquery = \"SELECT *, 12*age AS age_months FROM table\"\n\n## Using Spark SQL API:\ndf.withColumn(\"age_months\", df.age * 12).show()\n\n## Or\ndf.select(\"*\", \n          (df.age * 12).alias(\"age_months\")\n  ).show()\n```\n:::\n\n\n\n## Basic operations\n\n- The *full list of operations* that can be applied to a `DataFrame` can be found in the [[DataFrame doc]](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n\n- The *list of operations on columns* can be found in the [[Column docs]](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column)\n\n\n# Spark APIs for `R` {background-color=\"#1c191c\"}\n\n## `sparkR` and `sparklyr`\n\n::: {.fragment .fade-in-then-semi-out}\n-   `sparkR` is the official Spark API for `R` users\n:::\n\n::: {.fragment .fade-in}\n-   `sparklyr` (released 2016) is the de facto Spark API for `tidyverse`\n:::\n\n## A glimpse at Sparklyr\n\n::: {.fragment .fade-in}\nSpark dataframes can be handled through `dplyr` pipelines\n\n``` r\n#| code-line-numbers: |4|5|6|7\nsc <- spark_connect(master=\"local\", version=\"3.5\")\nwh <- copy_to(sc, whiteside)\n\nwh |> \n    group_by(Insul) |> \n    mutate(Fn=(1+n()-min_rank(desc(Temp)))/n()) |> \n    arrange(Insul, Temp)\n```\n:::\n\n::: {.fragment .fade-in}\n```         \n# Source:     spark<?> [?? x 4]\n# Groups:     Insul\n# Ordered by: Insul, Temp\n   Insul  Temp   Gas     Fn\n   <chr> <dbl> <dbl>  <dbl>\n 1 After  -0.7   4.8 0.0333\n 2 After   0.8   4.6 0.0667\n 3 After   1     4.7 0.1   \n 4 After   1.4   4   0.133 \n 5 After   1.5   4.2 0.167 \n 6 After   1.6   4.2 0.2   \n 7 After   2.3   4.1 0.233 \n 8 After   2.5   4   0.3   \n 9 After   2.5   3.5 0.3   \n10 After   3.1   3.2 0.333 \n# ℹ more rows\n# ℹ Use `print(n = ...)` to see more rows\n```\n:::\n\n## Under the hood\n\n``` r\n> wh |>  \n    summarise(x=quantile(Temp,.25)) |> \n    show_query()\n```\n\n::: {.fragment .fade-in}\n```         \n<SQL>\nSELECT PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY `Temp`) AS `x`\nFROM `whiteside`\n```\n:::\n\n::: {.fragment .fade-in}\n`dplyr` queries are translated into `Spark/Hive SQL`\n\n{{< fa hand-point-right >}} `quantile()` is a `base R` function, it is matched to the `Spark/Hive` `percentile()` function\n\n`sparklyr` aims at avoiding sending `R` functions/objects across the cluster\n\n:::\n\n\n## In words\n\n> ... `sparklyr` translates `dplyr` functions such as `arrange` into a SQL query plan that is used by SparkSQL. This is not the case with `SparkR`, which has functions for SparkSQL tables and Spark DataFrames.\n\n. . .\n\n> ... Databricks does not recommended combining `SparkR` and `sparklyr` APIs in the same script, notebook, or job.\n\n \n##  {background-iframe=\"https://therinspark.com\"}\n\n\n# Pandas Spark API {background-color=\"#1c191c\"}\n\n--- \n\n> Apache Spark includes Arrow-optimized execution of Python logic in the form of `pandas` function APIs, which allow users to apply `pandas` transformations directly to PySpark DataFrames. Apache Spark also supports `pandas` UDFs, which use similar Arrow-optimizations for arbitrary user functions defined in Python.\n\n##  {background-iframe=\"https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html\"}\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}