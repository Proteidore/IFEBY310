{
  "hash": "d01db098523e271619f61d2f50fc76b3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Dask\"\nengine: knitr\ndate: \"2025-01-17\"\n---\n\n\n\n# Dask:  Big picture  {background-color=\"#1c191c\"}\n\n------------------------------------------------------------------------\n\n![Dask in picture](IMG/dask_nutshell.png)\n\n\n\n\n## {{< fa map >}}  {.scrollable}\n\n-   Overview - dask's place in the universe.\n\n-   `Delayed` - the single-function way to parallelize general python code.\n\n-   1x. `Lazy` - some of the principles behind lazy execution, for the interested\n\n-   `Bag` - the first high-level collection: a generalized iterator for use with a functional programming style and to clean messy data (scaling up and out Python lists)\n\n-   `Array` - blocked `numpy`-like functionality with a collection of `numpy` arrays spread across your cluster\n\n-   `Dataframe` - parallelized operations on many `pandas` `dataframes` spread across your cluster\n\n-   `Distributed` - Dask's scheduler for clusters, with details of how to view the UI\n\n-   Advanced Distributed - further details on distributed computing, including how to debug\n\n-   `Dataframe` Storage - efficient ways to read and write dataframes to disc\n\n-   Machine Learning - applying `dask` to machine-learning problems.\n\n## Flavours of (big) data\n\n| Type        |     Typical size      | Features                                     |   Tool |\n|:--------------|:-----------------------:|:-----------------|--------------:|\n| Small data  |     Few GigaBytes     | Fits in RAM                                  | Pandas |\n| Medium data | Less than 2 Terabytes | Does not fit in RAM, fits on hard drive |   Dask |\n| Large data  |       Petabytes       | Does not fit on hard drivve                  |  Spark |\n\n---\n\n![](./IMG/dask_horizontal.svg)\n\n\nDask provides multi-core and distributed parallel execution on larger-than-memory datasets.\n\n. . .\n\nDask provides high-level `Array`, `Bag`, and `DataFrame` collections that mimic `NumPy`, `lists`, and `Pandas` but can operate in parallel on datasets that don't fit into memory\n\n. . . \nDask provides dynamic task schedulers that execute *task graphs* in parallel.\n\nThese execution engines power the high-level collections but can also power custom, user-defined workloads.\n\nThese schedulers are low-latency and work hard to run computations in a small memory footprint\n\n---\n\n\n[Dask Tutorial SciPy 2020](https://tutorial.dask.org \"Dask Tutorial SciPy 2020\")\n\n---\n\n[Dask FAQ](https://docs.dask.org/en/latest/faq.html)\n\n\n## Trends\n\n![Dask adoption metrics](IMG/dask_adoption.png)\n\n------------------------------------------------------------------------\n\n# Delayed  {background-color=\"#1c191c\"}\n\n---\n\n> The single-function way to parallelize general python code.\n\n## Imports\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport dask\n\ndask.config.set(scheduler='threads')\ndask.config.set({'dataframe.query-planning': True})\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport dask.dataframe as dd\nimport dask.bag as db\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom dask import delayed\nimport dask.threaded\n\nfrom dask.distributed import Client\nfrom dask.diagnostics import ProgressBar\nfrom dask.diagnostics import Profiler, ResourceProfiler, CacheProfiler\n```\n:::\n\n\n\n## `LocalCluster`\n\nDask can set itself up easily in your Python session if you create a `LocalCluster` object, which sets everything up for you.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# from dask.distributed import LocalCluster\n\n# cluster = LocalCluster()\n# client = cluster.get_client()\n```\n:::\n\n\n\n### Normal Dask work ...\n\nAlternatively, you can skip this part, and Dask will operate within a thread pool contained entirely with your local process.\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Delaying Pyhton tasks\n\n------------------------------------------------------------------------\n\n------------------------------------------------------------------------\n\n## A job (I)\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef inc(x):\n  return x + 1\n\ndef double(x):\n  return x * 2\n\ndef add(x, y):\n  return x + y\n```\n:::\n\n\n\n## A job (II)\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndata = [1, 2, 3, 4, 5]\n\noutput = []\nfor x in data:\n  a = inc(x)   #<<\n  b = double(x) #<<\n  c = add(a, b) #<<\n  output.append(c)\n  \ntotal = sum(output)\n  \ntotal \n```\n:::\n\n\n\n## Delaying existing functions\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\noutput = []\n\nfor x in data:\n  a = dask.delayed(inc)(x)   #<<\n  b = dask.delayed(double)(x) #<<\n  c = dask.delayed(add)(a, b) #<<\n  output.append(c)\n  \ntotal = dask.delayed(sum)(output) #<< \n  \ntotal\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ntotal.compute()\n```\n:::\n\n\n\n## Another way of using decorators\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n@dask.delayed\ndef inc(x):\n  return x + 1\n\n@dask.delayed\ndef double(x):\n  return x * 2\n\n@dask.delayed\ndef add(x, y):\n  return x + y\n\ndata = [1, 2, 3, 4, 5]\n\noutput = []\nfor x in data:\n  a = inc(x)\n  b = double(x)\n  c = add(a, b)\n  output.append(c)\n  \ntotal = dask.delayed(sum)(output)\ntotal\ntotal.compute()\n```\n:::\n\n\n\n## Visualizing the task graph\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntotal.visualize()\n```\n:::\n\n\n\n# Tweaking the task graph  {background-color=\"#1c191c\"}\n\n## Another job\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nDATA = []\n\n@dask.delayed\ndef inc(x):\n  return x + 1\n\n@dask.delayed\ndef add_data(x):\n  DATA.append(x)\n\n@dask.delayed\ndef sum_data(x):\n  return sum(DATA) + x\n\na = inc(1)\nb = add_data(a)\nc = inc(3)\nd = add_data(c)\ne = inc(5)\nf = sum_data(e)\nf.compute()\n```\n:::\n\n\n\n## A flawed task graph\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nf.visualize()\n```\n:::\n\n\n\n## Fixing\n\n::: columns\n::: {.column width=\"40%\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom dask.graph_manipulation import bind\n\ng = bind(sum_data, [b, d])(e)\n\ng.compute()\n```\n:::\n\n\n\n::: smaller\n\nThe result of the evaluation of `sum_data()` depends not only on its argument, hence on the `Delayed` `e` but also on the side effects of `add_data()`, that is on the `Delayed` `b` and `d`\n\nNote that not only the DAG was wrong but the result obtained above was not the intended result.\n\n:::\n\n:::\n\n::: {.column width=\"60%\"}\n::: smaller\n\n\n::: {.cell}\n\n```{.python .cell-code}\ng.visualize()\n```\n:::\n\n\n:::\n:::\n\n:::\n\n------------------------------------------------------------------------\n\n> By default, Dask `Delayed` uses the *threaded* scheduler in order to avoid data transfer costs.\n\n> Consider using *multi-processing* scheduler or *dask.distributed* scheduler on a local machine or on a cluster if your code does not release the `GIL` well (computations that are dominated by pure Python code, or computations wrapping external code and holding onto it).\n\n------------------------------------------------------------------------\n\n# Futures  {background-color=\"#1c191c\"}\n\n------------------------------------------------------------------------\n\n## Objectives\n\nSometimes you want to create and destroy work during execution, launch tasks from other tasks, etc. For this, see the `Futures` interface.\n\n------------------------------------------------------------------------\n\n# High level collections  {background-color=\"#1c191c\"}\n\n------------------------------------------------------------------------\n\n## Importing the usual suspects\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\nimport dask.dataframe as dd\nimport dask.array as da\nimport dask.bag as db\n```\n:::\n\n\n\n---\n\n## Bird-eye view\n\n\n# Arrays \n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport xarray as xr\n```\n:::\n\n\n\n# Bags  {background-color=\"#1c191c\"}\n\n------------------------------------------------------------------------\n\n------------------------------------------------------------------------\n\n# Dataframes   {background-color=\"#1c191c\"}\n\n\n##\n\n> Dask Dataframes parallelize the popular pandas library, providing:\n\n> - Larger-than-memory execution for single machines, allowing you to process data that is larger than your available RAM\n\n> - Parallel execution for faster processing\n\n> - Distributed computation for terabyte-sized datasets\n\n. . .\n\n> Dask Dataframes are similar in this regard to Apache Spark, but use the familiar pandas API and memory model. One Dask dataframe is simply a collection of pandas dataframes on different computers.\n\n\n##\n\n> Dask DataFrame helps you process large tabular data by parallelizing pandas, either on your laptop for larger-than-memory computing, or on a distributed cluster of computers.\n\n![](IMG/dask-dataframe.svg)\n\n> Just pandas: Dask DataFrames are a collection of many pandas DataFrames.\n\n> The API is the same. The execution is the same.\n\n> Large scale: Works on 100 GiB on a laptop, or 100 TiB on a cluster.\n\n> Easy to use: Pure Python, easy to set up and debug.\n\n> Column of four squares collectively labeled as a Dask DataFrame with a single constituent square labeled as a pandas DataFrame.\n\n> Dask DataFrames coordinate many pandas DataFrames/Series arranged along the index. A Dask DataFrame is partitioned row-wise, grouping rows by index value for efficiency. These pandas objects may live on disk or on other machines.\n\n##\n\n[Demo](https://www.youtube.com/watch?v=HTKzEDa2GA8)\n\n\n------------------------------------------------------------------------\n\n## Creating a dask dataframe\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nindex = pd.date_range(\"2021-09-01\", \n                      periods=2400, \n                      freq=\"1H\")\n\ndf = pd.DataFrame({\n  \"a\": np.arange(2400), \n  \"b\": list(\"abcaddbe\" * 300)}, \n  index=index)\n  \nddf = dd.from_pandas(df, npartitions=20) # <1>\n\nddf.head()                              # <2>\n```\n:::\n\n\n\n1.  As in Spark, in Dask, proper partitioning is a key performance issue\n2.  The dataframe API is (almost) the same as in Pandas!\n\n\n------------------------------------------------------------------------\n\n## Inside the dataframe\n\n::: columns\n\n::: {.column width=\"50%\"}\n\n::: smaller\n\n### A sketch of the interplay between index and partitioning\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nddf.divisions\n```\n:::\n\n\n\n### A dataframe has a task graph\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nddf.visualize()\n```\n:::\n\n\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n::: smaller\n\n\n### What's in a partition?\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nddf.partitions[1] # <1>\n```\n:::\n\n\n\n1.  This is the second class of the partition\n\n### Slicing\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nddf[\"2021-10-01\":\"2021-10-09 5:00\"] # <1>\n```\n:::\n\n\n\n1.  Like slicing NumPy arrays or pandas DataFrame.\n:::\n:::\n:::\n\n\n##\n\n> Dask DataFrames coordinate many Pandas DataFrames/Series arranged along an index. \n\n> We define a Dask DataFrame object with the following components:\n\n> - A Dask graph with a special set of keys designating partitions, such as ('x', 0), ('x', 1), ...\n\n> - A name to identify which keys in the Dask graph refer to this DataFrame, such as 'x'\n\n> - An empty Pandas object containing appropriate *metadata* (e.g. column names, dtypes, etc.)\n\n> - A sequence of partition boundaries along the index called *divisions*\n\n\n------------------------------------------------------------------------\n\n## Methods\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\n( \n  ddf.a\n    .mean()\n)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n( \n  ddf.a\n    .mean()\n    .compute()\n)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n(\n  ddf\n    .b\n    .unique()\n)\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n::: smaller\n\n\n:::\n:::\n\n:::\n\n------------------------------------------------------------------------\n\n## Reading and writing from `parquet`\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfname = 'fhvhv_tripdata_2022-11.parquet'\ndpath = '../../../../Downloads/'\n\nglobpath = 'fhvhv_tripdata_20*-*.parquet'\n\n!ls -l ../../../../Downloads/fhvhv_tripdata_20*-*.parquet\n```\n:::\n\n\n\n---\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n%%time \n\ndata = dd.read_parquet(os.path.join(dpath, globpath),\n                       categories= ['PULocationID',      'DOLocationID'], \n                       engine='auto'\n                      )\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ntype(data)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf = data.to_dask_dataframe()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.info()\ndf._meta.dtypes\n\ndf.npartitions\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.head()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ntype(df)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf._meta.dtypes\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf._meta_nonempty\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.info()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.divisions\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.describe(include=\"all\")\n```\n:::\n\n\n\n## Partitioning and saving to parquet\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pyarrow as pa\n\nschm = pa.Schema.from_pandas(df._meta)\n\nschm\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.PULocationID.unique().compute()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.to_parquet( \n  'fhvhv_tripdata_2022-11',\n  partition_on= ['PULocationID'],\n  engine='pyarrow', \n  schema = schm\n  )\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.info(memory_usage=True)\n```\n:::\n\n\n\n\n\n\n\n\n# Schedulers   {background-color=\"#1c191c\"}\n\n---\n\n> After you have generated a task graph, it is the scheduler's job to execute it (see Scheduling).\n\n> By default, for the majority of Dask APIs, when you call `compute()` on a Dask object, Dask uses the *thread pool* on your computer (a.k.a threaded scheduler) to run computations in parallel. This is true for `Dask Array`, `Dask DataFrame`, and `Dask Delayed`. The exception being `Dask Bag` which uses the multiprocessing scheduler by default.\n\n> If you want more control, use the `distributed scheduler` instead. Despite having \"distributed\" in it's name, the distributed scheduler works well on both single and multiple machines. Think of it as the \"advanced scheduler\".\n\n------------------------------------------------------------------------\n\n# Performance  {background-color=\"#1c191c\"}\n\n------------------------------------------------------------------------\n\n> Dask schedulers come with diagnostics to help you understand the performance characteristics of your computations.\n\n\n> By using these diagnostics and with some thought, we can often identify the slow parts of troublesome computations.\n\n> The single-machine and distributed schedulers come with different diagnostic tools. These tools are deeply integrated into each scheduler, so a tool designed for one will not transfer over to the other.\n\n------------------------------------------------------------------------\n\n## Visualize task graphs\n\n------------------------------------------------------------------------\n\n## Single threaded scheduler and a normal Python profiler\n\n------------------------------------------------------------------------\n\n## Diagnostics for the single-machine scheduler\n\n------------------------------------------------------------------------\n\n## Diagnostics for the distributed scheduler and dashboard\n\n\n# References   {background-color=\"#1c191c\"}\n\n##  Reference\n    *  [Docs](https://dask.org/)\n    *  [Examples](https://examples.dask.org/)\n    *  [Code](https://github.com/dask/dask/)\n    *  [Blog](https://blog.dask.org/)\n  \n##  Tutorials\n    *  [Scipy 2020]()\n\n##  Ask for help\n    *   [`dask`](http://stackoverflow.com/questions/tagged/dask) tag on Stack Overflow, for usage questions\n    *   [github issues](https://github.com/dask/dask/issues/new) for bug reports and feature requests\n    *   [gitter chat](https://gitter.im/dask/dask) for general, non-bug, discussion\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}