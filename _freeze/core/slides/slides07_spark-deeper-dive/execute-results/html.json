{
  "hash": "e5b70bd59a2aa9497d7fb90948b39c1b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Spark deeper dive\"\nengine: knitr\ndate: \"2025-01-17\"\n--- \n\n\n\n\n\n#  A deeper dive into Spark {.unlisted background-color=\"#1c191c\"}\n\n \n\n\n\n#  Shuffles {background-color=\"#1c191c\"}\n\n\n\n## Shuffles\n\nWhat happens when we do a `reduceByKey` on a RDD?\n\n```{.python}\n>>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n>>> rdd.reduceByKey(lambda a, b: a + b).collect()\n```\n\nLet's have a look at the *Spark UI* available at :\n\n\nhttp://localhost:4040/jobs/]\n\n<img src=\"IMG/sparkui.png\" style=\"width: 100%;\" />\n\n::: {.notes} \n\n\nRelate shuffles to wide transformations \n\nIn Spark parlance, in a narrow transformation, each input partition (class of the input partition) \ncontributes to at most one output partition (one class of the output partition). \n\nIn a wide transformation, input partitions contribute to several or even many output partitions. \n\nShuffles correspond to wide transformations.\n\n:::\n \n\n## Shuffles\n\n\n\n\n<img src=\"IMG/dag_reducebykey1.png\" style=\"width: 30%;\" />\n<img src=\"\" style=\"width: 10%;\" />\n<img src=\"IMG/dag_reducebykey2.png\" style=\"width: 30%;\" />\n\n\n- Spark has to **move data from one node to another** to be \"grouped\" with its \"key\"\n\n- Doing this is called *shuffling*. It is never called directly, it happens behind the curtains for some other functions as `reduceByKey` above.\n\n- This might be *very expensive* because of **latency**\n\n\n::: {.notes} \n\n:::\n\n \n\n## Shuffles\n\n- `reduceByKey` results in *one* key-value pair **per key**\n\n- This single key-value pair **cannot** span across **multiple workers**.\n\n\n\n## Example\n\n```{.python}\n>>> from collections import namedtuple\n>>> columns = [\"client_id\", \"destination\", \"price\"]\n>>> CFFPurchase = namedtuple(\"CFFPurchase\", columns)\n>>> CFFPurchase(100, \"Geneva\", 22.25)\nCFFPurchase(client_id=100, destination='Geneva', price=22.25)\n```\n\n**Goal**: calculate *how many trips and how much money was spent by each client*\n\n \n\n## Example (cont.)\n\n```{.python}\n>>> purchases = [\n        CFFPurchase(100, \"Geneva\", 22.25),\n        CFFPurchase(100, \"Zurich\", 42.10),\n        CFFPurchase(100, \"Fribourg\", 12.40),\n        CFFPurchase(101, \"St.Gallen\", 8.20),\n        CFFPurchase(101, \"Lucerne\", 31.60),\n        CFFPurchase(100, \"Basel\", 16.20)\n    ]\n>>> purchases = sc.parallelize(purchases)\n>>> purchases.collect()\n[CFFPurchase(client_id=100, destination='Geneva', price=22.25),\n CFFPurchase(client_id=100, destination='Zurich', price=42.1),\n CFFPurchase(client_id=100, destination='Fribourg', price=12.4),\n CFFPurchase(client_id=101, destination='St.Gallen', price=8.2),\n CFFPurchase(client_id=101, destination='Lucerne', price=31.6),\n CFFPurchase(client_id=100, destination='Basel', price=16.2)]\n```\n\n::: {.notes} \n\nHow are `namedtuple` translated into Spark types?\n\n:::\n\n## Example (cont.)\n\n```{.python}\n>>> purchases_per_client = (purchases\n        # Pair RDD\n        .map(lambda p: (p.client_id, p.price))\n        # RDD[p.customerId, List[p.price]]\n        .groupByKey()                          \n        .map(lambda p: (p[0], (len(p[1]), sum(p[1]))))\n        .collect()\n    )\n>>> purchases_per_client\n[(100, (4, 92.95)), (101, (2, 39.8))]\n```\n\nHow would this **looks on a cluster?** (imagine that the dataset has millions of purchases)\n\n \n\n## Shuffles\n\n- Lets say we have **3 worker nodes** and our data is **evenly distributed on it**, so the operations above look like this:\n\n\n<img src=\"IMG/shuffline.png\" style=\"width: 95%;\" />\n\n- This shuffling is very expensive because of *latency*\n\n- Can we do a **better job**?\n\n \n\n## Shuffles\n\nPerhaps we can *reduce before we shuffle* in order to greatly reduce the amount of data sent over network. We use `reduceByKey`.\n\n```{.python}\n>>> purchases_per_client = (purchases\n        .map(lambda p: (p.client_id, (1, p.price)))\n        .reduceByKey(lambda v1, v2: (v1[0] + v2[0], v1[1] + v2[1]))\n        .collect()\n    )\n```\n\nThis looks like on the cluster:\n \n\n<img src=\"IMG/shuffline_2.png\" style=\"width: 80%;\" />\n\n\n \n\n## Shuffles\n\n- `groupByKey` (left) VS `reduceByKey` (right) :\n\n\n\n<img src=\"IMG/shuffline.png\" style=\"width: 49%;\" />\n<img src=\"IMG/shuffline_2.png\" style=\"width: 49%;\" />\n\n\n- With `reduceByKey` we shuffle considerably less amount of data\n\n**Benefits of this approach:** \n\n- by *reducing the data first*, the amount of data sent during the shuffle is **greatly reduced**, leading to strong performance gains!\n- This is because `groupByKey` requires collecting **all key-value pairs with the same key on the same machine** while `reduceByKey` **reduces locally** before shuffling.\n\n \n\n\n\n#  Partitions {background-color=\"#1c191c\"}\n\n \n\n\n## Partitioning\n\nHow does Spark know which key to put on which machine?\n\n- By default, Spark uses *hash partitioning* to determine **which key-value pair** should be sent to **which machine**.\n\nThe data **within an RDD** is *split* into several *partitions*. Some properties of partitions:\n\n- Partitions **never** span *multiple machines*, data in the **same partition** is always on the **same worker machine**.\n- **Each machine** in the cluster contains **one** or **more** partitions.\n- The *number of partitions* to use is **configurable**. By default, it is the *total number of cores on all executor nodes*;  6 workers with 4 cores should lead to 24 partitions.\n\n \n\n## Partitioning\n\nTwo kinds of partitioning are available in Spark: \n\n- *Hash* partitioning\n\n- *Range* partitioning\n\nCustomizing a partitioning is only possible on a `PairRDD` and `DataFrame`, namely **something with keys**.\n\n\n\n## Hash Partitioning\n\nGiven a Pair RDD that should be grouped, `groupByKey` first computes per tuple `(k,v)` its partition `p`:\n\n```{.python}\np = k.hashCode() % numPartitions\n```\n\nThen, all tuples in the same partition `p` are sent to the machine hosting `p`.\n\n\n\n## Partitioning\n\n**Intuition:** hash partitioning attempts to *spread data evenly* across partitions **based on the key**.\n\nThe other kind of partitioning is *range partitioning*\n\n\n## Range Partitioning\n\n- Pair RDDs may contain keys that have an *ordering* defined, such as `int`, `String`, etc.\n- For such RDDs, *range partitioning* may be more efficient.\n\nUsing a **range partitioner**, keys are partitioned according to 2 things:\n\n1. an **ordering** for keys\n1. a set of **sorted ranges** of keys\n\n(key, value) pairs with keys in the **same range** end up in the **same partition**.\n\n## Partitioning\n\nConsider a Pair RDD with keys: `[8, 96, 240, 400, 401, 800]`, and a desired number of partitions of 4.\n\nWith **hash partitioning**\n\n```{.python}\np = k.hashCode() % numPartitions\n  = k % 4\n``` \n\nleads to\n- Partition 0: `[8, 96, 240, 400, 800]`\n- Partition 1: `[401]`\n- Partition 2: `[]`\n- Partition 3: `[]`\n\nThis results in a *very unbalanced distribution* which **hurts performance**, since the data is **spread mostly on 1 node**, so not very parallel.\n\n\n\n## Partitioning\n\nIn this case, using **range partitioning** can *improve the distribution* significantly.\n\n- Assumptions: (a) keys are non-negative, (b) 800 is the biggest key\n- Ranges: `[1-200], [201-400], [401-600], [601-800]`\n\nBased on this the keys are distributed as follows:\n\n- Partition 0: `[8, 96]`\n- Partition 1: `[240, 400]`\n- Partition 2: `[401]`\n- Partition 3: `[800]`\n\nThis is *much more balanced*.\n\n\n## Partitioning\n\nHow do we set a partitioning for our data?\n\n1. On a Pair RDD: call `partitionBy`, providing an explicit `Partitioner` (`scala` only, use a partitioning function in `pyspark`)\n\n1. On a DataFrame: Call `repartition` for **hash partitioning** and `repartitionByRange` for **range partitioning**\n\n1. Using transformations that return a RDD or a DataFrame with **specific partitioners**.\n\n\n\n## Partitioning a `RDD` using `partitionBy`\n\n```{.python}\n>>> pairs = purchases.map(lambda p: (p.client_id, p.price))\n>>> pairs = pairs.partitionBy(3, \"client_id\")\n>>> pairs.getNumPartitions()\n3\n```\n\n## Partitioning\n\nUsing `RangePartitioner` with `pyspark` requires\n\n1. Specifying the desired number of partitions\n\n1. Providing a DataFrame with **orderable keys**\n\n```{.python}\npairs = purchases.map(lambda p: (p.client_id, p.price))\npairs = spark.createDataFrame(pairs, [\"id\", \"price\"])\npairs.repartitionByRange(3, \"price\").persist()\npairs.show()\n```\n\n::: {.callout-important}\n\n- The result of `partitionBy`, `repartition`, `repartitionByRange` *should be persisted*. \n\n- Otherwise **partitioning is repeatedly applied** (with shuffling!) each time the partitioned data is used.\n\n:::\n\n\n## Partitioning using transformations\n\n- Pair RDDs that are **result of a transformation** on a *partitioned* Pair RDD use typically the *same* hash partitioner\n\n- Some operations on RDDs **automatically** result in an RDD with a **known** partitioner - when it makes sense. \n\n\n::: {.callout-note}\n\n### Examples\n\n- When using `sortByKey`, a `RangePartitioner` is used. \n\n- With `groupByKey`, a default hash partitioner is used.\n\n:::\n\n\n## Operations on Pair RDDs that **hold to** and **propagate** a partitioner:\n\n- `cogroup`, `groupWith`, `join`, `leftOuterJoin`, `rightOuterJoin`\n\n- `[group,reduce,fold,combine]ByKey`, `partitionBy`, `sort`\n\n- `mapValues`, `flatMapValues`, `filter` (if parent has a partitioner)\n\n**All other operations** will produce a result *without partitioner*! \n\n::: {.notes} \n\nWhy?\n\n:::\n\n\n## Partitioning\n\nConsider the `map` transformation. Given that we have a hash-partitioned Pair RDD, *why loosing the partitioner* in the returned RDD?\n\nBecause its possible for `map` or `flatMap` to *change* the key:\n```{.python}\nhashPartitionedRdd.map(lambda k, v: (\"ooooops\", v))\n```\nIf the `map` transformation preserved the previous partitioner, it would no longer makes sense: the keys are all same after this `map`\n\nHence, use `mapValues`, since it enables to do `map` transformations **without changing the keys**, thereby preserving the partitioner.\n\n\n\n#  Optimizing shuffles with partitioning {background-color=\"#1c191c\"}\n\n\n## Optimizing with partitioning\n\n**Why would we want to repartition the data?**\n\n- Because it can bring *substantial performance gains*, especially before **shuffles**.\n\n- We saw that using `reduceByKey` instead of `groupByKey` *localizes data better* due to different **partitioning** strategies and thus **reduces latency** to deliver **performance gains**.\n\n- By manually repartitioning the data for the same example as before, we can *improve the performance even further*.\n\n- By using **range partitioners** we can optimize the use of `reduceByKey` in that example so that it *does not involve any shuffling* over the network at all!\n\n\n## Optimizing with partitioning\n\nCompared to what we did previously, we use `sortByKey` to produce a range partitioner for the RDD that we immediately persist.\n\n```{.python}\n>>> pairs = purchases.map(lambda p: (p.client_id, (1, p.price)))\n>>> pairs = pairs.sortByKey().persist()\n>>> pairs.reduceByKey(\n       lambda v1, v2: (v1[0] + v2[0], v1[1] + v2[1])\n    ).collect()\n```\n\nThis typically leads to *much faster computations* in this case (for large RDD, not the small toy example from before).\n\n\n\n#  Optimizing with broadcast join {background-color=\"#1c191c\"}\n\n\n## Optimizing with broadcasting\n\nWhen joining two DataDrames, where one is small enough to **fit in memory**, it is *broadcasted* over **all the workers** where the large DataFrame resides (and a hash join is performed). This has two phases:\n\n- **Broadcast**: the smaller DataFrame is broadcasted across workers containing the large one\n- **Hash join**: a standard hash join is executed on each workder\n\nThere is therefore *no shuffling* involved and this can be **much faster** than a regular join.\n\nThe default threshold for broadcasting is\n```{.python}\n>>> spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n'10485760'\n```\nmeaning 10MB\n\n\n\n## Optimizing with broadcasting\n\nCan be changed using\n```{.python}\n>>> spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"20971520\")\n```\n\n`\"spark.broadcast.compress\"` can be used to configure whether to compress the data before sending it (`True` by default). \n\nIt uses the compression specified in `\"spark.io.compression.codec config\"` and the default is `\"lz4\"`. We can use other compression codecs but what the hell. \n\nMore important: even though a DataFrame is small, sometimes Spark can't estimate the size of it. We can **enforce** using a *broadcast hint*:\n```{.python}\n>>> from pyspark.sql.functions import broadcast\n>>> largeDF.join(broadcast(smallDF))\n```\n\n\n## Optimizing with broadcasting\n\n- If a **broadcast hint** is specified, the side with the hint will be *broadcasted* irrespective of `autoBroadcastJoinThreshold`. \n\n- If **both sides** have broadcast hints, the side **with a smallest estimated size** will be broadcasted. \n\n- If there is **no hint** and the estimated size of DataFrame < `autoBroadcastJoinThreshold`, that table is usually broadcasted\n\n- Spark has a **BitTorrent-like** implementation to perform broadcast. Allows to **avoid the driver being the bottleneck** when sending data to multiple executors. \n\n## Optimizing with broadcasting\n\n- **Usually**, a **broadcast join** performs *faster* than other join algorithms when the broadcast side is small enough. \n\n- However, broadcasting tables is **network-intensive** and can cause `out of memory` errors or even **perform worse** than other joins if the *broadcasted table is too large*.\n\n- Broadcast join is **not** supported for a **full outer join**. For **right outer join**, only **left side** table can be broadcasted and for other **left joins** only the **right table** can be broadcasted.\n\n\n#  Shuffle hash VS sort merge joins {background-color=\"#1c191c\"}\n\n\n## Shuffle hash VS sort merge joins \n\nSpark can use mainly **two strategies** for joining:\n\n- *Shuffle hash* join\n- *Sort merge* join\n\nSort merge join is the **default join strategy**, since it is very scalable and *performs better* than other joins **most of the times**.\n\n**Shuffle hash join** is used as the join strategy when: \n\n- `spark.sql.join.preferSortMergeJoin` is set to `False`\n- the cost to build a hash map is **less** than sorting the data. \n\n\n## Shuffle hash\n\n**Shuffle hash** join has 2 phases:\n\n1. A *shuffle* phase, where data from the join tables are **partitioned based on the join key**. This **shuffles data** across the partitions. Data with the *same keys* end up in the *same partition*: the **data required** for joins is available in the **same partition**.\n1. *Hash join* phase: data on **each partition** performs a **single node** hash join.\n\nThus, shuffle hash join **breaks apart** the big join of two tables into **localized smaller chunks**.\n\n- Shuffle is **very expensive**, the creation of Hash tables is also **expensive** and **memory bound**. \n- Shuffle hash join is **not suited** for joins that *wouldn't fit in memory*.\n\n\n## Shuffle hash\n\n- The **performance** of shuffle hash join depends on the *distribution of the keys*. The **greater** number of **unique** join keys the **better** data distribution we get.\n- Maximum amount of **achievable parallelism** is proportional to the **number of unique keys**.\n\nBy default, **sort merge join** is *preferred* over **shuffle hash join**.\n`ShuffledHashJoin` is still useful when:\n\n- Any partition of the resulting table can **fit in memory**\n- One table is **much smaller** than the other one, so that building a **hash table** of the **small** table is *smaller* than **sorting the large table**.\n\nThis explains why this shuffle is used for **broadcast joins**.\n\n\n## Sort merge join\n\n**Sort merge join** is Spark's *default join strategy* if:\n\n- matching join keys are **sortable** \n- **not eligible** for **broadcast join** or **shuffle hash join**. \n\nIt is **very scalable** and is an **inheritance of Hadoop** and map-reduce programs.\nWhat makes it **scalable** is that it can **spill data to the disk** and doesn't require the **entire data** to *fit inside the memory*.\n\nIt has **three** phases:\n\n1. *Shuffle* phase: the two large tables are **(re)partitioned** using the join key(s) across the partitions in the cluster.\n1. *Sort* phase: **sort** the data **within each partition** in parallel.\n1. *Merge* phase: **join** the **sorted** and **partitioned** data. This is simply **merging the datasets** by iterating over the elements and joining the rows having the **same value for the join key**.\n\n\n## Sort merge join\n\n- For ideal performance of the sort merge join, all rows **with the same join key(s)** are available in the **same partition**. This can help with the infamous partition exchange (shuffle) between workers.\n- **Collocated partitions** can *avoid unnecessary data shuffle*. \n- Data needs to be *evenly distributed* in the join keys, so that they can be **equally distributed across the cluster** to achieve the **maximum parallelism** from the available partitions.\n\n## Other join types\n\nThere are **other join types**, such as `BroadcastNestedLoopJoin` in weird situations where *no joining keys are specified* and either there is a broadcast hint or the size of a table is < `autoBroadcastJoinThreshold`.\n\nIn words: *don't use these*, if you see these in an execution plan or in the Spark UI, it usually means that something **has been done poorly**.\n\n\n## Take home messages on joins\n\n- **Sort merge join** is the *default* join and performs well in most of the scenarios.\n\n- In **some** cases, if you are confident enough that **shuffle hash join** is *better* than sort merge join, you can **disable sort merge join** for those scenarios. \n\n- Tune `spark.sql.autoBroadcastJoinThreshold` accordingly if deemed necessary. Try to use *broadcast joins* wherever possible and *filter out the irrelevant rows* to the join key **before** the join to **avoid unnecessary data shuffling**.\n\n- Joins **without unique join keys** or **no join keys** can often be very expensive and *should be avoided*.\n\n\n## How do I know when a shuffle will occur?\n\n**Rule of thumb**: a shuffle **can** occur when the resulting data *depends on other data* (can be the same or another RDD/DataFrame).\n\nWe can also **figure out** if a *shuffle* has been **planned** or **executed** via:\n\n1. The return type of certain transformations (in `scala` only)\n1. By looking at the *Spark UI*\n1. Using `toDebugString` on a RDD to see its execution plan: \n\n```{.python}\n>>> print(pairs.toDebugString().decode(\"utf-8\"))\n(3) PythonRDD[157] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n |       CachedPartitions: 3; MemorySize: 233.0 B; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n |  MapPartitionsRDD[156] at mapPartitions at PythonRDD.scala:133 [Memory Serialized 1x Replicated]\n |  ShuffledRDD[155] at partitionBy at <unknown>:0 [Memory Serialized 1x Replicated]\n +-(8) PairwiseRDD[154] at sortByKey at <ipython-input-35-112008c310ec>:2 [Memory Serialized 1x Replicated]\n    |  PythonRDD[153] at sortByKey at <ipython-input-35-112008c310ec>:2 [Memory Serialized 1x Replicated]\n    |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 [Memory Serialized 1x Replicated]\n```\n\n## How do I know when a shuffle will occur?\n\nOperations that **might** cause a shuffle:\n\n- `cogroup`, `groupWith`, `join`, `leftOuterJoin`, `rightOuterJoin`, `groupByKey`, `reduceByKey`, `combineByKey`, `distinct`, `intersection`, `repartition`, `coalesce`\n\nWhen can we *avoid shuffles* using **partitioning** ?\n\n1. `reduceByKey` running on a pre-partitioned RDD will cause the values to be computed **locally**, requiring only the final reduced value to be sent to the driver.\n1. `join` called on 2 RDDs that are pre-partitioned with the same partitioner and cached on the same machine will cause the join to be computed **locally**, with no shuffling across the network.\n\n\n#  Wide versus narrow dependencies   {background-color=\"#1c191c\"}\n\n\n## Wide versus narrow dependencies\n\n- We have seen that some transformations are **significantly more expensive (latency)** than others\n\n- This is often explained by *wide* versus *narrow dependencies*, which dictate **relationships between RDDs** in **graphs of computation**, that has a lot to do with *shuffling*\n\n## Lineages\n\n- Computations on RDDs are represented as a *lineage graph*: a *DAG* representing the **computations done on the RDD**. \n\n- This DAG is what Spark **analyzes** to do **optimizations**. Thanks to this, it is possible for an operation to step back and figure out how a result is derived from a particular point.\n\n\n## Wide versus narrow dependencies\n\n::: {.columns}\n::: {.column width=\"50%\"}\n\n**Example**\n\n```{.python}\nrdd = sc.textFile(...)\n\nfiltered = (\n  rdd.map(...)\n     .filter(...)\n     .persist()\n)\n\ncount = filtered.count()\nreduced = filtered.reduce()\n```\n\n:::\n\n::: {.column width=\"50%\"}\n\n<img src=\"IMG/lineage_graph.png\" />\n\n:::\n::: \n\n<!-- end columns -->\n\n\n\n \n\n## Wide versus narrow dependencies  {.smaller .scrollable}\n\n::: {.columns}\n\n::: {.column width=\"30%\"}\n\nRDDs are made up of **4 parts**: \n\n- **Partitions**: *atomic pieces* of the dataset. **One** or **many** per worker\n\n- **Dependencies**: models *relationship* between **this RDD** and **its partitions** with the RDD(s) it was derived from (dependencies maybe modeled per partition) \n\n- A **function** for *computing the dataset* based on its parent RDDs\n\n- **Metadata** about partitioning *scheme* and data placement.\n\n\n:::\n\n::: {.column width=\"5%\"}\n\n\n:::\n\n\n\n::: {.column width=\"30%\"}\n\n<img src=\"IMG/rdd_anatomy_1.png\" />\n\n:::\n\n::: {.column width=\"5%\"}\n\n\n:::\n\n::: {.column width=\"30%\"}\n\n<img src=\"IMG/rdd_anatomy_2.png\"  />\n\n\n:::\n\n\n::: \n\n<!-- end columns -->\n\n\n \n\n## Wide versus narrow dependencies\n\n**RDD dependencies and shuffles**\n\n- We saw a **rule of thumb**: a *shuffle* can occur when the **resulting RDD depends on other elements** from the same RDD or another RDD.\n- In fact, *RDD dependencies* **encode** when data *must be shuffled*.\n\nTransformations cause shuffles, and can have **2 kinds of dependencies**:\n\n- **Narrow dependencies:** each partition of the parent RDD is used by *at most* one partition of the child RDD. \n\n\n```{.python}\n[parent RDD partition] --> [child RDD partition]\n```\n\n\n**Fast!** *No shuffle* necessary. Optimizations like **pipelining** possible. \n\n**Transformations with narrow dependencies are fast**.\n\n \n\n## Wide versus narrow dependencies\n\n- **Wide dependencies:** each partition of the parent RDD may be used by *multiple* child partitions\n\n\n\n\n```{.default}\n                           ---> [child RDD partition 1]\n[parent RDD partition] ---> [child RDD partition 2]\n                           ---> [child RDD partition 3]\n```\n\n\n\n**Slow!** *Shuffle necessary* for **all** or **some** data over the network. \n\n**Transformations with wide dependencies are slow.**\n\n \n\n## Wide versus narrow dependencies\n\n\n\n<img src=\"IMG/narrow_vs_wide_dependencies.png\" style=\"width: 100%;\" />\n\n\n \n\n## Wide versus narrow dependencies\n\nAssume that we have a following DAG: \n\n\n<img src=\"IMG/visual_dag.png\" style=\"width: 55%;\" />]\n\n \n\n## Wide versus narrow dependencies\n\nWhat do the dependencies look like? Which is **wide** and which is **narrow?**\n\n\n\n<img src=\"IMG/visual_dag_resolved.png\" style=\"width: 85%;\" />\n\n\n- The B to G `join` is *narrow* because `groupByKey` **already partitions** the keys and places them appropriately in B.\n- Thus, `join` operations can be *narrow or wide* depending on **lineage**\n\n \n\n## Wide versus narrow dependencies\n\nTransformations with (usually) *narrow* dependencies:\n\n- `map`, `mapValues`, `flatMap`, `filter`, `mapPartitions`, `mapPartitionsWithIndex`\n\nTransformations with (usually) *wide* dependencies (might cause a **shuffle**):\n\n- `cogroup`, `groupWith`, `join`, `leftOuterJoin`, `rightOuterJoin`, `groupByKey`, `reduceByKey`, `combineByKey`, `distinct`, `intersection`, `repartition`, `coalesce`\n\nThese list are usually correct, but as seen above for `join`, a correct answer **depends on lineage**\n\n \n\n## Wide versus narrow dependencies  {.smaller}\n\nHow do we **find out** if an operation is **wide** or **narrow**?\n\n- Monitor the job with the *Spark UI* and check if **ShuffleRDD** are used.\n\n- Use the `toDebugString` method. It prints the **RDD lineage** along with other information relevant to scheduling. Indentations separate **groups of narrow transformations** that may be **pipelined** together with wide transformations that require shuffles. These groupings are called **stages**.\n\n```{.python}\n>>> print(pairs.toDebugString().decode(\"utf-8\"))\n(3) PythonRDD[157] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n |       CachedPartitions: 3; MemorySize: 233.0 B; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n |  MapPartitionsRDD[156] at mapPartitions at PythonRDD.scala:133 [Memory Serialized 1x Replicated]\n |  ShuffledRDD[155] at partitionBy at <unknown>:0 [Memory Serialized 1x Replicated]\n +-(8) PairwiseRDD[154] at sortByKey at <ipython-input-35-112008c310ec>:2 [Memory Serialized 1x Replicated]\n    |  PythonRDD[153] at sortByKey at <ipython-input-35-112008c310ec>:2 [Memory Serialized 1x Replicated]\n    |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 [Memory Serialized 1x Replicated]\n```\n\n \n\n## Wide versus narrow dependencies {.smaller}\n\n- Check the *dependencies* (only with `scala` and `java` APIs): there is a `dependencies` method on RDDs. It returns a sequence of `Dependency` objects, which are the dependencies used by **Spark's scheduler** to know how this RDD **depends on other (or itself) RDDs**.\n\nThe types of **dependency objects** that this method may return include:\n\n- **Narrow** dependency objects: `OneToOneDependency`, `PruneDependency`, `RangeDependency`\n- **Wide** dependency objects: `ShuffleDependency`\n\nExample in `scala`:\n```scala\nval wordsRDD = sc.parallelize(largeList)\nval pairs = wordsRdd.map(c=>(c,1))\n                    .groupByKey\n                    .dependencies\n// pairs: Seq[org.apache.spark.Dependency[_]] \n//   = List(org.apache.spark.ShuffleDependency@4294a23d)\n```\n\n \n\n## Wide versus narrow dependencies\n\nAlso, `toDebugString` is more precise with the `scala` API:\n\n```{.scala}\nval pairs = wordsRdd.map(c=>(c,1))\n                    .groupByKey\n                    .toDebugString\n// pairs: String =\n// (8) ShuffledRDD[219] at groupByKey at <console>:38 []\n//  +-(8) MapPartitionsRDD[218] at map at <console>:37 []\n//     | ParallelCollectionRDD[217] at parallelize at <console>:36 []\n```\n\nWe can see immediately that a `ShuffledRDD` is used\n\n \n\n\n#  Lineage allows fault tolerance  {background-color=\"#1c191c\"}\n\n\n \n\n## Lineage allows fault tolerance  {.smaller}\n\n**Lineages** are the key to *fault tolerance* in Spark\n\nIdeas from **functional programming** enable fault tolerance in Spark:\n\n- RDDs are *immutable*\n- Use **higher order functions** like `map`, `flatMap`, `filter` to do *functional* transformations on this **immutable** data\n- A function for **computing an RDD based on its parent RDDs** is part the RDD's representation\n\nThis is all done in Spark RDDs: a by product of these ideas is *fault tolerance*:\n\n- We just need to **keep the information** required by these 3 properties.\n- **Recovering from failures** is simply achieved by *recomputing lost partitions* using the lineage graph\n\n \n\n## Lineage allows fault tolerance {.smaller}\n\nAren't you amazed by this ?!?\n\n- Spark provides **fault tolerance** *without having to write data on disk!*\n- Data can be **rebuilt** using the above information.\n\n::: {.columns}\n\n::: {.column width=\"50%\"}\n\nIf a partition **fails**:\n\n![](IMG/fault_tol_1.png){height=\"400\"}\n\n:::\n\n\n::: {.column width=\"50%\"}\n\nSpark **recomputes it** to get back on track:\n\n![](IMG/fault_tol_2.png){height=\"400\"}\n\n:::\n\n::: \n\n\n\n \n\n\n\n# Catalyst: the secret weapon of `spark.sql` {background-color=\"#1c191c\"}\n\n \n\n## Catalyst optimizer  {.smaller}\n\nWhat is the *Catalyst* **optimizer** ?\n\n::: {.incremental}\n\n- An **optimizer** that **automatically finds** out the *most efficient plan* to execute data operations specified in the user's program.\n\n- It *translates transformations* used to build the dataset to an **optimized physical plan of execution**, which is a DAG of **low-level operations on RDDs**.\n\n- A *precious tool* for `spark.sql` in terms of **performance**. It understands the **structure** of the data used and of the **operations** made on it, so the optimizer *can make some decisions* helping to **reduce execution time**.\n\n:::\n\n## Catalyst optimizer  {.smaller}\n\nLet's first define some terminology used in the optimizer\n\n::: {.incremental}\n\n- *Logical plan*: **series of algebraic** or **language constructs**, for example: SELECT, GROUP BY, UNION, etc. Usually **represented as a DAG** where **nodes** are the constructs.\n\n- *Physical plan*: similar to the logical plan, also **represented by a DAG** but concerning **low-level operations** (operations on RDDs).\n\n- *Unoptimized/optimized plans*:  a plan becomes **optimized** when the optimizer passed on it and **made some optimizations**, such as merging `filter()` methods, replacing some instructions by faster another ones, etc.\n\n:::\n\n\nCatalyst helps to move from **an unoptimized logical query plan** to an **optimized physical plan** \n\n \n\n## Catalyst optimizer: how it works?\n\n\n\n<img src=\"IMG/catalyst01.png\" style=\"width: 90%;\" />\n\n\n \n\n## Catalyst optimizer: how it works?  {.smaller}\n\n::: {.incremental}\n\n- Try to *optimize logical plan* through **predefined rule-based optimizations**. Some optimizations are:\n  - **predicate or projection pushdown**, helps to eliminate data not respecting preconditions earlier in the computation;\n  - **rearrange filter**;\n  - **conversion of decimals operations** to long integer operations;\n  - **replacement of some RegEx** expressions by Java's methods `startsWith` or `contains`;\n  - **if-else** clauses simplification.\n\n- Create the **optimized logical plan**.\n\n- Construct *multiple physical plans* from the **optimized logical plan**. These are also optimized, some examples are: **merging** different `filter()`, sending **predicate/projection pushdown** to the **data source** to eliminate data directly from the source.\n\n:::\n\n## Catalyst optimizer: how it works? {.smaller}\n\n::: {.incremental}\n\n- Determine **which physical plan** has the *lowest cost of execution* and choses it as the **physical plan used for the computation**.\n\n- Generate *bytecode* for the **best physical plan** thanks to a `scala` feature called `quasiquotes`.\n\n- Once a physical plan is defined, it’s *executed* and retrieved data is put to the output DataFrame.\n\n::: \n\n## Catalyst optimizer\n\nLet’s understand how Catalyst optimizer works for a given query\n\n\n<img src=\"IMG/catalyst02.png\" style=\"width: 80%;\" />\n\n \n\n## Catalyst optimizer\n\n\n<img src=\"IMG/catalyst03.png\" style=\"width: 100%;\" />\n\n \n\n## Catalyst optimizer\n\n\n<img src=\"IMG/catalyst04.png\" style=\"width: 100%;\" />\n\n \n\n## Catalyst optimizer\n\n\n<img src=\"IMG/catalyst05.png\" style=\"width: 100%;\" />\n\n \n\n## Catalyst optimizer\n\n\n<img src=\"IMG/catalyst06.png\" style=\"width: 100%;\" />\n\n \n\n## Catalyst optimizer\n\n\n<img src=\"IMG/catalyst07.png\" style=\"width: 100%;\" />\n\n \n\n## Catalyst optimizer\n\n\n<img src=\"IMG/catalyst08.png\" style=\"width: 100%;\" />\n\n \n\n## Catalyst optimizer\n\n\n<img src=\"IMG/catalyst09.png\" style=\"width: 100%;\" />\n\n \n\n## Catalyst optimizer\n\n\n<img src=\"IMG/catalyst10.png\" style=\"width: 100%;\" />\n\n \n\n## Catalyst optimizer\n\n\n<img src=\"IMG/catalyst11.png\" style=\"width: 100%;\" />\n\n \n\n## Catalyst optimizer\n\n\n<img src=\"IMG/catalyst12.png\" style=\"width: 100%;\" />\n\n \n\n## Catalyst optimizer\n\n::: {.incremental}\n\n- By performing these **transformations**, Catalyst *improves the execution times* of relational queries and **mitigates the importance of semantics**\n\n- Catalyst makes use of some **powerful functional programming** features from `Scala` to allow developers to concisely specify complex relational optimizations.\n\n- Catalyst helps *but only when it can*: **explicit** schemas, **precise function calls**, **clever** order of operations **can only help** Catalyst.\n\n::: \n\n# Thank you!   {.unlisted background-color=\"#1c191c\"}\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}