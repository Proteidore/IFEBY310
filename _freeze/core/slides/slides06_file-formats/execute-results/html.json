{
  "hash": "bbcd8398a5a31b395f2abe7fa4a1c028",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"File formats\"\nengine: jupyter\ndate: \"2025-01-17\"\n---\n\n\n#  File formats  {background-color=\"#1c191c\"}\n\n\n\n## File formats\n\n- You will need to choose the *right format* for your data\n\n- The right format typically *depends on the use-case*\n\n## Why different file formats ?\n\n- A **huge bottleneck** for big data applications is *time spent to find data* in a particular location and *time spent to write it* back to another location\n\n- Even more complicated with **large datasets** with *evolving schemas*, or *storage constraints*\n\n- Several `Hadoop` file formats **evolved to ease these issues** across a number of use cases\n\n\n## File formats  (trade-offs)\n\nChoosing an appropriate file format has the following potential benefits\n\n- Faster **reads** or faster **writes**\n\n- **Splittable** files\n\n- **Schema evolution** support (schema changes over time)\n\n- **Advanced compression** support\n\nSome file formats are designed for *general use*\n\nOthers for more *specific use cases*\n\nSome with *specific data characteristics* in mind\n\n\n::: {.notes}\n\nWhat is the meaning of *Schema*? May depend on format. \n\n:::\n\n\n\n# Main file formats for big data {background-color=\"#1c191c\"}\n\n\n---\n\n## Main file formats\n\n::: {.center}\n\n\n\n<img src=\"/images/parquet.png\" style=\"width: 30%;\" />\n<img src=\"\" style=\"width: 5%;\" />\n<img src=\"/images/orc.png\" style=\"width: 23%;\" />\n<img src=\"\" style=\"width: 5%;\" />\n<img src=\"/images/avro.png\" style=\"width: 25%;\" />\n\n:::\n\nWe shall talk about the *core concepts* and *use-cases* for the following popular data formats:\n\n- `Avro` : [https://avro.apache.org](https://avro.apache.org)\n\n- `ORC` : [https://parquet.apache.org](https://parquet.apache.org)\n\n- `Parquet` : [https://orc.apache.org](https://orc.apache.org)\n\n\n\n\n#  `Avro` {background-color=\"#1c191c\"}\n\n\n## Avro: Principles \n\n::: {.columns}\n::: {.column}\n\n- `Avro` is a *row-based data format and data serialization system* released by the `Hadoop` working group in 2009\n\n- Data schema is stored as `JSON` in the header. Rest of the data stored in a **binary format** to make it compact and efficient\n\n- `Avro` is language-neutral and can be used by many languages (for now `C`, `C++`, `...`, `Python`, and `R`)\n\n- One shining point of `Avro`: *robust support for schema evolution*\n\n\n:::\n\n::: {.column}\n\n![](/images/avro.png)\n\n:::\n::: \n\n\n\n\n::: {.notes}\n\nAvro is used in streaming applications\n\nOn Hadoop portal, Avro is described as a data serialization system\n\nIn the old days, Avro used to be an aircraft manufacturer\n\n\n> https://en.wikipedia.org/wiki/Avro\n\n> Confluent Platform works with any data format you prefer, but we added some special facilities for Avro because of its popularity. In the rest of this document I’ll go through some of the reasons why.\n\n> Avro has a JSON like data model, but can be represented as either JSON or in a compact binary form. It comes with a very sophisticated schema description language that describes data.\n\n> We think Avro is the best choice for a number of reasons:\n\n- It has a direct mapping to and from JSON\n- It has a very compact format. The bulk of JSON, repeating every field name with every single record, is what makes JSON inefficient for high-volume usage.\n- It is very fast.\n- It has great bindings for a wide variety of programming languages so you can generate Java objects that make working with event data easier, but it does not require code generation so tools can be written generically for any data stream.\n- It has a rich, extensible schema language defined in pure JSON\n- It has the best notion of compatibility for evolving your data over time.\n- Though it may seem like a minor thing handling this kind of metadata turns out to be one of the most critical and least appreciated aspects in keeping data high quality and easily useable at organizational scale.\n\n\n\n:::\n\n\n\n\n\n\n## `Avro`: rationale\n\n- `Avro` provides rich data structures: can create a record that contains an array, an enumerated type and a sub-record\n\nIdeal candidate to *store data in a data lake* since:\n\n1. Data is usually **read as a whole** in a data lake for further processing by downstream systems\n\n2. Downstream systems can **retrieve schemas easily from files** (no need to store the schemas separately).\n\n3. Any source *schema change is easily handled*\n\n\n::: {.aside}\n\n- data lake \n- data warehouse\n- databse \n\nSpot the differences \n\n\n:::\n\n\n## `Avro`: organization\n\n::: {.center}\n\n<img src=\"/images/avro-file.png\" style=\"width: 100%;\" />\n\n:::\n\n\n#  `Parquet` {background-color=\"#1c191c\"}\n\n\n## Parquet: History and Principles\n\n\n::: {.columns}\n\n::: {.column}\n\n- `Parquet` is an open-source file format for [`Hadoop`](https://hadoop.apache.org) created by [`Cloudera`](https://en.wikipedia.org/wiki/Cloudera) and [`Twitter` {{< fa brands twitter >}}]() in 2013\n\n- It stores **nested data structures** in a *flat columnar format*. \n\n- Compared to traditional **row-oriented approaches**, `Parquet` is *more efficient in terms of storage and performance*\n\n- It is especially good for queries that need *read a small subset of columns* from a data file with many columns : *only the required columns are read* (optimized I/O)\n\n:::\n\n::: {.column}\n\n![](/images/parquet.png)\n\n:::\n\n::: \n\n\n<!-- end columns -->\n\n\n\n::: {.notes}\n\n- meaning of nested data structure\n\n:::\n\n\n\n\n\n## Parquet: Row-wise VS columnar storage format\n\nIf you have a dataframe  like this\n\n```\n+----+-------+----------+\n| ID | Name  | Product  | \n+----+-------+----------+\n| 1  | name1 | product1 |\n| 2  | name2 | product2 |\n| 3  | name3 | product3 |\n+----+-------+----------+\n```\n\nIn **row-wise** storage format *records are contiguous* in the file:\n\n```python\n1 name1 product1 2 name2 product2 3 name3 product3 \n```\n\nWhile in the **columnar storage** format, *columns are stored together*:\n\n```python\n1 2 3 name1 name2 name3 product1 product2 product3\n```\n\n\n## `Parquet`: organization\n\n- This makes **columnar storage** more efficient when *querying a few columns* from the table\n\n- No need to read whole records, but only the *required columns*\n\n- A unique feature of `Parquet` is that even *nested fields* can be read individually without the need to read all the fields \n\n- `Parquet` uses **record shredding** and an **assembly algorithm** to store nested structures in a columnar fashion\n\n\n::: {.notes}\n\nExamples of nested fields \n\n:::\n\n\n\n\n---\n\n## `Parquet`: organization (continued) \n\n::: {.center}\n\n<img src=\"/images/parquet-format.gif\" style=\"width: 80%;\" />\n\n:::\n\n\n\n\n## `Parquet`: organization (lexikon)\n\nThe main entities in a `Parquet` file are the following:\n\nRow group\n: a horizontal partitioning of the data into rows. A row group consists of a column chunk for each column in the dataset\n\nColumn chunk\n: a chunk of the data for a particular column. These column chunks live in a particular row group and are guaranteed to be contiguous in the file\n\nPage\n: column chunks are divided up into pages written back to back. The pages share a common header and readers can skip the page they are not interested in\n\n\n\n## About `Parquet`\n\n::: {.center}\n\n<img src=\"/images/parquet-dive.png\" style=\"width: 80%;\" />\n\n:::\n\n\n## `Parquet`: headers and footers\n\n- The header just contains a magic number \"PAR1\" (4-byte) that identifies the file as `Parquet` format file\n\nThe footer contains:\n\n- **File metadata**: all the locations of all the column metadata start locations. Readers first read the file metadata to find the column chunks they need. Column chunks are then read sequentially. It also includes the format version, the schema, and any extra key-value pairs.\n\n- **length** of file metadata (4-byte)\n\n- **magic number** \"PAR1\" (4-byte)\n\n\n# `ORC` {background-color=\"#1c191c\"}\n\n## ORC: principles\n\n::: {.columns}\n\n::: {.column  width=\"60%\"}\n\n- `ORC` stands for *Optimized Row Columnar* file format. Created by Hortonworks in 2013 in order to speed up `Hive` {{< fa brands hive >}}\n\n- `ORC` file format provides a *highly efficient way to store data*\n\n- It is a *raw columnar data format* highly optimized for reading, writing, and processing data in `Hive`\n\n- It stores data in a *compact way* and enables *skipping quickly irrelevant parts*\n\n:::\n\n\n::: {.column width=\"40%\"}\n\n::: {.center}\n\n![](/images/orc.png)\n\n:::\n\n:::\n\n::: \n\n\n\n::: {.notes}\n\nA few words about `Hive`\n\n[Hive official site](https://hive.apache.org)\n\n:::\n\n\n## About `ORC`: organization\n\n- `ORC` stores *collections of rows in one file*. Within the collection, row data is stored in a *columnar format*\n\n- An `ORC` file contains **groups of row data** called *stripes*, along with auxiliary information in a file footer. \nAt the end of the file a postscript holds compression parameters and the size of the compressed footer\n\n- The default stripe size is 250 MB. **Large stripe** sizes enable *large, efficient reads from HDFS*\n\n- The **file footer** contains a list of stripes in the file, the number of rows per stripe, and each column’s data type. It also contains column-level aggregates count, min, max, and sum\n\n\n## About `ORC`  (onctinued)\n\n::: {.columns}\n\n::: {.column width=\"50%\"}\n\n- **Index data** include min and max values for each column and the row’s positions within each column\n\n- **`ORC` indexes** are used only for the selection of stripes and row groups and not for answering queries\n\n:::\n\n::: {.column width=\"50%\"}\n\n![ORC file structure](/images/orc-file-structure.png)\n\n:::\n\n::: \n\n<!-- end columns -->\n\n\n## About `ORC`\n\n`ORC` file format has many advantages such as:\n\n- `Hive` type support including `DateTime`, `decimal`, and the complex types (`struct`, `list`, `map` and `union`)\n\n- **Concurrent reads** of the same file\n\n- Ability to split files **without scanning for markers**\n\n- Estimate an **upper bound on heap memory allocation** based on the information in the file footer.\n\n\n# Comparison between formats {background-color=\"#1c191c\"}\n\n\n## `Avro` versus `Parquet`\n\n- `Avro` is a *row-based* storage format whereas `Parquet` is a *columnar based* storage format\n\n- `Parquet` is much better for *analytical querying* i.e. reads and querying are much more efficient than writing.\n\n- **Write operations** in `Avro` are better than in `Parquet`.\n\n- `Avro` is more mature than `Parquet` for *schema evolution*: `Parquet` supports only **schema append** while `Avro` supports more things, such as **adding or modifying columns**\n\n- `Parquet` is ideal for *querying a subset of columns* in a multi-column table. `Avro` is ideal for **operations where all the columns are needed** (such as in a ETL workflow)\n\n\n\n## `ORC` vs `Parquet`\n\n- `Parquet` is more capable of *storing nested data*\n\n- `ORC` is more capable of *predicate pushdown* (SQL queries on a data file are better optimized, chunks of data can be **skipped** directly while reading)\n\n- `ORC` is more *compression efficient*\n\n\n\n## In summary...\n\n::: {.center}\n\n<img src=\"/images/file-formats.png\" style=\"width: 80%;\" />\n\n:::\n\n\n\n# How to choose a file format {background-color=\"#1c191c\"}\n\n\n\n## R ead / write intensive & query pattern\n\n- **Row-based** file formats are overall better for storing write-intensive data because *appending new records is easier*\n\n- If only a **small subset of columns** is queried frequently, *columnar formats will be better* since  only those needed columns will be accessed and transmitted (whereas row formats need to pull all the columns)\n\n\n\n## C ompression\n\n- Compression is one of the key aspects to consider since *compression helps reduce the resources* required to store and transmit data\n\n- *Columnar formats are better than row-based formats in terms of compression* because **storing the same type of values together allows more efficient compression**\n\n- In columnar formats, **a different and efficient encoding is utilized for each column** \n\n- `ORC` has the best compression rate of all three, thanks to its **stripes**\n\n\n## S chema Evolution\n\n- One challenge in big data is the *frequent change of data schema*: e.g. **adding/dropping columns** and changing columns names\n\n- If you know that the **schema of the data will change** several times, *the best choice is `Avro`*\n\n- `Avro` data schema is in JSON and `Avro` is able to keep data compact even when many different schemas exist\n\nSee [Schema merging for parquet files](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#schema-merging)\n\n## N ested Columns\n\n- If you have a lot of *complex nested columns* in your dataset and often only query a **subset of columns or subcolumns**, *Parquet is the best choice*\n\n- Parquet allows to *access and retrieve subcolumns without pulling the rest* of the nested column\n\n\n## F ramework support\n\n- You have *consider the framework* you are using when choosing a data format\n\n- Data formats **perform differently** depending on where they are used\n\n- `ORC` works best with [`Hive` {{< fa brands hive >}}]() (it was designed for it)\n\n- `Spark` provides great support for processing `Parquet` formats.\n\n- `Avro` is often a good choice for [`Kafka`](https://kafka.apache.org) (streaming applications)\n\nBut... you can *use an try all formats with any framework*\n\n\n\n# Thank you !  {.unlisted background-color=\"#1c191c\"}\n\n",
    "supporting": [
      "slides06_file-formats_files"
    ],
    "filters": [],
    "includes": {}
  }
}