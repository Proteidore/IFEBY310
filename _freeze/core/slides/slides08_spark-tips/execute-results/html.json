{
  "hash": "6add3816cc33411318d4fd67a13ed9ab",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Spark tips\"\nengine: jupyter\ndate: \"2025-01-17\"\n--- \n\n\n# Spark tips  {background-color=\"#1c191c\"}\n\n\n\n\n##  Tip 1. Use DataFrames instead of RDDs\n\n- Instead of using the `RDD` API\n \n```{.python}\nrdd = sc.textFile(\"/path/to/file.txt\")\n```\n\n- Use the `DataFrame` API\n  \n```{.python}\ndf = spark.read.textFile(\"/path/to/file.txt\")\n```\n\n- The DataFrame API uses the * `Catalyst`* optimizer to **improve** the execution plan of your Spark Job\n\n- The low-level `RDD` API does not\n\n- Most of the **recent Spark advances** are towards an improvement of the `SQL`\n\n\n\n##  Tip 2. Avoid using regular expressions\n\n- Java `Regex` is great to parse data in an expected structure\n\n- But, unfortunately, it is generally a *slow process* when processing millions of rows\n\n- Increasing **a little bit** the parsing of rows *increases a lot* the entire job\n\n- If possible, *avoid using Regex’s* and try to load your data in a **more structured format**\n\n::: {.notes}\n\n\n[https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)\n\n[https://docs.python.org/fr/3/howto/regex.html](https://docs.python.org/fr/3/howto/regex.html)\n\n\n:::\n\n\n\n##  Tip 3. Joins: largest dataset on the left\n\n- When joining two datasets where **one is smaller than the other**, you **must** put the *largest on the left*\n\n\n```{.python}\njoinedDF = largeDF.join(smallDF, on=\"id\")\n```\n\n- The data specified *on the left* is **static on the executors** while the data *on the right* is **transfered** between the executors\n\n- Something like \n\n```{.python}\njoinedDF = smallDF.join(largeDF, on=\"id\")\n```\ncan be *much longer* or even *fail* if `largeDF` is large\n\n\n\n##  Tip 4. Joins: use broadcast joining\n\n\n- Often, we need to join a **huge** dataframe with a **small** one\n\n- Use *broadcast joins* for joining small datasets to larger ones\n\n```{.python}\nfrom pyspark.sql.functions import broadcast\n\njoinedDF = largeDF.join(broadcast(smallDF), on=\"id\")\n```\n\n- Usually leads to **much faster joins** since is allows to *avoid shuffles*\n\n\n\n##  Tip 5. Use caching when repeating queries\n\n- If you are constantly using the same DataFrame on multiple queries, you can  use *caching* or *persistence*:\n\n```{.python}\ndf = (\n  spark\n    .read\n    .textFile(\"/path/to/file.txt\")\n    .cache()\n)\n```\n\n- But *avoid overusing* this. Depending on caching strategy (in-memory then swap to disk), cache can **end up being slower** than reading\n\n- Storage space used for caching means **less space** for processing\n\n- Caching can **cost more** than reading the DataFrame (e.g. only few columns are useful, predictate pushdown)\n\n\n\n##  Tip 6. COMPUTE STATISTICS of tables\n\n- **Before querying** a table, it can be helpful to *compute the statistics* of those tables so that Catalyst can **find a better plan** to process it:\n\n```{.python}\nquery = \"ANALYZE TABLE db.table COMPUTE STATISTICS\"\nspark.sql(query)\n```\n\n- However, Spark **does not always get everything** it needs just from the above broad `COMPUTE STATISTICS` call\n\n\n\n##  Tip 6. COMPUTE STATISTICS of tables\n\n- Also helps to *check specific columns* so that **Catalyst** can better check those columns\n\n- It's recommended to **COMPUTE STATISTICS** for any *columns involved in filtering and joining* :\n\n```{.python}\nquery = \"ANALYZE TABLE db.table COMPUTE STATISTICS\"\n            \" FOR COLUMNS joinColumn, filterColumn\"\n\nspark.sql(query)\n```\n\n\n\n##  Tip 7. Shuffles: know your data\n\n- Shuffle is the *transportation of data between workers* across a Spark cluster's network\n\n- It's central for operations where a **reorganization of data is required**, referred to as *wide dependencies* (**wide** vs **narrow** dependencies)\n\n- This kind of operation *usually is the bottleneck* of your Spark application\n\n- To use Spark well, you *need to know what you shuffle*, and for this it’s **essential that you know your data**\n\n\n\n##  Tip 8. Shuffles: beware of skews\n\n- *Skew* is an *imbalance* in the **distribution of your data**\n\n- If you fail to account for **how your data is distributed**, you may find that Spark naively places an overwhelming *majority of rows on one executor*, and a *fraction on all the rest*\n\n- This is **skew**, and *it will kill your application*\n\n- Whether by causing **out of memory** errors, **network timeouts**, or **exponentially long running processes** that will never terminate\n\n\n##  Tip 9. Partitions: change the default  {.smaller}\n\n- It's **absolutely essential** to *model the number of partitions* around the kind of stuff you're solving\n\n- The default value for `spark.sql.shuffle.partitions` is 200. It controls the *number of partitions* used by *shuffles* (= number of partitions in the **resulting** DataFrame of RDD).\n\n- Number of shuffle partitions *does not change* with **different data size**. For **small** data, 200 is **overkill**, for **large** data, it does not **effectively use the all resources**.\n\n- **Rule of thumb**: set this configuration to the *number of cores* you have available *across all your executors*\n\n```{.python}\n(\n  spark\n    .conf\n    .set(\"spark.sql.shuffle.partitions\", 42)\n)\n```\n\n\n\n##  Tip 10. Partitions: well-distributed columns\n\n- A powerful way to **control Spark shuffles** is to *partition your data intelligently*\n\n- Partitioning on the *right column* (or set of columns) helps to *balance* the **amount of data mapped across the cluster network** in order to perform actions\n\n- Partitioning on a **unique ID** is generally a good strategy, but **don't partition** on **sparsely filled columns** (with many NAs) or columns that **over-represent particular values**\n\n\n\n##  Tip 11. Joins again: highly flammable\n\n- *Joins* are *shuffle offenders*. Dangers of SQL joining are amplified by the **scale** enabled by Spark\n\n- Even joining **medium sized data** can *cause an explosion* if there are *repeated join values* on both sides of your join\n\n- Million rows datasets with **\"pseudo unique\"** keys can *explode* into a *billions rows join*!\n\n- **Join columns** with *null values* usually means *massive skew* and an **explosive join**\n\n- A solution is to *pre-fill empty cells* to **arbitrary balanced values** (e.g. uniform random values) before running a join\n\n\n##  Tip 12. Is your data real yet?\n\n- Don't forget that operations in Spark are divided between *transformations* and *actions*. Transformations are **lazy** operations allowing Spark to **optimize your query**\n\n-  Transformations **set up** a DataFrame for changes (adding a column, joining it to another, etc.) but *will not execute these* until an **action** is performed.\n\n- This can result in **surprising results:** imagine that you create an id column using `monotonically_increasing_id`, and then join on that column. If you do not place an **action** in between, your values **have not been materialized**. The result will be **non-deterministic!**\n\n\n\n##  Tip 13. Checkpointing is your friend\n\n- *Checkpointing* means *saving data to disk* and *reloading it back in*, which is **redundant** anywhere else besides Spark.\n\n- It **triggers an action** on any waiting transformations, and **truncates** the Spark **query plan** for the checkpointed data.\n\n- This action **shows up in your Spark UI**, indicating **where you are in your job**.\n\n- It can help to **conserve resources**, since it can **release memory** that would otherwise be cached for downstream access. \n\n- Checkpointed data is also a valuable source for **data-debugging**.\n\n\n\n##  Tip 14. Check your runtime with monitoring\n\n- *Spark UI* is your friend, and so are other **monitoring tools** that let you know how your run is going in **real-time**.\n\n- The Spark UI contains information on the **job level**, the **stage level**, and the **executor level**. You can see if the **volume of data** going to each **partition** or each **executor** makes sense, if some part of your job is taking **too much time**.\n\n- Such a monitoring tool allowing to view your **total memory** and **CPU usage** across executors is essential for **resource planning** and \"autopsies\" on **failed jobs**.\n\n\n\n##  Tip 15. CSV reading is brittle\n\n- Naively reading CSVs in Spark can result in **silent** *escape-character errors*\n\n```{.python}\ndf = spark.read.csv(\"quote-happy.csv\")\n```\n\n- Your DataFrame **seems happy**: no runtime exceptions, and you can execute operations on the DataFrame\n\n- But after careful debugging, you realize that at some point in the data, **everything has shifted** over one or several columns!\n\n- To be safe, you can include `escape` and `quote` options in your reads. Even better: *use `Parquet`* instead of CSV files!\n\n\n\n\n##  Tip 16. Parquet is your friend\n\n- Read/Write operations are *order of magnitude more efficient* with `Parquet` than with uncompressed CSV files\n\n- Parquet is \"columnar\": **reads only** the columns required for a sql query and **skip over** those that are not requested. \n\n- And also *predicate pushdown* operations on filtering operations: run queries **only on relevant subsets** of the values.\n\n- Switching from CSV to Parquet is the *first thing you can do* to **improve performance**.\n\n- If you are generating Parquet files from another format (using `PyArrow`, `Pandas`, etc.) be conscious that creating a **single** parquet file gives up a *major benefit of the format*: you need to **partition it**!\n\n\n\n##  Tip 17. Problems with UDFs  {.smaller}\n\n*UDF* = User Defined Function = something **very convenient**\n\n```{.python}\n>>> from pyspark.sql import functions as F, types as T\n\n>>> data = [{'a': 1, 'b': 0}, {'a': 10, 'b': 3}]\n>>> df = spark.createDataFrame(data)\n\n>>> def calculate_a_b_ratio(a, b):\n>>>     if b > 0:\n>>>         return a / b\n>>>     return 0.\n\n>>> udf_ratio_calculation = F.udf(calculate_a_b_ratio, T.FloatType())\n\n>>> df = df.withColumn('a_b_ratio_float', udf_ratio_calculation('a', 'b'))\n>>> df.show()\n+---+---+---------------+\n|  a|  b|a_b_ratio_float|\n+---+---+---------------+\n|  1|  0|            0.0|\n| 10|  3|      3.3333333|\n+---+---+---------------+\n```\n\n\n\n##  Tip 17. Problems with UDFs\n\nUDF are *Excruciatingly slow* with `pyspark` and spark *won't complain* if the **return type is incorrect** and just return `nulls`\n\n```{.python}\n>>> udf_ratio_calculation = F.udf(calculate_a_b_ratio, T.DecimalType())\n>>> df = df.withColumn('a_b_ratio_dec', udf_ratio_calculation('a', 'b'))\n>>> df.show()\n+---+---+---------------+-------------+\n|  a|  b|a_b_ratio_float|a_b_ratio_dec|\n+---+---+---------------+-------------+\n|  1|  0|            0.0|         null|\n| 10|  3|      3.3333333|         null|\n+---+---+---------------+-------------+\n```\n\n```{.python}\n>>> udf_ratio_calculation = F.udf(calculate_a_b_ratio, T.BooleanType())\n>>> df = df.withColumn('a_b_ratio_bool', udf_ratio_calculation('a', 'b'))\n>>> df.show()\n+---+---+---------------+-------------+--------------+\n|  a|  b|a_b_ratio_float|a_b_ratio_dec|a_b_ratio_bool|\n+---+---+---------------+-------------+--------------+\n|  1|  0|            0.0|         null|          null|\n| 10|  3|      3.3333333|         null|          null|\n+---+---+---------------+-------------+--------------+\n```\n\n\n##  Tip 18. Use all of the resources\n\n- Spark **driver memory** and **executor memory** are set by default to 1 Go. \n\n- It is in general very useful to take a look at the *many configuration parameters* and their defaults:\n\nhttps://spark.apache.org/docs/latest/configuration.html\n\n- Many things there that can **influence your spark application**\n\n- When running **locally**, adjust `spark.driver.memory` to something that’s reasonable for your system, e.g. `\"8g\"`\n\n- When running on a **cluster**, you might also want to tweak the `spark.executor.memory` (though it depends on your cluster and its configuration).\n\n\n\n##  Tip 18. Use all of the resources\n\n```{.python}\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf()\nconf.set('spark.executor.memory', '16g')\nconf.set('spark.driver.memory', '8g')\n\nspark_session = SparkSession.builder \\\n        .config(conf=conf) \\\n        .appName('Name') \\\n        .getOrCreate()\n```\n\n\n\n# Interpret error messages  {background-color=\"#1c191c\"}\n\n\n\n## Interpret error messages \n\n- Error messages *don't mean what they say*\n\n- Takes quite a while to understand that Spark **complains about one thing**, when the problem is **somewhere else**\n\n- `\"Connection reset by peer\"` often means that you have skews and *one particular worker has run out of memory*\n\n- `\"java.net.SocketTimeoutException: Write timed out\"` can mean that the *number of partitions too high*, so that the filesystem is too slow to handle the **number of simultaneous writes** attempted by Spark\n\n\n\n## Interpret error messages \n\n- `\"Total size of serialized results[...] is bigger than spark.driver.maxResultSize\"` can mean that the *number of partitions is too high* and **results can't fit onto a particular worker**\n\n- `\"Column a is not a member of table b\"`: you have a *sql join error*. Try your **job locally on a small sample** to avoid reverse engineering of such errors\n\n- Sometimes you get a true `\"out of memory\"` error. You can **increase the size of individual workers**, but before you do that, ask yourself, *is the data well distributed* ?\n\n\n\n## Interpret error messages \n\n- `\"ClassNotFoundException\"`: usually when you are trying to **connect** your application to an external a database. Here is an example\n\n\n![](/images/classnotfounderror.png)\n\n\n## Interpret error messages  {.smaller}\n\n- Means that Spark **cannot find the necessary jar driver** to connect to the database\n  \n- Need to **provide the correct jars** to your application using the spark configuration or as a command line argument\n\n```{.python}\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\njars = \"/full/path/to/postgres.jar,/full/path/to/other/jar\"\nconf = SparkConf()\nconf.set(\"spark.jars\", jars)\n\nspark = (\n  SparkSession\n    .builder\n    .config(conf=conf)\n    .appName('test')\n    .getOrCreate()\n)\n```\n\nor\n\n```{.bash}\nspark-submit --jars /full/path/to/postgres.jar,/full/path/to/other/jar ...\n```\n\n\n\n## Interpret error messages \n\n- All the jars must be accessible to **all nodes** and not local to the driver.\n\n- This error might also mean a **Spark version mismatch** between the cluster components\n\n- Make sure there is **no space** between the commas in the list of jars.\n\n\n\n## Interpret error messages \n\nTrying to connect to a database: `\"java.sql.SQLException: No suitable driver\"`\n\n![](/images/sqlerror.png)\n\n\n\n## Interpret error messages \n\nError happens while trying to save to a database: `\"java.lang.NullPointerException\"`\n\n![](/images/nullpointer.png)\n\n\n## Interpret error messages \n\nThis errors usually mean that we forgot to set the driver, `\"org.postgresql.Driver\"` for `Postgres`:\n\n```{.python}\ndf = spark.read.format('jdbc').options(\n    url= 'db_url',\n    driver='org.postgresql.Driver',  # <-- here\n    dbtable='table_name',\n    user='user',\n    password='password'\n).load()\n```\n\nand also make sure that the drivers' jars are set.\n\n\n\n## Interpret error messages \n\nHorrible error : `'NoneType' object has no attribute '_jvm'`\n\n![](/images/nonetypeerror.png)\n\n...mainly comes from two mistakes\n\n\n\n## Interpret error messages \n\n1) You are using pyspark functions without having an active spark session\n\n```{.python}\nfrom pyspark.sql import SparkSession, functions as fn\n\nclass A(object):\n    def __init__(self):\n        self.calculations = fn.col('a') / fn.col('b')\n...\n# Instantiating A without an active spark session \n# will give you this error\na = A()\n```\n\n\n\n## Interpret error messages \n\n2) You are using pyspark functions within a UDF:\n\n```{.python}\n# Create a dataframe\ndata = [{'a': 1, 'b': 0}, {'a': 10, 'b': 3}]\ndf = spark.createDataFrame(data)\n\n# Define a simple function that returns a / b\ndef calculate_a_b_max(a, b):\n    return F.max([a, b])\n\n# and a udf for this function - notice the return datatype\nudf_max_calculation = F.udf(calculate_a_b_ratio, T.FloatType())\n\ndf = df.withColumn('a_b_max', udf_max_calculation('a', 'b'))\n\ndf.show()\n```\n\nWe CANNOT use `pyspark` functions inside a udf: a UDF operates on a row per row basis while pyspark functions on a column basis.\n\n\n\n\n# Thank you !  {.unlisted background-color=\"#1c191c\"}\n\n",
    "supporting": [
      "slides08_spark-tips_files"
    ],
    "filters": [],
    "includes": {}
  }
}