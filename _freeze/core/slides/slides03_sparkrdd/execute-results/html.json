{
  "hash": "3857d201ab0008e39d1bbbe5e5d21e5a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Apache and RDD\"\ndate: \"2025/01/17 (updated: 2025-01-18)\"\nengine: knitr\n---\n\n\n\n\n# Introduction  {background-color=\"#1c191c\"}\n\n\n\n## Principles\n\n`Spark` computing framework deals with many complex issues: fault tolerance, slow machines, big datasets, etc.\n\n. . .\n\nIt follows the next guideline\n\n*Here is an operation, run it on all the data.*\n\n::: {.callout-note}\n\n###\n\n- I do not care where it runs\n- Feel free to run it twice on different nodes\n\n:::\n\n. . .\n\n*Jobs* are divided in *tasks* that are executed by the *workers*\n\n::: {.callout-note}\n###\n\n- How do we deal with *failure*? Launch *another* task!  \n- How do we deal with *stragglers*? Launch *another task*! <br> ... and kill the original task\n\n:::\n\n\n\n\n## Job \n\nA *job* in Spark represents a complete computation triggered by an *action* in the application code.\n\nWhen you invoke an *action* (such as `collect()`, `saveAsTextFile()`, etc.) on a Spark RDD,\nDataFrame, or Dataset, it triggers the execution of one or more *jobs*.\n\n. . .\n\nEach *job* consists of one or more *stages*, where each stage represents a set of *tasks*\nthat can be executed in parallel.\n\n*Jobs* in Spark are created by *transformations* that have no dependency on each other,\nmeaning each *stage* can execute independently.\n\n\n## Task\n\nA *task* is the smallest unit of work in Spark and represents \nthe execution of a computation on a single *partition* of data.\n\n. . .\n\n*Tasks* are created for each *partition* of the RDD, DataFrame, or Dataset involved in the computation.\n\n. . .\n\nSpark's execution engine assigns *tasks* to individual *executor* nodes in the *cluster* for parallel execution.\n\n. . .\n\n*Tasks* are executed within the context of a specific *stage*, \nand each *task* typically operates on a subset of the data distributed across the *cluster*.\n\n. . .\n\nThe number of *tasks* within a *stage* depends on the number of *partitions* of the input data and the degree of parallelism configured for the `Spark` application.\n\n. . .\n\nIn summary, a *job* represents the entire computation triggered by an *action*, \ncomposed of one or more *stages*, \neach of which is divided into smaller units of work called *tasks*. \n\n. . .\n\n*Tasks* operate on individual *partitions* of the data in parallel to achieve efficient and scalable distributed computation in `Spark`.\n\n---\n\n## API\n\nAn *API* allows a user to interact with the software\n\n`Spark` is implemented in [Scala](https://www.scala-lang.org), runs on the *JVM* (Java Virtual Machine)\n\n. . .\n\n*Multiple* Application Programming Interfaces (APIs):\n\n- `Scala` (JVM)\n- `Java` (JVM)\n- {{< fa brands python >}} `Python`\n- {{< fa brands r-project >}} `R` \n\n. . .\n\n*This course uses primarily the `Python` API*. Easier to learn than `Scala` and `Java`\n\n::: {.callout-tip}\n\nAbout the `R` APIs:  See [Mastering Spark in R](https://therinspark.com)\n\n:::\n\n\n## Digression on acronym API (Application Programming Interface) \n\nSee [https://en.wikipedia.org/wiki/API](https://en.wikipedia.org/wiki/API) for more on this acronym\n\nIn `Python` language, look at `interface`  and corresponding chapter *Interfaces, Protocols and ABCs* in [Fluent Python](https://www.fluentpython.com)\n\n. . .\n\nFor `R` there are in fact two APIs, or two packages that offer a `Spark` API\n\n- [`sparklyr`](https://spark.rstudio.com)\n- [`SparkR`](https://spark.apache.org/docs/latest/sparkr.html)\n\nSee [Mastering `Spark` with `R` by Javier Luraschi, Kevin Kuo, Edgar Ruiz](https://therinspark.com/index.html)\n\n\n\n## Architecture\n\nWhen you interact with `Spark` through its API, you send instructions to the *Driver*\n\n\n- The *Driver* is the *central coordinator*\n- It communicates with distributed workers called *executors*\n- Creates a *logical directed acyclic graph* (DAG) of operations\n- *Merges operations* that can be merged\n- *Splits* the operations in *tasks* (smallest unit of work in Spark)\n- *Schedules* the tasks and send them to the *executors*\n- *Tracks* data and tasks\n\n#### Example\n\n- Example of DAG: `map(f) - map(g) - filter(h) - reduce(l)`\n- `map(f o g)`\n\n\n\n\n# SparkSession and SparkContext {background-color=\"#1c191c\"}\n\n## `SparkContext` versus `SparkSession`\n\n`SparkContext` and `SparkSession` serve different purposes\n\n. . .\n\n`SparkContext` was the main entry point for Spark applications in first versions of Apache Spark.\n\n`SparkContext` represented the connection to a Spark *cluster*, allowing the application to interact with the *cluster manager*.\n\n`SparkContext` was responsible for coordinating and managing the execution of *jobs* and *tasks*.\n\n`SparkContext` provided APIs for creating `RDDs` (Resilient Distributed Datasets), which were the primary abstraction in Spark for representing distributed data.\n\n\n\n## SparkContext object\n\nYour `python` session interacts with the *driver* through a `SparkContext` object \n\n\n- In the `Spark` interactive shell <br> An object of class `SparkContext` is automatically created in the session and named `sc`\n\n- In a `jupyter notebook` <br> Create a `SparkContext` object using:\n\n```{.python}\n>>> from pyspark import SparkConf, SparkContext\n\n>>> conf = (\n  SparkConf()\n  .setAppName(appName)\n  .setMaster(master)\n)\n>>> sc = SparkContext(conf=conf)\n```\n\n## SparkSession\n\nIn Spark 2.0 and later versions, `SparkContext` is still available \nbut is not the primary entry point. \n\nInstead, `SparkSession` is preferred. \n\n`SparkSession` was introduced in Spark 2.0 as a higher-level abstraction that encapsulates `SparkContext`, `SQLContext`, and `HiveContext`.\n\n`SparkSession` provides a unified entry point for Spark functionality, integrating \nStructured APIs:\n\n- `SQL`, \n- `DataFrame`, \n- `Dataset`  \n \nand the traditional RDD-based APIs.\n\n## \n\n`SparkSession` is designed to make it easier to work with structured data (like data stored in tables or files with a schema) using Spark's DataFrame and Dataset APIs.\n\n. . .\n\n`SparkSession` also provides built-in support for reading data from various sources (like Parquet, JSON, JDBC, etc.) into DataFrames and writing DataFrames back to different formats.\n\n. . .\n\nAdditionally, `SparkSession` simplifies the configuration of Spark properties and provides a Spark SQL CLI and a Spark Shell with SQL and DataFrame support.\n\n. . .\n\n::: {.callout-note}\n\n###\n\n`SparkSession` internally creates and manages a `SparkContext`, so when you create a `SparkSession`, you don't need to create a `SparkContext` separately.\n\n:::\n\n## {{< fa bullhorn >}}\n\n`SparkContext` is lower-level and primarily focused on managing the execution of Spark *jobs* and interacting with the *cluster* \n\n`SparkSession` provides a higher-level, more user-friendly interface for working with structured data and integrates various Spark functionalities, including SQL, DataFrame, and Dataset APIs.\n\n\n## RDDs and running model\n\nSpark programs are written in terms of operations on *RDDs*\n\n- *RDD* stands for *Resilient Distributed Dataset* <br>\n\n-  An *immutable distributed collection* of objects spread across the cluster disks or memory\n\n- *RDDs* can contain any type of Python, Java, or Scala objects, including user-defined classes\n\n- Parallel *transformations* and *actions* can be applied to RDDs\n\n- *RDDs* are automatically rebuilt on machine failure\n\n\n\n## Creating a RDD\n\nFrom an iterable object `iterator` (e.g. a Python `list`, etc.):\n\n```{.python}\nlines = sc.parallelize(iterator)\n```\n\nFrom a text file:\n\n```{.python}\nlines = sc.textFile(\"/path/to/file.txt\")\n```\n\nwhere `lines` is the resulting RDD, and `sc` the spark context\n\n\n::: {.callout-note}\n\n### Remarks\n\n- `parallelize` not really used in practice\n- In real life: *load data from external storage*\n- External storage is often *HDFS* (Hadoop Distributed File System)\n- Can read most formats (`json`, `csv`, `xml`, `parquet`, `orc`, etc.)\n\n:::\n\n\n## {{< fa hand-point-right >}}\n\nFor iterators look again at [Fluent Python](https://www.fluentpython.com), chapter 17 *Iterators, Generators, and Classic Coroutines*\n\n\n\n\n## Operations on RDD\n\n*Two families of operations* can be performed on RDDs\n\n. . .\n\n- *Transformations* <br> Operations on RDDs which return a new RDD <br> *Lazy evaluation*\n\n. . .\n\n- *Actions* <br> Operations on RDDs that return some other data type <br> *Triggers computations*\n\n. . .\n\n{{< fa brain >}} What is *lazy evaluation* ? \n\n## {{< fa hand-point-right >}}\n\nWhen a *transformation* is called on a RDD:\n\n- The operation is *not immediately performed*\n- Spark internally *records that this operation has been requested*\n- Computations are triggered only *if an action requires the result of this transformation* at some point\n\n\n\n\n# Transformations {background-color=\"#1c191c\"}\n\n\n\n## Transformations\n\nThe most important transformation is `map`\n\n| transformation | description                                     |\n|:-------------:|:-----------------------------------------------|\n| `map(f)`       | apply a function `f` to each element of the RDD |\n\n. . .\n\nHere is an example:\n\n```{.python}\n>>> rdd = sc.parallelize([2, 3, 4])\n>>> rdd.map(lambda x: list(range(1, x))).collect()\n[[1], [1, 2], [1, 2, 3]]\n```\n\n. . .\n\n- We need to call `collect` (an *action*) otherwise *nothing happens*\n- Once again, transformation `map` is lazy-evaluated  `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 512 512\" style=\"height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M256 32c14.2 0 27.3 7.5 34.5 19.8l216 368c7.3 12.4 7.3 27.7 .2 40.1S486.3 480 472 480H40c-14.3 0-27.6-7.7-34.7-20.1s-7-27.8 .2-40.1l216-368C228.7 39.5 241.8 32 256 32zm0 128c-13.3 0-24 10.7-24 24V296c0 13.3 10.7 24 24 24s24-10.7 24-24V184c0-13.3-10.7-24-24-24zm32 224a32 32 0 1 0 -64 0 32 32 0 1 0 64 0z\"/></svg>`{=html}\n\n. . .\n\n- In `Python`, *three options for passing functions* into `Spark`\n  - for short functions: `lambda` expressions (anonymous functions)\n  - top-level functions \n  - *locally/user defined functions* with `def`\n\n\n\n## Transformations\n\nAbout passing functions to `map`:\n\n- Involves *serialization* with `pickle`\n- `Spark` sends the *entire pickled function* to worker nodes\n\n\n::: {.callout-warning}\n\n### Warning \n\nIf the function is an *object method*:\n\n- The *whole object is pickled* since the method contains references to the object (`self`) and references to attributes of the object\n- The whole object can be *large* \n- The whole object *may not be serializable with `pickle`*\n\n:::\n\n::: aside\n\n[Let's go to notebook05_sparkrdd.ipynb](http://localhost:8888/notebooks/notebooks/notebook05_sparkrdd.ipynb)\n\n:::\n\n\n\n## {{< fa hand-point-right >}} Serialization\n\n> Converting an object from its in-memory structure to a binary or text-oriented format for storage or transmission, in a way that allows the future reconstruction of a clone of the object on the same system or on a different one. \n\n. . .\n\n> The `pickle` module supports serialization of arbitrary `Python` objects to a binary format\n\n\nfrom [Fluent Python]() by Ramalho\n\n\n## Transformations (continued)\n\nThen we have `flatMap`\n\n\n| transformation | description                      |\n|:------------:|:-------------------------------|\n| `flatMap(f)`   | apply `f` to each element of the RDD, then flattens the results |\n\n\n. . .\n\n::: {.callout-note}\n\n### Example\n\n```{.python}\n>>> rdd = sc.parallelize([2, 3, 4, 5])\n>>> rdd.flatMap(lambda x: range(1, x)).collect()\n[1, 1, 2, 1, 2, 3, 1, 2, 3, 4]\n```\n\n:::\n\n\n## Transformations (continued)\n\n`filter` allows to filter an RDD\n\n\n| transformation | description                      |\n|:-------------:|:------------------------------|\n| `filter(f)`    | Return an RDD consisting of only elements that pass the condition `f` passed to `filter()` |\n\n\n. . .\n\n::: {.callout-note}\n\n### Example\n\n```{.python}\n>>> rdd = sc.parallelize(range(10))\n>>> rdd.filter(lambda x: x % 2 == 0).collect()\n[0, 2, 4, 6, 8]\n```\n\n:::\n\n## Transformations\n\nAbout `distinct` and `sample`\n\n\n| transformation | description                      |\n|:-------------:|:-------------------------------|\n| `distinct()`  | Removes duplicates |\n| `sample(withReplacement, fraction, [seed])`  | Sample an RDD, with or without replacement |\n\n\n. . .\n\n::: {.callout-note}\n\n### Example\n\n```{.python}\n>>> rdd = sc.parallelize([1, 1, 4, 2, 1, 3, 3])\n>>> rdd.distinct().collect()\n[1, 2, 3, 4]\n```\n\n:::\n\n## Transformations\n\nWe have also pseudo-set-theoretical operations\n\n\n| transformation  | description                      |\n|:-------------:|:-------------------------------|\n| `union(otherRdd)`  | Returns union with `otherRdd` |\n| `instersection(otherRdd)`  | Returns intersection with `otherRdd` |\n| `subtract(otherRdd)`  | Return each value in `self` that is not contained in `otherRdd`. |\n\n. . .\n\n::: {.callout-note}\n\n### \n\n- If there are duplicates in the input RDD, the result of `union()` *will* contain duplicates (fixed with `distinct()`)\n- `intersection()` removes all duplicates (including duplicates from a single RDD)\n- Performance of `intersection()` is much worse than `union()` since it requires a *shuffle* to identify common elements\n- `subtract` also requires a *shuffle*\n\n:::\n\n## Transformations\n\nWe have also pseudo-set-theoretical operations\n\n\n| transformation | description                      |\n|:-------------:|:-------------------------------|\n| `union(otherRdd)`  | Returns union with `otherRdd` |\n| `instersection(otherRdd)`  | Returns intersection with `otherRdd` |\n| `subtract(otherRdd)`  | Return each value in `self` that is not contained in `otherRdd`. |\n\n\n. . .\n\n::: {.callout-note}\n\n### Example with `union` and `distinct`\n\n```{.python}\n>>> rdd1 = sc.parallelize(range(5))\n>>> rdd2 = sc.parallelize(range(3, 9))\n>>> rdd3 = rdd1.union(rdd2)\n>>> rdd3.collect()\n[0, 1, 2, 3, 4, 3, 4, 5, 6, 7, 8]\n```\n\n```{.python}\n>>> rdd3.distinct().collect()\n[0, 1, 2, 3, 4, 5, 6, 7, 8]\n```\n\n:::\n\n## About shuffles \n\n- Certain operations trigger a *shuffle*\n- It is `Spark`’s mechanism for *redistributing data* so as tp modify the partitioning\n- It involves *moving data across executors and machines*, making  *shuffle* a complex and costly operation\n- More on *shuffles*  later\n\n## Performance Impact  \n\n- A *shuffle* involves \n  - disk I/O, \n  - data serialization \n  - network I/O. \n\n. . .\n\n- To organize data for the shuffle, `Spark` generates sets of *tasks*:\n  - *map tasks* to organize the data and\n  - *reduce tasks* to aggregate it. \n\n::: aside\n\nThis nomenclature comes from MapReduce and does not directly relate to Spark’s map and reduce operations.\n\n:::\n\n## Transformations\n\nAnother \"pseudo set\" operation\n\n\n| transformation | description                      |\n|:-------------:|:-------------------------------|\n| `cartesian(otherRdd)`  | Return the Cartesian product of this RDD and another one |\n\n\n. . .\n\n::: {.callout-note}\n\n### Example\n\n```{.python}\n>>> rdd1 = sc.parallelize([1, 2])\n>>> rdd2 = sc.parallelize([\"a\", \"b\"])\n>>> rdd1.cartesian(rdd2).collect()\n[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]\n```\n\n:::\n\n{{< fa hand-point-right >}} `cartesian()` is **very expensive** for large RDDs\n\n::: aside\n\n[Let's go to notebook05_sparkrdd.ipynb](http://localhost:8888/notebooks/notebooks/notebook05_sparkrdd.ipynb)\n\n:::\n\n# Actions  {background-color=\"#1c191c\"}\n\n\n## Actions\n\n`collect` brings the `RDD` back to the driver\n\n| transformation | description                      |\n|:-------------:|:-------------------------------|\n| `collect()`    | Return all elements from the RDD |\n\n\n::: {.callout-note}\n\n### Example\n\n```{.python}\n>>> rdd = sc.parallelize([1, 2, 3, 3])\n>>> rdd.collect()\n[1, 2, 3, 3]\n```\n\n:::\n\n::: {.callout-note}\n\n- {{< fa triangle-exclamation >}} Be sure that the *retrieved data fits in the driver memory* !\n- Useful when developping and working on small data for testing\n- {{< fa hand-point-right >}} We'll use it a lot here, but *we don't use it in real-world problems*\n\n:::\n\n\n## Actions\n\nIt's important to count!\n\n\n| transformation | description                      |\n|:-------------:|:-------------------------------|\n| `count()`      | Return the number of elements in the RDD |\n| `countByValue()` | Return the count of each unique value in the RDD as a dictionary of `{value: count}` pairs. |\n\n\n::: {.callout-note}\n\n### Example\n\n```{.python}\n>>> rdd = sc.parallelize([1, 3, 1, 2, 2, 2])\n>>> rdd.count()\n6\n```\n\n```{.python}\n>>> rdd.countByValue()\ndefaultdict(int, {1: 2, 3: 1, 2: 3})\n```\n\n:::\n\n\n\n## Actions: cherry-picking\n\nHow to get some (but not all) values in an RDD ?\n\n\n| action         | description                      |\n|:-------------:|:-------------------------------|\n| `take(n)`      | Return `n` elements from the RDD (deterministic)|\n| `top(n)`       | Return first `n` elements from the RDD (descending order)|\n| `takeOrdered(num, key=None)`    | Get the N elements from a RDD ordered in ascending order or as specified by the optional key function.|\n\n. . .\n\n::: {.callout-note}\n\n- `take(n)` returns n elements from the RDD and attempts to **minimize the number of partitions it accesses**\n- {{< fa triangle-exclamation >}} the result  may be a *biased* collection\n- `collect` and `take` may return the elements in an order you  don't expect\n\n:::\n\n\n## Actions\n\nHow to get some values in an RDD ?\n\n\n\n| action         | description                      |\n|:-------------:|:-------------------------------|\n| `take(n)`      | Return `n` elements from the RDD (deterministic)|\n| `top(n)`       | Return first `n` elements from the RDD (decending order)|\n| `takeOrdered(num, key=None)`  | Get the $N $elements from a RDD ordered in ascending order or as specified by the optional key function.|\n\n. . .\n\n::: {.callout-note }\n\n### Example \n\n```{.python}\n>>> rdd = sc.parallelize([(3, 'a'), (1, 'b'), (2, 'd')])\n>>> rdd.takeOrdered(2)\n[(1, 'b'), (2, 'd')]\n```\n\n```{.python}\n>>> rdd.takeOrdered(2, key=lambda x: x[1])\n[(3, 'a'), (1, 'b')]\n```\n\n:::\n\n\n\n## Actions: reduction(s)\n\n::: {#reductions-preamble}\n\n| action         | description                      |\n|:-------------:|:-------------------------------|\n| `reduce(f)`    | Reduces the elements of this RDD using the specified commutative and associative binary operator `f`. |\n| `fold(zeroValue, op)`    | Same as `reduce()` but with the provided zero value. |\n\n:::\n\n\n\n{{< contents reductions-preamble >}}\n\n\n\n\n. . .\n\n- `op(x, y)` is allowed to modify x and return it as its result value to avoid object allocation; however, it should not modify y.\n- `reduce` applies some operation to pairs of elements until there is just one left. Throws an exception for empty collections.\n- `fold` has initial zero-value: defined for empty collections.\n\n\n## Actions: reduction(s)\n\n\n\n{{< contents reductions-preamble >}}\n\n\n\n\n. . .\n\n::: {.callout-note}\n\n### Example\n\n```{.python}\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.reduce(lambda a, b: a + b)\n6\n```\n\n```{.python}\n>>> rdd.fold(0, lambda a, b: a + b)\n6\n```\n\n:::\n\n## Actions: reduction(s)\n\n\n\n{{< contents reductions-preamble >}}\n\n\n\n\n. . .\n\n::: {.callout-warning}\n\n### Warning \n\nWith `fold`,  solutions can depend on the number of partitions\n\n```{.python}\n>>> rdd = sc.parallelize([1, 2, 4], 2) # RDD with 2 partitions\n>>> rdd.fold(2.5, lambda a, b: a + b)\n14.5\n```\n\n- RDD has 2 partition: say [1, 2] and [4] \n- Sum in the partitions: 2.5 + (1 + 2) = 5.5  and  2.5 + (4) = 6.5\n- Sum over partitions: 2.5 + (5.5 + 6.5) = 14.5\n\n:::\n\n## Actions: reduction(s)\n\n\n\n{{< contents reductions-preamble >}}\n\n\n\n\n. . .\n\n\n::: {.callout-warning}\n\n### Warning \n\nSolutions can depend on the number of partitions\n\n```{.python}\n>>> rdd = sc.parallelize([1, 2, 3], 5) # RDD with 5 partitions\n>>> rdd.fold(2, lambda a, b: a + b)\n```\n\n:::\n\n::: {.callout-note}\n\n[Let's go to notebook05_sparkrdd.ipynb](http://localhost:8888/notebooks/notebooks/notebook05_sparkrdd.ipynb)]\n\n:::\n\n\n## Actions: reduction(s)\n\n\n\n{{< contents reductions-preamble >}}\n\n\n\n\n. . .\n\n::: {.callout-warning}\n\n### Warning \n\nSolutions can depend on the number of partitions\n\n```{.python}\n>>> rdd = sc.parallelize([1, 2, 3], 5) # RDD with 5 partitions\n>>> rdd.fold(2, lambda a, b: a + b)\n18\n```\n\n- Yes, even if there is less partitions than elements !\n- 18 = 2 * 5 + (1+2+3) + 2\n\n:::\n\n\n## Actions : aggregate\n\n::: {#action-aggregate}\n\n\n| action         | description                      |\n|:-------------:|:-------------------------------|\n| `aggregate(zero, seqOp, combOp)` | Similar to `reduce()` but used to return a different type. |\n\n:::\n\n\n\n{{< contents action-aggregate >}}\n\n\n\n\n. . .\n\nAggregates the elements of each partition, and then the results for all the partitions, given aggregation functions and zero value.\n\n- `seqOp(acc, val)`: function to combine the elements of a partition from the RDD (`val`) with an accumulator (`acc`). <br> \nIt can return a different result type than the type of this `RDD`\n- `combOp`: function that merges the accumulators of two partitions\n- In both functions, the first argument can be modified while the second cannot\n\n## Actions : aggregate\n\n\n\n{{< contents action-aggregate >}}\n\n\n\n\n. . .\n\n::: {.callout-note}\n\n### Example\n\n\n```{.python}\n>>> seqOp = lambda x, y: (x[0] + y, x[1] + 1)\n>>> combOp = lambda x, y: (x[0] + y[0], x[1] + y[1])\n>>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n(10, 4)\n```\n\n```{.python}\n>>> ( \n      sc.parallelize([])\n        .aggregate((0, 0), seqOp, combOp)\n)\n(0, 0)\n```\n:::\n\n\n::: aside\n\n[Let's go to notebook05_sparkrdd.ipynb]](http://localhost:8888/notebooks/notebooks/notebook05_sparkrdd.ipynb)]\n\n:::\n\n\n## Actions\n\nThe `foreach` action\n\n\n| action         | description                      |\n|:-------------:|:-------------------------------|\n| `foreach(f)` | Apply a function `f` to each element of a RDD |\n\n. . .\n\n\n- Performs an action on all of the elements in the RDD without returning any result to the driver.\n\n- Example : insert records into a database with `f`\n\n. . .\n\n{{< fa hand-point-right >}} The `foreach()` action lets us perform computations on each element in the RDD without bringing it back locally\n\n\n\n\n# Persistence  {background-color=\"#1c191c\"}\n\n\n\n## Lazy evaluation and persistence\n\n- Spark RDDs are *lazily evaluated*\n\n- Each time an action is called on a RDD, this RDD and all its dependencies are *recomputed*\n\n- If you plan to reuse a RDD multiple times, you should use *persistence*\n\n::: {.callout-note}\n\n- Lazy evaluation helps `spark` to **reduce the number of passes** over the data it has to make by grouping operations together\n- No substantial benefit to writing a single complex map instead of chaining together many simple operations\n- Users are free to organize their program into **smaller**, more **manageable operations**\n\n:::\n\n## Persistence\n\nHow to use persistence ?\n\n| method                       | description                                  |\n|:---------------------------:|:--------------------------------------------|\n| `cache()`                    | Persist the RDD in memory                    |\n| `persist(storageLevel)`      | Persist the RDD according to `storageLevel`  |\n\n. . .\n\n{{< fa hand-point-right >}} These methods must be called *before* the action, and do not trigger the computation\n\n\n## Usage of `storageLevel`\n\n```{.python}\npyspark.StorageLevel(\n  useDisk, useMemory, useOffHeap, deserialized, replication=1\n)\n```\n\n## Shades of persistence {.incremental}\n\n- What does persistence *in memory* mean?\n- Make `storageLevel` explicit\n- Any difference between `cache()` and `persist()` with `useMemory`?\n- Why do we call persistence caching? \n\n\n## Options for persistence\n\n::: {#options-persistence}\n\nOptions for persistence\n\n| argument        | description                      |\n|:-------------:|:-------------------------------|\n| `useDisk`      | Allow caching to use disk if `True`  |\n| `useMemory`    | Allow caching to use memory if `True`  |\n| `useOffHeap`   | Store data outside of JVM heap if `True`. Useful if using some in-memory storage system (such a `Tachyon`) |\n| `deserialized` | Cache data without serialization if `True` |\n| `replication`  | Number of replications of the cached data  |\n\n:::\n\n\n\n{{< contents options-persistence >}}\n\n\n\n\n`replication`:\nIf you cache data that is quite slow to be recomputed, you can use replications. If a machine fails, data will not have to be recomputed.\n\n\n\n## Options for persistence\n\n\n\n{{< contents options-persistence >}}\n\n\n\n\n`deserialized`\n:\n\n- Serialization is conversion of the data to a binary format\n- To the best of our knowledge, `PySpark` only support serialized caching (using `pickle`) \n\n\n\n## Options for persistence\n\n\n\n{{< contents options-persistence >}}\n\n\n\n\n\n`useOffHeap`\n: \n\n- Data cached in the JVM heap by default\n- Very interesting alternative in-memory solutions such as `tachyon`\n- Don't forget that `spark` is `scala` running on the JVM\n\n\n\n## Back to options for persistence  {.smaller}\n\n```{.python}\nStorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication)\n```\n\nYou can use these constants:\n```{.python}\nDISK_ONLY = StorageLevel(True, False, False, False, 1)\nDISK_ONLY_2 = StorageLevel(True, False, False, False, 2)\nMEMORY_AND_DISK = StorageLevel(True, True, False, True, 1)\nMEMORY_AND_DISK_2 = StorageLevel(True, True, False, True, 2)\nMEMORY_AND_DISK_SER = StorageLevel(True, True, False, False, 1)\nMEMORY_AND_DISK_SER_2 = StorageLevel(True, True, False, False, 2)\nMEMORY_ONLY = StorageLevel(False, True, False, True, 1)\nMEMORY_ONLY_2 = StorageLevel(False, True, False, True, 2)\nMEMORY_ONLY_SER = StorageLevel(False, True, False, False, 1)\nMEMORY_ONLY_SER_2 = StorageLevel(False, True, False, False, 2)\nOFF_HEAP = StorageLevel(False, False, True, False, 1)\n```\nand simply call\nfor instance\n\n```{.python}\nrdd.persist(MEMORY_AND_DISK)\n```\n\n\n\n## Persistence\n\nWhat if you attempt to *cache too much data to fit in memory ?*\n\nSpark will automatically evict old partitions using a *Least Recently Used* (LRU) cache policy:\n\n- For the *memory-only* storage levels, it will recompute these partitions the next time they are accessed\n\n- For the *memory-and-disk* ones, it will write them out to disk\n\nUse `unpersist()` to RDDs to **manually remove them** from the cache\n\n\n\n## Reminder: about passing functions {{< fa syringe >}}\n\n\n::: {.callout-warning}\n\n### Warning \n\nWhen passing functions, you can *inadvertently serialize the object containing the function*. \n\n:::\n\nIf you pass a function that:\n\n- is the member of an object (a method)\n- contains references to fields in an object\n\nthen `Spark` sends the *entire object to worker nodes*, which can be *much larger* than the bit of information you need\n\n::: {.callout-caution}\n\n### Caution\n\nThis can cause your *program to fail*, if your class contains objects that *Python can't pickle*\n\n:::\n\n\n## About passing functions\n\nPassing a function with field references (don’t do this !  {{< fa hammer >}} {{< fa skull-crossbones >}})\n\n```{.python}\nclass SearchFunctions(object):\n  \n  def __init__(self, query):\n      self.query = query\n\n  def isMatch(self, s):\n      return self.query in s\n\n  def getMatchesFunctionReference(self, rdd):\n      # Problem: references all of \"self\" in \"self.isMatch\"\n      return rdd.filter(self.isMatch)\n\n  def getMatchesMemberReference(self, rdd):\n      # Problem: references all of \"self\" in \"self.query\"\n      return rdd.filter(lambda x: self.query in x)\n```\n\n::: {.callout-tip}\n\n### Tip\n\nInstead, *just extract the fields you need* from your object into a local variable and pass that in\n\n:::\n\n\n\n## About passing functions\n\n`Python` function passing without field references\n\n```{.python}\nclass WordFunctions(object):\n  ...\n\ndef getMatchesNoReference(self, rdd):\n  # Safe: extract only the field we need into a local variable\n  query = self.query\n  return rdd.filter(lambda x: query in x)\n```\n\n. . .\n\nMuch better to do this instead  {{< fa champagne-glasses >}}\n\n\n\n# Pair RDD: key-value pairs   {background-color=\"#1c191c\"}\n\n\n## Pair RDD: key-value pairs\n\nIt's roughly a RDD where each element is a *tuple* with two elements: a *key* and a *value*\n\n. . .\n\n- For numerous tasks, such as aggregations tasks, storing information as `(key, value)` pairs into RDD is very convenient\n- Such RDDs are called `PairRDD`\n- Pair RDDs expose *new operations* such as *grouping together* data with the same *key*, and *grouping together two different RDDs*\n\n## Creating a pair RDD\n\nCalling `map` with a function returning a `tuple` with two elements\n\n```{.python}\n>>> rdd = sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\n>>> rdd = rdd.map(lambda x: (x[0], x[1:]))\n>>> rdd.collect()\n[(1, ['a', 7]), (2, ['b', 13]), (2, ['c', 17])]\n```\n\n\n## {{< fa triangle-exclamation >}} Warning\n\nAll elements of a `PairRDD` must be tuples with two elements (the key and the value)\n\n```{.python}\n>>> rdd = sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\n>>> rdd.keys().collect()\n[1, 2, 2]\n>>> rdd.values().collect()\n['a', 'b', 'c']\n```\n\n. . .\n\nFor things to work as expected you *must* do\n\n```{.python}\n>>> rdd = sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\\\n      .map(lambda x: (x[0], x[1:]))\n>>> rdd.keys().collect()\n[1, 2, 2]\n>>> rdd.values().collect()\n[['a', 7], ['b', 13], ['c', 17]]\n```\n\n\n\n\n\n\n## Transformations for a single `PairRDD`\n\n::: {#transformations-for-a-single-PairRDD}\n\n| transformation | description                      |\n|:-------------:|:-------------------------------|\n| `keys()`       | Return an RDD containing the keys |\n| `values()`     | Return an RDD containing the values |\n| `sortByKey()`  | Return an RDD sorted by the key |\n| `mapValues(f)`  | Apply a function `f` to each value of a pair RDD without changing the key |\n| `flatMapValues(f)` | Pass each value in the key-value pair RDD through a flatMap function `f` without changing the keys |\n\n:::\n\n\n\n{{< contents transformations-for-a-single-PairRDD >}}\n\n\n\n\n## Transformations for a single `PairRDD`\n\n\n\n\n{{< contents transformations-for-a-single-PairRDD >}}\n\n\n\n\n. . .\n\nExample with `mapValues`\n\n```{.python}\n>>> rdd = sc.parallelize([(\"a\", \"x y z\"), (\"b\", \"p r\")])\n>>> rdd.mapValues(lambda v: v.split(' ')).collect()\n[('a', ['x', 'y', 'z']), ('b', ['p', 'r'])]\n```\n\n## Transformations for a single `PairRDD`\n\n\n\n{{< contents transformations-for-a-single-PairRDD >}}\n\n\n\n\n. . .\n\nExample with `flatMapValues`\n\n```{.python}\n>>> texts = sc.parallelize([(\"a\", \"x y z\"), (\"b\", \"p r\")])\n>>> tokenize = lambda x: x.split(\" \")\n>>> texts.flatMapValues(tokenize).collect()\n[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n```\n\n\n\n\n## Transformations for a single `PairRDD` (keyed)\n\n::: {#transformations-for-a-single-PairRDD-keyed}\n\n| transformation | description                      |\n|:-------------:|:-------------------------------|\n| `groupByKey()`  | Group values with the same key  |\n| `reduceByKey(f)`| Merge the values for each key using an associative reduce function `f`. |\n| `foldByKey(f)`  | Merge the values for each key using an associative reduce function `f`. |\n| `combineByKey(createCombiner, mergeValue, mergeCombiners, [partitioner])` | Generic function to combine the elements for each key using a custom set of aggregation functions. |\n\n:::\n\n\n\n{{< contents transformations-for-a-single-PairRDD-keyed >}}\n\n\n\n\n## Transformations for a single `PairRDD` (keyed)\n\n\n\n\n{{< contents transformations-for-a-single-PairRDD-keyed >}}\n\n\n\n\n. . .\n\nExample with `groupByKey`\n\n```{.python}\n>>> rdd = sc.parallelize([\n        (\"a\", 1), (\"b\", 1), (\"a\", 1), \n        (\"b\", 3), (\"c\", 42)\n        ])\n>>> rdd.groupByKey().mapValues(list).collect()\n[('c', [42]), ('b', [1, 3]), ('a', [1, 1])]\n```\n\n##\n\n<center>\n![](/images/group_by.png) \n</center>\n\n## Transformations for a single `PairRDD` (keyed)\n\n\n\n{{< contents transformations-for-a-single-PairRDD-keyed >}}\n\n\n\n\n. . .\n\nExample with `reduceByKey`\n\n```{.python}\n>>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n>>> rdd.reduceByKey(lambda a, b: a + b).collect()\n[('a', 2), ('b', 1)]\n```\n\n- The reducing occurs first **locally** (within partitions)\n- Then, a shuffle is performed with the local results to reduce globally\n\n##\n\n<center>\n![](/images/reduce_by.png)\n</center>\n\n## Transformations for a single `PairRDD` (keyed)\n\n\n\n\n{{< contents transformations-for-a-single-PairRDD-keyed >}}\n\n\n\n\n. . .\n\n`combineByKey` Transforms an `RDD[(K, V)]` into another RDD of type `RDD[(K, C)]` for a *combined* type `C` that can be different from `V`\n\n. . .\n\nThe user must define\n\n- `createCombiner` : which turns a `V` into a `C`\n- `mergeValue` : to merge a `V` into a `C`\n- `mergeCombiners` : to combine two `C`’s into a single one\n\n## Transformations for a single `PairRDD` (keyed)\n\n\n\n{{< contents transformations-for-a-single-PairRDD-keyed >}}\n\n\n\n\n. . .\n\nIn this example\n\n- `createCombiner` : converts the value to `str`\n- `mergeValue` : concatenates two `str`\n- `mergeCombiners` : concatenates two `str`\n\n\n\n```{.python}\n>>> rdd = sc.parallelize([('a', 1), ('b', 2), ('a', 13)])\n>>> def add(a, b):\n        return a + str(b)\n>>> rdd.combineByKey(str, add, add).collect()\n[('a', '113'), ('b', '2')]\n```\n\n\n\n## Transformations for two `PairRDD`\n\n\n| transformation | description                      |\n|:-------------:|:-------------------------------|\n| `subtractByKey(other)` | Remove elements with a key present in the `other` RDD. |\n| `join(other)` | Inner join with `other` RDD. |\n| `rightOuterJoin(other)` | Right join with `other` RDD. |\n| `leftOuterJoin(other)` | Left join with `other` RDD. |\n\n\n- Right join: the key must be present in the first RDD\n- Left join: the key must be present in the `other` RDD\n\n<center>\n![](/images/join-types.png){width=600px}\n</center>\n\n\n## Transformations for two `PairRDD`\n\n- Join operations are mainly used through the high-level API: `DataFrame` objects and the `spark.sql` API \n\n- We will use them a lot with the high-level API (`DataFrame` from `spark.sql`)\n\n::: aside\n\n[Let's go to notebook05_sparkrdd.ipynb](http://localhost:8888/notebooks/notebooks/notebook05_sparkrdd.ipynb)\n\n:::\n\n\n\n## Actions for a single  `PairRDD`\n\n\n| action         | description                      |\n|:-------------:|:-------------------------------|\n| `countByKey()` | Count the number of elements for each key. |\n| `lookup(key)`  | Return all the values associated with the provided `key`. |\n| `collectAsMap()` | Return the key-value pairs in this RDD to the master as a Python dictionary. |\n\n::: aside\n\n[Let's go to notebook05_sparkrdd.ipynb](http://localhost:8888/notebooks/notebooks/notebook05_sparkrdd.ipynb)\n\n:::\n\n\n## Data partitionning\n\n- Some operations on `PairRDD`s, such as `join`, require to scan the data **more than once**\n- Partitionning the RDDs **in advance** can reduce network communications\n- When a key-oriented dataset is reused several times, partitionning can improve  performance \n- In `Spark`: you can *choose which keys will appear on the same node*, but no explicit control of which worker node each key goes to.\n\n\n\n## Data partitionning\n\nIn practice, you can specify the number of partitions with\n\n```{.python}\nrdd.partitionBy(100)\n```\n\n. . .\n\nYou can also use a custom partition function `hash` such that `hash(key)` returns a hash value\n\n```{.python}\nimport urlparse\n\n>>> def hash_domain(url):\n        # Returns a hash associated to the domain of a website\n        return hash(urlparse.urlparse(url).netloc)\n\nrdd.partitionBy(20, hash_domain) # Create 20 partitions\n```\n\nTo have finer control on partitionning, you must use the Scala API.\n\n\n# Thank you !  {background-color=\"#1c191c\"}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}