{
  "hash": "28144b194ff358808699e30a183f2360",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Spark NLP\"\nengine: knitr\ndate: \"2025-01-17\"\n---\n\n\n\n# Spark-NLP in perspective {background-color=\"#1c191c\"}\n\n## Spark NLP\n\n[Spark NLP](https://sparknlp.org) provides an example of an application in the Apache Spark Ecosystem\n\n. . .\n\nSpark NLP relies on the Spark SQL Lib and Spark Dataframes (high level APIs) and also on the Spark ML Lib.\n\n. . .\n\nSpark NLP borrows ideas from existing NLP softwares and adapts the known techniques to the Spark principles\n\n\n...\n\nNLP deals with many applications of machine learning\n\n- Automatic translation  (see [deepl.com](https://www.deepl.com/translator))\n- Topic modeling (text clustering)\n- Sentiment Analysis\n- LLMs\n- ...\n\n\n\n# NLP Libraries {background-color=\"#1c191c\"}\n\n## Two flavors of NLP libraries\n\n-   *Functionality* Libraries  [nltk.org](https://www.nltk.org)\n\n-   *Annotation* Libraries [spaCy's site](https://spacy.io)\n\n\n## spaCy and Spark?\n\nA [databricks notebook](https://winf-hsos.github.io/databricks-notebooks/big-data-analytics/ss-2020/NLP%20with%20Python%20and%20spaCy%20-%20First%20Steps.html) discusses possible interactions between spaCy and Spark on a use case:\n\n. . .\n\n-   Get the tweets (the texts) into a Spark dataframe using `spark.sql()`\n-   Convert the Spark dataframe to a `numpy` array\n-   Stream all tweets in batches using `nlp.pipe()`\n-   Go through the processed tweets and take copy everything we need in a large array object\n-   Convert back the large array object into a Spark dataframe\n-   Save the dataframe as table, so we can query the whole thing withh SQL again\n\n. . .\n\n::: callout-warning\n\n### No hint at parallelizing spaCy's annotation process\n\n:::\n\n## spaCy v2 (current v3.7)\n\n> spaCy v2 now fully supports the `Pickle` protocol, making it easy to use spaCy with Apache Spark.\n\n[spaCy v2 documentation](https://spacy.io/usage/v2)\n\n# A short example (from [John Snow Labs](https://sparknlp.org)) {background-color=\"#1c191c\"}\n\n---\n\n-   Initializing a `sparknlp` session\n-   Building a toy NLP pipeline for detecting dates in a text\n\n## Imports sparknlp and others\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Import Spark NLP\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\nfrom sparknlp.pretrained import PretrainedPipeline\nimport sparknlp\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Initiate Spark session\n\nAssuming `standalone` mode on a laptop. `master` runs on `localhost`\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nspark = SparkSession.builder \\\n            .appName(\"Spark NLP\") \\\n#            .master(\"spark://localhost:7077\") \\\n            .config(\"spark.driver.memory\", \"16G\") \\\n            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n            .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n            .config(\"spark.driver.maxResultSize\", \"0\") \\\n            .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.2.3\") \\\n            .getOrCreate()\n```\n:::\n\n\n\n. . .\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsparknlp.version()\n```\n:::\n\n\n\n...\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nspark\n```\n:::\n\n\n\n\n## Toy (big) data\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfr_articles = [\n  (\"Le dimanche 11 juillet 2021, Chiellini a utilisé le mot Kiricocho lorsque Saka s'est approché du ballon pour le penalty.\",),\n  (\"La prochaine Coupe du monde aura lieu en novembre 2022.\",),\n  (\"À Noël 800, Charlemagne se fit couronner empereur à Rome.\",),\n  (\"Le Marathon de Paris a lieu le premier dimanche d'avril 2024\",)\n]\n```\n:::\n\n\n\n. . .\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\narticles_cols = [\"text\"]\n\ndf = spark.createDataFrame(\n  data=fr_articles, \n  schema=articles_cols)\n\ndf.printSchema()\n```\n:::\n\n\n\n## Pipelines\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndocument_assembler = DocumentAssembler() \\\n            .setInputCol(\"text\") \\\n            .setOutputCol(\"document\")\n```\n:::\n\n\n\n::: callout\nColumn `document` contains the 'text' to be annotated as well as some possible metadata.\n\nStarting point of any annotation process\n\nSpark NLP relies on Saprk SQL for storing, moving, data.\n:::\n\n. . .\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndate_matcher = DateMatcher() \\\n            .setInputCols(['document']) \\\n            .setOutputCol(\"date\") \\\n            .setOutputFormat(\"MM/dd/yyyy\") \\\n            .setSourceLanguage(\"fr\")\n```\n:::\n\n\n\n::: callout\n-   Spark NLP adopts an original way of storing annotations\n-   Spark NLP creates columns for annotations\n-   Spark NLP stores annotation in Spark dataframes\n-   Annotators are\n    -   Tranformers\n    -   Estimators\n    -   Models\n:::\n\n## Transformation/Action\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nassembled = ( \n  document_assembler.transform(df)\n)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n(\n date_matcher\n  .transform(assembled)\n  .select('date')\n  .show(10, False)\n)\n```\n:::\n\n\n\n## More\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfr_articles.append((\"Nous nous sommes rencontrés le 13/05/2018 puis le 18/05/2020.\",))\n\nfr_articles.append((\"Nous nous sommes rencontrés il y a 2 jours et il m'a dit qu'il nous rendrait visite la semaine prochaine.\",))\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf = spark.createDataFrame(\n  data=fr_articles, \n  schema=articles_cols)\n\ndf.printSchema()\ndf.show()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nassembled = ( \n  document_assembler.transform(df)\n)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n(\n date_matcher\n  .transform(assembled)\n  .select('date')\n  .show(10, False)\n)\n```\n:::\n\n\n\n## Another annotator\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndate_matcher_bis = MultiDateMatcher() \\\n            .setInputCols(['document']) \\\n            .setOutputCol(\"date\") \\\n            .setOutputFormat(\"MM/dd/yyyy\") \\\n            .setSourceLanguage(\"fr\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n(\n  date_matcher_bis\n    .transform(assembled)\n    .select(\"date\")\n    .show(10, False)\n)\n```\n:::\n\n\n\n# Spark NLP Design {background-color=\"#1c191c\"}\n\n## SQL Lib and Dataframes\n\n## ML Lib, Transformers and Estimators\n\n# Spark NLP Pipelines {background-color=\"#1c191c\"}\n\n##  Getting a corpus : ETL\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\npattern = 'URL: http://www.nytimes.com/(?P<zedate>[0-9]{4}/[0-9]{2}/[0-9]{2})/.*'\ntitle = 'URL: http://www.nytimes.com/[0-9]{4}/[0-9]{2}/[0-9]{2}/(.*)'\nreg_date = re.compile(pattern)\nreg_title = re.compile(title)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnypath = Path('../data/nytimes_news_articles.txt')\ncorpus_list = list()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nwith open(nypath, encoding='UTF-8')  as fd:\n    doc, document = None, None\n    while l := fd.readline():        \n        if m := reg_date.match(l):\n            if doc is not None:\n                corpus_list.append((*document, doc))\n                doc, document = None, None\n            ymd = date(*[int(n) for n in m.groups()[0].split('/')])\n            title = (\n                reg_title.match(l)\n                  .groups()[0]\n                  .split('/')\n            )\n            document =  (ymd, title[-1], '/'.join(title[:-1]))\n            doc = ''\n        else: doc = doc + l\n    else:\n        if doc is not None:\n            corpus_list.append((*document, doc))\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf_texts = spark.createDataFrame(corpus_list,\n                      schema= StructType([\n    StructField('date', DateType(), False),\n    StructField('title', StringType(), False),\n    StructField('topic', StringType(), False),\n    StructField('text', StringType(), True)\n]))\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf_texts.printSchema()\ndf_texts.count()\n```\n:::\n\n\n\n## Saving \n\nLocally\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf_texts.write.parquet('../data/ny_corpus_pq')\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nspam = spark.read.parquet('../data/ny_corpus_pq')\n\nspam.printSchema()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nspam.rdd.getNumPartitions()\n```\n:::\n\n\n\n## \n\n\n::: {.cell}\n\n```{.python .cell-code}\ncorpus_assembled = ( \n  document_assembler.transform(df_texts)\n)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ncorpus_assembled.printSchema()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n(\n  date_matcher_bis\n    .transform(corpus_assembled)\n    .select(\"title\", \"date\")\n    .show(10, False)\n)\n```\n:::\n\n\n\n::: {.callout-warning}\nExtracted dates should be taken with a grain of salt \n:::\n\n\n## Public pipelines\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sparknlp.pretrained import PretrainedPipeline\nexplain_document_pipeline = PretrainedPipeline(\"explain_document_ml\")\n```\n:::\n\n\n## Chaining annotators\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsentenceDetector = SentenceDetector() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"sentence\")\nregexTokenizer = Tokenizer() \\\n    .setInputCols([\"sentence\"]) \\\n    .setOutputCol(\"token\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfinisher = Finisher() \\\n    .setInputCols([\"token\"]) \\\n    .setIncludeMetadata(True)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npipeline = Pipeline().setStages([\n    document_assembler,\n    sentenceDetector,\n    regexTokenizer,\n    finisher\n])\n```\n:::\n\n\n\n## Fitting and transforming\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nspam = ( \n  pipeline.fit(df_texts)\n    .transform(df_texts)\n    .select(\"finished_token\")\n    .collect()\n)\n```\n:::\n\n\n\n## A customized pipeline\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nstemmer = (\n  Stemmer()\n    .setInputCols(['token'])\n    .setOutputCol('stem')\n)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlemmatizer = (\n  LemmatizerModel.pretrained()\n    .setInputCols(['token'])\n    .setOutputCol('lemma')\n)\n```\n:::\n\n\n\n::: {.callout-warning}\n```{.verbatim}\nlemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n[ / ]lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n[ / ]Download done! Loading the resource.\n[ — ]\n\n[OK!]\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nposTagger = PerceptronModel.pretrained() \\\n    .setInputCols([\"document\", \"token\"]) \\\n    .setOutputCol(\"pos\")\n```\n:::\n\n\n\n::: {.callout-warning}\n\n```\npos_anc download started this may take some time.\nApproximate size to download 3.9 MB\n[ — ]pos_anc download started this may take some time.\nApproximate size to download 3.9 MB\n[ \\ ]Download done! Loading the resource.\n[Stage 34:===========================================>              (3 + 1) / 4]\n[ | ]\n                                                                \n[OK!]\n```\n\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfinisher = (\n  Finisher()\n    .setInputCols([\n      'token', \n#      'stem', \n#      'lemma', \n      'pos'])\n    .setIncludeMetadata(False)\n    .setOutputAsArray(True)\n)\n```\n:::\n\n\n## \n\n\n\n::: {.cell}\n\n```{.python .cell-code}\npipeline = (\n  Pipeline()\n    .setStages([\n      document_assembler,\n      sentenceDetector,\n      regexTokenizer,\n      posTagger, \n      finisher\n    ])\n)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nspam = ( \n  pipeline.fit(df_texts)\n    .transform(df_texts)\n    .selectExpr(\"*\")\n    .collect()\n)\n```\n:::\n\n\n\n# Spark NLP and feature engineering {background-color=\"#1c191c\"}\n\n## Topic modelling\n\n## TF-IDF\n\n## Latent Dirichlet Allocation\n\n# Distributed computations {background-color=\"#1c191c\"}\n\n## Execution modes\n\n-   standalone\n-   client\n-   cluster\n-   \n\n# Spark NLP and composite types in Spark Dataframes \n\n```{.python}\n\n>>> result = documentAssembler.transform(data)\n>>> result.select(\"document\").show(truncate=False)\n+----------------------------------------------------------------------------------------------+\n|document                                                                                      |\n+----------------------------------------------------------------------------------------------+\n|[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -> 0], []]]|\n+----------------------------------------------------------------------------------------------+\n>>> result.select(\"document\").printSchema()\nroot\n|-- document: array (nullable = True)\n|    |-- element: struct (containsNull = True)\n|    |    |-- annotatorType: string (nullable = True)\n|    |    |-- begin: integer (nullable = False)\n|    |    |-- end: integer (nullable = False)\n|    |    |-- result: string (nullable = True)\n|    |    |-- metadata: map (nullable = True)\n|    |    |    |-- key: string\n|    |    |    |-- value: string (valueContainsNull = True)\n|    |    |-- embeddings: array (nullable = True)\n|    |    |    |-- element: float (containsNull = False)\n```\n\n\nColumn `document` is of type `ArrayType()`. The basetype of `document` column is of `StructType()` (`element`), the `element` contains subfields of primitive type, but alo a field of type `map` (`MapType()`) and a field of type `StructType()`.  ",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}