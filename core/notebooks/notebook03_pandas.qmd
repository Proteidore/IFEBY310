---
title: Introduction to `pandas`
jupyter: python3
---


The `pandas` library (https://pandas.pydata.org) is one of the most used tool at the disposal of people working with data in `python` today.

- It allows to **crunch data** easily
- It mainly provides a `DataFrame` object (a **table of data**) with a huge set of functionalities


## Why ?

Through `pandas`, you get acquainted with your data by **analyzing** it 

- What's the average, median, max, or min of each column?
- Does column A correlate with column B?
- What does the distribution of data in column C look like?

## Why  (con't) ?

you get acquainted with your data by **cleaning** and  **transforming** it 

- Removing missing values, filter rows or columns using some criteria
- Store the cleaned, transformed data back into virtually any format or database
- Data visualization (when combined `matplotlib` or `seaborn` or others)

## Where ?

`pandas` is a central component of the `python` "stack" for data science

- `pandas` is built on top of `numpy`
- often used in conjunction with other libraries
- a `DataFrame` is often fed to plotting functions or machine learning algorithms (such as `scikit-learn`)
- Well-interfaced with `jupyter`, leading to a nice interactive environment for data exploration and modeling

## Core components of pandas

The two primary components of pandas are the `Series` and `DataFrame`.

- A `Series` is essentially a column

- A `DataFrame` is a multi-dimensional table made up of a collection of `Series` with equal length

## Creating a `DataFrame` from scratch

```{python}
#| 
import pandas as pd

fruits = {
    "apples": [3, 2, 0, 1],
    "oranges": [0, 3, 7, 2]
}

df_fruits = pd.DataFrame(fruits)
df_fruits
```

```{python}
#| 
type(df_fruits)
```

```{python}
#| 
#| scrolled: true
df_fruits["apples"]
```

```{python}
#| 
type(df_fruits["apples"])
```

## Indexing

- By default, a `DataFrame` uses a contiguous index
- But what if we want to say **who** buys the fruits ?

```{python}
#| 
df_fruits = pd.DataFrame(fruits, index=["Daniel", "Sean", "Pierce", "Roger"])
df_fruits
```

## `.loc` versus `.iloc`

- `.loc` **loc**ates by name
- `.iloc` **loc**ates by numerical **i**ndex

```{python}
#| 
df_fruits
```

```{python}
#| 
#| scrolled: true
# What's in Sean's basket ?
df_fruits.loc['Sean']
```

```{python}
#| 
# Who has oranges ?
df_fruits.loc[:, 'oranges']
```

```{python}
#| 
# How many apples in Pierce's basket ?
df_fruits.loc['Pierce', 'apples']
```

```{python}
#| 
#| scrolled: true
df_fruits
```

```{python}
#| 
df_fruits.iloc[2, 1]
```

## Main attributes and methods of a `DataFrame`

A `DataFrame` has many **attributes**

```{python}
#| 
#| scrolled: true
df_fruits.columns
```

```{python}
#| 
df_fruits.index
```

```{python}
#| 
df_fruits.dtypes
```

A `DataFrame` has many **methods**

```{python}
#| 
df_fruits.info()
```

```{python}
#| 
#| 
df_fruits.describe()
```

## Missing values

What if we don't know how many apples are in Sean's basket ?

```{python}
#| 
#| scrolled: true
df_fruits.loc['Sean', 'apples'] = None
df_fruits
```

```{python}
#| 
#| scrolled: true
df_fruits.describe()
```

Note that `count` is **3** for apples now, since we have 1 missing value among the 4


::: {.callout-note}

To review the members of objects of class `pandas.DataFrame`, `dir()` and module `inspect` are convenient. 
:::

```{python}
#| eval: false
[x for x in dir(df_fruits) if not x.startswith('_') and not callable(x)]
```

```{python}
import inspect

# Get a list of methods
membres = inspect.getmembers(df_fruits)

method_names = [m[0] for m in membres 
    if callable(m[1]) and not m[0].startswith('_')]

print(method_names)
```

```{python}
others = [x for x in membres
    if not callable(x[1])]

[x[0] for x in others if not x[0].startswith('_')]
```
## Adding a column

Ooooops, we forgot about the bananas !

```{python}
#| 
df_fruits["bananas"] = [0, 2, 1, 6]
df_fruits
```

## Adding a column with the date

And we forgot the dates !

```{python}
#| 
#| scrolled: true
df_fruits['time'] = [
    "2020/10/08 12:13", "2020/10/07 11:37", 
    "2020/10/10 14:07", "2020/10/09 10:51"
]
df_fruits
```

```{python}
#| 
#| scrolled: true
#| 
df_fruits.dtypes
```

```{python}
#| 
#| scrolled: true
type(df_fruits.loc["Roger", "time"])
```

It is not a date but a string (`str`) ! So we convert this column to something called `datetime`  

```{python}
#| 
#| scrolled: true
df_fruits["time"] = pd.to_datetime(df_fruits["time"])
df_fruits
```

```{python}
#| 
df_fruits.dtypes
```

::: {.callout-note}

Every data science framework implements some `datetime` handling scheme. For Python see [Python official documentation on `datetime` module](https://docs.python.org/3/library/datetime.html#module-datetime)

:::

What if we want to keep only the baskets after (including) October, 9th ?

```{python}
#| 
#| scrolled: false
df_fruits.loc[df_fruits["time"] >= pd.Timestamp("2020/10/09")]
```

## Slices and subsets of rows or columns

```{python}
#| 
df_fruits
```

```{python}
#| 
df_fruits.loc[:, "oranges":"time"]
```

```{python}
#| 
df_fruits.loc["Daniel":"Sean", "apples":"bananas"]
```

```{python}
#| 
#| scrolled: true
df_fruits[["apples", "time"]]
```

## Write our data to a CSV file

What if we want to write the file ?

```{python}
df_fruits
```

```{python}
#| 
df_fruits.to_csv("fruits.csv")
```

```{python}
#| 
#| scrolled: false
# Use !dir on windows
!ls -alh | grep fru
```

```{python}
#| 
!head -n 5 fruits.csv
```

## Reading data and working with it



::: {.callout-note}

The `tips` dataset comes through [Kaggle](https://www.kaggle.com/code/sanjanabasu/tips-dataset/input)

> This dataset is a treasure trove of information from a collection of case studies for business statistics. Special thanks to Bryant and Smith for their diligent work:

> Bryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing.

> You can also access this dataset now through the Python package Seaborn.

:::

It contains data about a restaurant: the bill, tip and some informations about the customers.

::: {.callout-note}

### A toy extraction pattern

A data pipeline usually starts with Extraction, that is gathering data from some source, possibly in a galaxy far, far awy. Here follows a toy extraction pattern

- obtain the data from some `URL` using package `requests`
- save the data on the hard drive
- load the data using Pandas 


```{python}
#| scrolled: true
#| eval: false
import requests
import os

# The path containing your notebook
path_data = './'
# The name of the file
filename = 'tips.csv'

if os.path.exists(os.path.join(path_data, filename)):
    print('The file %s already exists.' % os.path.join(path_data, filename))
else:
    url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/tips.csv'
    r = requests.get(url)
    with open(os.path.join(path_data, filename), 'wb') as f:
        f.write(r.content)
    print('Downloaded file %s.' % os.path.join(path_data, filename))
```

```{python}
#| eval: false
#| scrolled: true
df = pd.read_csv("tips.csv", 
    delimiter=","
)
```

:::

The data can be obtained from package `seaborn`.

```{python}
import seaborn as sns

sns_ds = sns.get_dataset_names()

'tips' in sns_ds

df = sns.load_dataset('tips')
```

```{python}
# `.head()` shows the first rows of the dataframe
df.head(n=10)
```

```{python}
#| 
#| scrolled: true
df.info()
```

```{python}
#| 
#| scrolled: true
df.loc[42, "day"]
```

```{python}
#| 
#| scrolled: true
type(df.loc[42, "day"])
```

By default, columns that are non-numerical contain strings (`str` type)

## The `category` type

An important type in `pandas` is `category` for variables that are **non-numerical**

**Pro tip.** It's always a good idea to tell `pandas` which columns should be imported as **categorical**

So, let's read again the file specifying some `dtype`s to the `read_csv` function

```{python}
#| 
dtypes = {
    "sex": "category",
    "smoker": "category",
    "day": "category",
    "time": "category"
} 

df = pd.read_csv("tips.csv", dtype=dtypes)
```

```{python}
#| 
#| scrolled: true
df.dtypes
```

## Computing statistics

```{python}
#| 
# The describe method only shows statistics for the numerical columns by default
df.describe()
```

```{python}
#| 
#| scrolled: true
# We use the include="all" option to see everything
df.describe(include="all")
```

```{python}
#| 
# Correlation between the numerical columns
df.corr(numeric_only = True)
```

```{python}
?df.corr
```

# Data visualization with `matplotlib` and `seaborn`

Let's show how we can use `matplotlib` and `seaborn` to visualize data contained in a `pandas` dataframe

```{python}
#| 
import matplotlib.pyplot as plt

```

## How do the tip depends on the total bill ?

```{python}
#| 
sns.jointplot(x="total_bill", y="tip", data=df)
```

## When do customers go to this restaurant ?

```{python}
#| 
#| scrolled: false
sns.countplot(x='day', hue="time", data=df)
```

## When do customers spend the most ?

```{python}
#| 
plt.figure(figsize=(7, 5))
sns.boxplot(x='day', y='total_bill', hue='time', data=df)
plt.legend(loc="upper left")
```

```{python}
#| 
plt.figure(figsize=(7, 5))
sns.violinplot(x='day', y='total_bill', hue='time', split=True, data=df)
plt.legend(loc="upper left")
```

## Who spends the most ?

```{python}
#| 
#| scrolled: true
sns.boxplot(x='sex', y='total_bill', hue='smoker', data=df)
```

## When should waiters want to work ?

```{python}
#| 
#| scrolled: true
sns.boxplot(x='day', y='tip', hue='time', data=df)
```

```{python}
#| 
sns.violinplot(x='day', y='tip', hue='time', data=df)
```

# Data processing with `pandas`

Let us read again the `tips.csv` file

```{python}
#| 
#| scrolled: true
import pandas as pd

dtypes = {
    "sex": "category",
    "smoker": "category",
    "day": "category",
    "time": "category"
} 

df = pd.read_csv("tips.csv", dtype=dtypes)
df.head()
```

## Computations using `pandas` : broadcasting

Let's add a column that contains the tip percentage

```{python}
#| 
#| scrolled: false
df["tip_percentage"] = df["tip"] / df["total_bill"]
df.head()
```

The computation

```{{python}}
df["tip"] / df["total_bill"]
```
uses a **broadcast** rule.

- We can multiply, add, subtract, etc. together `numpy` arrays, `Series` or `pandas` dataframes when the computation **makes sense** in view of their respective **shape**

This principle is called **broadcast** or **broadcasting**.

::: {.callout-note}

Broadcasting is a key feature of `numpy` `ndarray`, see 

- [Numpy User's guide](https://numpy.org/doc/stable/user/basics.broadcasting.html)
- [Pandas book](https://wesmckinney.com/book/advanced-numpy.html#numpy_broadcasting)

:::

```{python}
#| 
#| scrolled: false
df["tip"].shape, df["total_bill"].shape
```

The `tip` and `total_bill`columns have the same `shape`, so broadcasting performs **pairwise division**.

This corresponds to the following "hand-crafted" approach with a `for` loop:

```{python}
#| 
for i in range(df.shape[0]):
    df.loc[i, "tip_percentage"] = df.loc[i, "tip"] / df.loc[i, "total_bill"]
```

But using such a loop is: 

- longer to write
- less readable 
- prone to mistakes
- and *slower* :(

*NEVER* use `Python` for-loops unless you need to !

```{python}
#| 
%%timeit -n 10
for i in range(df.shape[0]):
    df.loc[i, "tip_percentage"] = df.loc[i, "tip"] / df.loc[i, "total_bill"]
```

```{python}
#| 
%%timeit -n 10
df["tip_percentage"] = df["tip"] / df["total_bill"]
```

The `for` loop is $\approx$ **100 times slower** ! (even worse on larger data)

### Pitfall. Changing values in a `DataFrame`

When you want to change a value in a `DataFrame`, never use

```python
df["tip_percentage"].loc[i] = 42
```

but use

```python
df.loc[i, "tip_percentage"] = 42
```

::: {.callout-caution}

Use a **single** `loc` or `iloc` statement. The first version **might not work**: it might modify a copy of the column and not the dataframe itself !

:::

Another example of broadcasting is:

```{python}
#| 
(100 * df[["tip_percentage"]]).head()
```

where we multiplied **each entry** of the `tip_percentage` column by 100.

::: {.callout-note}

### Remark 

Note the difference between


```python
df[['tip_percentage']]
```

which returns a `DataFrame` containing only the `tip_percentage` column and

```python
df['tip_percentage']
```

which returns a `Series` containing the data of the `tip_percentage` column

:::


## Some more plots

### How do the tip percentages relates to the total bill ?

```{python}
#| 
sns.jointplot(
    x="total_bill", 
    y="tip_percentage", 
    data=df
)
```

### Who tips best ?

```{python}
#| 
sns.boxplot(
    x='sex', 
    y='tip_percentage', 
    hue='smoker', 
    data=df
)
```

### Who tips best without the `tip_percentage` outliers ?

```{python}
#| 
sns.boxplot(
    x='sex', 
    y='tip_percentage', 
    hue='smoker', 
    data=df.loc[df["tip_percentage"] <= 0.3]
)
```

Object identity

```{python}
#| 
id(df)
```

## The all-mighty `groupby` and `aggregate`

Many computations can be formulated as a **groupby** followed by and **aggregation**.

### What is the mean `tip` and `tip percentage` each day ?

```{python}
#| 
df.head()
```

```{python}
#| 
#| scrolled: true

try:

    df.groupby("day", observed=True).mean()
except TypeError:
    print('TypeError: category dtype does not support aggregation "mean"')
```

But we do not care about the `size` column here, so we can use instead

```{python}
(
    df[["total_bill", "tip", "tip_percentage", "day"]]
        .groupby("day")
        .mean()
)
```

If we want to be more precise, we can `groupby` using several columns

```{python}
#| 
(
    df[["total_bill", "tip", "tip_percentage", "day", "time"]]   # selection
        .groupby(["day","time"])                                # partition
        .mean()                                                  # aggregation
)
```

::: {.callout-note}

### Remarks 

- We obtain a `DataFrame` with a two-level indexing: on the `day` and the `time`
- Groups must be homogeneous: we have `NaN` values for empty groups (e.g. `Sat`, `Lunch`)

:::


### Pro tip

Sometimes, it is more convenient to get the groups as columns instead of a multi-level index.

For this, use `reset_index`:

```{python}
#| 
(
    df[["total_bill", "tip", "tip_percentage", "day", "time"]]   # selection
        .groupby(["day", "time"])                                # partition
        .mean() # aggregation
        .reset_index()   # ako ungroup
)
```

### Another pro tip: care about code readers

Computations with pandas can include many operations that are **pipelined** until the final computation.

Pipelining many operations is good practice and perfectly normal, but in order to make the code readable you can put it between parenthesis (`python` expression) as follows:

```{python}
#| 
#| scrolled: true
(
    df[["total_bill", "tip", "tip_percentage", "day", "time"]]
    .groupby(["day", "time"])
    .mean()
    .reset_index()
    # and on top of all this we sort the dataframe with respect 
    # to the tip_percentage
    .sort_values("tip_percentage")
)
```

## Displaying a `DataFrame` with `style`

Now, we can answer, with style, to the question: what are the average tip percentages along the week ?

```{python}
#| 
#| scrolled: true
(
    df[["tip_percentage", "day", "time"]]
    .groupby(["day", "time"])
    .mean()
    # At the end of the pipeline you can use .style
    .style
    # Print numerical values as percentages 
    .format("{:.2%}")
    .background_gradient()
)
```

## Removing the `NaN` values

But the `NaN` values are somewhat annoying. Let's remove them

```{python}
#| 
(
    df[["tip_percentage", "day", "time"]]
    .groupby(["day", "time"])
    .mean()
    # We just add this from the previous pipeline
    .dropna()
    .style
    .format("{:.2%}")
    .background_gradient()
)
```

Now, we see when `tip_percentage` is maximal. But what about the standard deviation?

- We used only `.mean()` for now, but we can use several aggregating function using `.agg()`

```{python}
#| 
#| scrolled: true
(
    df[["tip_percentage", "day", "time"]]
    .groupby(["day", "time"])
    .agg(["mean", "std"])   # we feed `agg`  with a list of names of callables 
    .dropna()
    .style
    .format("{:.2%}")
    .background_gradient()
)
```

And we can use also `.describe()` as aggregation function. Moreover we
- use the `subset` option to specify which column we want to style
- we use `("tip_percentage", "count")` to access multi-level index

```{python}
(
    df[["tip_percentage", "day", "time"]]
    .groupby(["day", "time"])
    .describe()    # all-purpose summarising function
)
```

```{python}
#| 
(
    df[["tip_percentage", "day", "time"]]
    .groupby(["day", "time"])
    .describe()
    .dropna()
    .style
    .bar(subset=[("tip_percentage", "count")])
    .background_gradient(subset=[("tip_percentage", "50%")])
)
```

## Supervised learning of `tip` based on the `total_bill` 

As an example of very simple **machine-learning** problem, let us try to understand how we can predict `tip` based on `total_bill`.

```{python}
#| 
import numpy as np

plt.scatter(df["total_bill"], df["tip"])
plt.xlabel("total_bill", fontsize=12)
plt.ylabel("tip", fontsize=12)
```

There's a rough **linear** dependence between the two. Let us try to find it by hand!<br>
Namely, we look for numbers $b$ and $w$ such that

```
tip ≈ b + w × total_bill
```

for all the examples of pairs of `(tip, total_bill)` we observe in the data.

In **machine learning**, we say that this is a very simple example of a **supervised learning** problem (here it is a regression problem), where `tip` is the **label** and where `total_bill` is the (only) **feature**, for which we intend to use a **linear predictor**.

```{python}
#| 
plt.scatter(df["total_bill"], df["tip"])
plt.xlabel("total_bill", fontsize=12)
plt.ylabel("tip", fontsize=12)

slope = 1.0
intercept = 0.0

x = np.linspace(0, 50, 1000)
plt.plot(x, intercept + slope * x, color="red")
```

### A more interactive way 

This might require


```{python}
#| 
# !pip install ipympl
```

```{python}
#| 
#| scrolled: false
#| 
import ipywidgets as widgets
import matplotlib.pyplot as plt
import numpy as np

%matplotlib widget
%matplotlib inline

x = np.linspace(0, 50, 1000)

@widgets.interact(intercept=(-5, 5, 1.), slope=(0, 1, .05))
def update(intercept=0.0, slope=0.5):
    plt.scatter(df["total_bill"], df["tip"])
    plt.plot(x, intercept + slope * x, color="red")
    plt.xlim((0, 50))
    plt.ylim((0, 10))
    plt.xlabel("total_bill", fontsize=12)
    plt.ylabel("tip", fontsize=12)
```

This is kind of tedious to do this by hand... it would be nice to come up with an **automated** way of doing this. Moreover:

- We are using a **linear** function, while something more complicated (such as a polynomial) might be better
- More importantly, we use **only** the `total_bill` column to predict the `tip`, while we know about many other things

```{python}
#| 
df.head()
```

## One-hot encoding of categorical variables

We can't perform computations (products and sums) with columns containing **categorical** variables. So, we can't use them like this to predict the `tip`.
We need to **convert** them to numbers somehow.

The most classical approach for this is **one-hot encoding** (or "create dummies" or "binarize") of the categorical variables, which can be easily achieved with `pandas.get_dummies`

Why *one-hot* ? See [wikipedia](https://en.wikipedia.org/wiki/One-hot) for a plausible explanation

```{python}
#| 
#| scrolled: true
#| 
df_one_hot = pd.get_dummies(df, prefix_sep='#')
df_one_hot.head(5)
```

Only the categorical columns have been one-hot encoded. For instance, the `"day"` column is replaced by 4 columns named `"day#Thur"`, `"day#Fri"`, `"day#Sat"`, `"day#Sun"`, since `"day"` has 4 modalities (see next line).

```{python}
#| 
#| scrolled: true
df['day'].unique()
```

```{python}
#| 
#| scrolled: false
df_one_hot.dtypes
```

## Pitfall. Colinearities with one-hot encoding

Sums over dummies for `sex`, `smoker`, `day`, `time` and `size` are all equal to one (by constrution of the one-hot encoded vectors).

- Leads to **colinearities** in the matrix of features
- It is **much harder** to train a linear regressor when the columns of the features matrix has colinearities

```{python}
#| 
#| scrolled: true
day_cols = [col for col in df_one_hot.columns if col.startswith("day")]
df_one_hot[day_cols].head()
df_one_hot[day_cols].sum(axis=1)
```

```{python}
#| 
all(df_one_hot[day_cols].sum(axis=1) == 1)
```

The most standard solution is to remove a modality (i.e. remove a one-hot encoding vector). Simply achieved by specifying `drop_first=True` in the `get_dummies` function.

```{python}
#| 
df["day"].unique()
```

```{python}
#| 
pd.get_dummies(df, prefix_sep='#', drop_first=True).head()
```

Now, if a categorical feature has $K$ modalities, we use only $K-1$ dummies.
For instance, there is no more `sex#Female` binary column. 

**Question.** So, a linear regression won't fit a weight for `sex#Female`. But, where do the model weights of the dropped binary columns go ?

**Answer.** They just "go" to the **intercept**: interpretation of the population bias depends on the "dropped" one-hot encodings.

So, we actually fit:
$$\begin{array}{rl} \texttt{tip} \approx b & + w_1 \times \texttt{total_bill} + w_2 \times \texttt{size} \\ & + w_3 \times \texttt{sex#Male} + w_4 \times \texttt{smoker#Yes} \\ & + w_5 \times \texttt{day#Sat} + w_6 \times \texttt{day#Sun} + w_7 \times \texttt{day#Thur} \\ & + w_8 \times \texttt{time#Lunch} \end{array}$$





